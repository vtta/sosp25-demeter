 Documentation/admin-guide/sysctl/kernel.rst |   18 +
 Documentation/admin-guide/sysctl/vm.rst     |   12 +
 arch/x86/boot/compressed/Makefile           |    2 +-
 arch/x86/entry/syscalls/syscall_64.tbl      |    2 +
 arch/x86/include/asm/pgtable.h              |   11 +
 arch/x86/include/asm/pgtable_64.h           |    4 +
 arch/x86/include/asm/pgtable_types.h        |    9 +
 arch/x86/kernel/ftrace_32.S                 |    1 +
 arch/x86/kernel/ftrace_64.S                 |    2 +
 arch/x86/mm/init_64.c                       |   15 +
 arch/x86/mm/pgtable.c                       |   22 +-
 drivers/dma/ioat/dma.c                      |   12 +-
 include/asm-generic/pgalloc.h               |    3 +
 include/linux/cgroup-defs.h                 |    3 +
 include/linux/cleanup.h                     |  250 +++++
 include/linux/compiler-clang.h              |    9 +
 include/linux/ftrace.h                      |    1 +
 include/linux/htmm.h                        |  224 +++++
 include/linux/huge_mm.h                     |    4 +
 include/linux/memcontrol.h                  |   56 +-
 include/linux/mempolicy.h                   |   34 +-
 include/linux/migrate.h                     |   21 +
 include/linux/mm.h                          |    8 +
 include/linux/mm_types.h                    |   31 +
 include/linux/mmzone.h                      |   16 +
 include/linux/node.h                        |    6 +
 include/linux/nomad.h                       |  102 ++
 include/linux/page-flags.h                  |   34 +
 include/linux/page_ext.h                    |    3 +
 include/linux/perf_event.h                  |    6 +
 include/linux/rmap.h                        |   26 +-
 include/linux/sched/numa_balancing.h        |   63 +-
 include/linux/sched/sysctl.h                |    6 +
 include/linux/swap.h                        |    2 +
 include/linux/syscalls.h                    |    4 +
 include/linux/vm_event_item.h               |   40 +
 include/linux/vmstat.h                      |    8 +
 include/trace/events/htmm.h                 |   62 ++
 include/trace/events/mmflags.h              |   29 +-
 kernel/cgroup/cgroup.c                      |    3 +
 kernel/events/Makefile                      |    1 +
 kernel/events/core.c                        |   26 +-
 kernel/events/internal.h                    |    2 +
 kernel/events/memtis.c                      |   77 ++
 kernel/exit.c                               |    3 +
 kernel/fork.c                               |   13 +
 kernel/sched/core.c                         |   36 +-
 kernel/sched/fair.c                         |   22 +-
 kernel/sched/sched.h                        |    4 +
 kernel/sysctl.c                             |   27 +-
 kernel/trace/fgraph.c                       |    3 +-
 kernel/trace/trace.h                        |    1 +
 kernel/trace/trace_entries.h                |    5 +-
 kernel/trace/trace_functions_graph.c        |   72 +-
 mm/Kconfig                                  |    6 +
 mm/Makefile                                 |    3 +
 mm/demeter/Kconfig                           |    7 +
 mm/demeter/Makefile                          |    5 +
 mm/demeter/balloon-compact.h                 |   63 ++
 mm/demeter/balloon.c                         |  798 +++++++++++++++
 mm/huge_memory.c                            |  259 ++++-
 mm/internal.h                               |   10 +
 mm/khugepaged.c                             |  104 +-
 mm/memcontrol.c                             |   64 +-
 mm/memory.c                                 |  104 +-
 mm/memory_hotplug.c                         |    9 +-
 mm/mempolicy.c                              |   83 +-
 mm/memtis/Kconfig                           |   11 +
 mm/memtis/Makefile                          |    2 +
 mm/memtis/core.c                            | 1417 +++++++++++++++++++++++++++
 mm/memtis/memcontrol.c                      |  242 +++++
 mm/memtis/migrate.c                         |  118 +++
 mm/memtis/migrater.c                        | 1103 +++++++++++++++++++++
 mm/memtis/rmap.c                            |  231 +++++
 mm/memtis/sampler.c                         |  419 ++++++++
 mm/memtis/sysfs.c                           |  576 +++++++++++
 mm/migrate.c                                |  151 ++-
 mm/mprotect.c                               |    8 +-
 mm/nomad/Kconfig                            |   13 +
 mm/nomad/Makefile                           |    4 +
 mm/nomad/async_promote_main.c               | 1144 +++++++++++++++++++++
 mm/nomad/buffer_ring.c                      |  182 ++++
 mm/nomad/buffer_ring.h                      |   31 +
 mm/nomad/memory.c                           |   13 +
 mm/nomad/migrate.c                          | 1383 ++++++++++++++++++++++++++
 mm/nomad/rmap.c                             | 1189 ++++++++++++++++++++++
 mm/page_alloc.c                             |   48 +-
 mm/profile.h                                |   34 +
 mm/rmap.c                                   |   10 +-
 mm/swap.c                                   |   19 +
 mm/vmscan.c                                 |  119 ++-
 mm/vmstat.c                                 |   66 +-
 scripts/Makefile.lib                        |    2 +-
 93 files changed, 11354 insertions(+), 152 deletions(-)

diff --git a/Documentation/admin-guide/sysctl/kernel.rst b/Documentation/admin-guide/sysctl/kernel.rst
index 48b91c485c99..423033ce63c1 100644
--- a/Documentation/admin-guide/sysctl/kernel.rst
+++ b/Documentation/admin-guide/sysctl/kernel.rst
@@ -616,6 +616,24 @@ numa_balancing_scan_delay_ms, numa_balancing_scan_period_max_ms,
 numa_balancing_scan_size_mb`_, and numa_balancing_settle_count sysctls.
 
 
+By default, NUMA hinting faults are generate on both toptier and non-toptier
+nodes. However, in a tiered-memory system, hot memories in toptier memory nodes
+may not need to be migrated around. In such cases, it's unnecessary to scan the
+pages in the toptier memory nodes. For a tiered-memory system, unnecessary scannings
+and hinting faults in the toptier nodes are disabled.
+
+This interface takes bits field as input. Supported values and corresponding modes are
+as follow:
+
+- 0x0: NUMA_BALANCING_DISABLED
+- 0x1: NUMA_BALANCING_NORMAL
+- 0x2: NUMA_BALANCING_TIERED_MEMORY
+
+If a system has single toptier node online, then default NUMA balancing will
+automatically be downgraded to the tiered-memory mode to avoid the unnecessary scanning
+and hinting faults.
+
+
 numa_balancing_scan_period_min_ms, numa_balancing_scan_delay_ms, numa_balancing_scan_period_max_ms, numa_balancing_scan_size_mb
 ===============================================================================================================================
 
diff --git a/Documentation/admin-guide/sysctl/vm.rst b/Documentation/admin-guide/sysctl/vm.rst
index f4804ce37c58..62aa687c325e 100644
--- a/Documentation/admin-guide/sysctl/vm.rst
+++ b/Documentation/admin-guide/sysctl/vm.rst
@@ -73,6 +73,7 @@ Currently, these files are in /proc/sys/vm:
 - vfs_cache_pressure
 - watermark_boost_factor
 - watermark_scale_factor
+- demote_scale_factor
 - zone_reclaim_mode
 
 
@@ -956,6 +957,17 @@ that the number of free pages kswapd maintains for latency reasons is
 too small for the allocation bursts occurring in the system. This knob
 can then be used to tune kswapd aggressiveness accordingly.
 
+demote_scale_factor
+===================
+
+This factor controls when kswapd wakes up to demote pages from toptier
+nodes. It defines the amount of memory left in a toptier node/system
+before kswapd is woken up and how much memory needs to be free from those
+nodes before kswapd goes back to sleep.
+
+The unit is in fractions of 10,000. The default value of 200 means if there
+are less than 2% of free toptier memory in a node/system, we will start  to
+demote pages from that node.
 
 zone_reclaim_mode
 =================
diff --git a/arch/x86/boot/compressed/Makefile b/arch/x86/boot/compressed/Makefile
index 15c5ae62a0e9..9458ab2cca4f 100644
--- a/arch/x86/boot/compressed/Makefile
+++ b/arch/x86/boot/compressed/Makefile
@@ -140,7 +140,7 @@ $(obj)/vmlinux.bin.lzo: $(vmlinux.bin.all-y) FORCE
 $(obj)/vmlinux.bin.lz4: $(vmlinux.bin.all-y) FORCE
 	$(call if_changed,lz4)
 $(obj)/vmlinux.bin.zst: $(vmlinux.bin.all-y) FORCE
-	$(call if_changed,zstd22)
+	$(call if_changed,zstd)
 
 suffix-$(CONFIG_KERNEL_GZIP)	:= gz
 suffix-$(CONFIG_KERNEL_BZIP2)	:= bz2
diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index 18b5500ea8bf..5ad0fe12edd8 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -370,6 +370,8 @@
 446	common	landlock_restrict_self	sys_landlock_restrict_self
 447	common	memfd_secret		sys_memfd_secret
 448	common	process_mrelease	sys_process_mrelease
+449	common	htmm_start		sys_htmm_start
+450	common	htmm_end		sys_htmm_end
 
 #
 # Due to a historical design error, certain syscalls are numbered differently
diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
index 448cd01eb3ec..3d788613ac79 100644
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@ -722,6 +722,17 @@ static inline pgd_t pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd)
 #include <linux/log2.h>
 #include <asm/fixmap.h>
 
+#ifdef CONFIG_HTMM
+static inline pginfo_t *get_pginfo_from_pte(pte_t *pte)
+{
+	struct page *page = virt_to_page((unsigned long)pte);
+	unsigned long idx;
+
+	idx = ((unsigned long)(pte) & ~PAGE_MASK) / 8;
+	return &page->pginfo[idx];
+}
+#endif
+
 static inline int pte_none(pte_t pte)
 {
 	return !(pte.pte & ~(_PAGE_KNL_ERRATUM_MASK));
diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h
index dd520b44e89c..1ca5b02bc637 100644
--- a/arch/x86/include/asm/pgtable_64.h
+++ b/arch/x86/include/asm/pgtable_64.h
@@ -266,6 +266,10 @@ static inline bool gup_fast_permitted(unsigned long start, unsigned long end)
 	return true;
 }
 
+#ifdef CONFIG_HTMM /* extern pginfo_cache */
+extern struct kmem_cache *pginfo_cache;
+#endif
+
 #include <asm/pgtable-invert.h>
 
 #endif /* !__ASSEMBLY__ */
diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h
index 28e59576c75b..f22625ea876f 100644
--- a/arch/x86/include/asm/pgtable_types.h
+++ b/arch/x86/include/asm/pgtable_types.h
@@ -282,6 +282,15 @@ typedef struct pgprot { pgprotval_t pgprot; } pgprot_t;
 
 typedef struct { pgdval_t pgd; } pgd_t;
 
+#ifdef CONFIG_HTMM /* pginfo_t */
+typedef struct {
+	uint32_t total_accesses;
+	uint16_t nr_accesses;
+	uint8_t cooling_clock;
+	bool may_hot;
+} pginfo_t;
+#endif
+
 static inline pgprot_t pgprot_nx(pgprot_t prot)
 {
 	return __pgprot(pgprot_val(prot) | _PAGE_NX);
diff --git a/arch/x86/kernel/ftrace_32.S b/arch/x86/kernel/ftrace_32.S
index a0ed0e4a2c0c..7611374ccce8 100644
--- a/arch/x86/kernel/ftrace_32.S
+++ b/arch/x86/kernel/ftrace_32.S
@@ -184,6 +184,7 @@ SYM_CODE_END(ftrace_graph_caller)
 return_to_handler:
 	pushl	%eax
 	pushl	%edx
+	movl	%eax, %edx	#  2nd argument: the return value
 	movl	$0, %eax
 	call	ftrace_return_to_handler
 	movl	%eax, %ecx
diff --git a/arch/x86/kernel/ftrace_64.S b/arch/x86/kernel/ftrace_64.S
index 6cc14a835991..f759e87c4728 100644
--- a/arch/x86/kernel/ftrace_64.S
+++ b/arch/x86/kernel/ftrace_64.S
@@ -340,6 +340,8 @@ SYM_FUNC_START(return_to_handler)
 	movq %rax, (%rsp)
 	movq %rdx, 8(%rsp)
 	movq %rbp, %rdi
+	/* Pass the return value to ftrace_return_to_handler */
+	movq %rax, %rsi
 
 	call ftrace_return_to_handler
 
diff --git a/arch/x86/mm/init_64.c b/arch/x86/mm/init_64.c
index 200ad5ceeb43..1f0dc2d0aa59 100644
--- a/arch/x86/mm/init_64.c
+++ b/arch/x86/mm/init_64.c
@@ -38,6 +38,7 @@
 #include <asm/processor.h>
 #include <asm/bios_ebda.h>
 #include <linux/uaccess.h>
+#include <asm/pgtable.h>
 #include <asm/pgalloc.h>
 #include <asm/dma.h>
 #include <asm/fixmap.h>
@@ -1703,3 +1704,17 @@ void __meminit vmemmap_populate_print_last(void)
 	}
 }
 #endif
+
+#ifdef CONFIG_HTMM
+struct kmem_cache *pginfo_cache;
+
+static int __init pginfo_cache_init(void)
+{
+	pginfo_cache =
+		kmem_cache_create("pginfo", sizeof(pginfo_t) * 512,
+				  sizeof(pginfo_t) * 512, SLAB_PANIC, NULL);
+	return 0;
+}
+core_initcall(pginfo_cache_init);
+
+#endif
diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c
index 3481b35cb4ec..16a30fd2ca36 100644
--- a/arch/x86/mm/pgtable.c
+++ b/arch/x86/mm/pgtable.c
@@ -28,9 +28,26 @@ void paravirt_tlb_remove_table(struct mmu_gather *tlb, void *table)
 
 gfp_t __userpte_alloc_gfp = GFP_PGTABLE_USER | PGTABLE_HIGHMEM;
 
+#ifdef CONFIG_HTMM
+static void __pte_alloc_pginfo(struct page *page)
+{
+	/* __userpte_alloc_gfp contains __GFP_ZERO */
+	page->pginfo = kmem_cache_alloc(pginfo_cache, __userpte_alloc_gfp);
+	if (page->pginfo)
+		SetPageHtmm(page);
+}
+#endif
 pgtable_t pte_alloc_one(struct mm_struct *mm)
 {
-	return __pte_alloc_one(mm, __userpte_alloc_gfp);
+	struct page *pgtable;
+
+	pgtable = __pte_alloc_one(mm, __userpte_alloc_gfp);
+#ifdef CONFIG_HTMM
+	if (mm->htmm_enabled) {
+		__pte_alloc_pginfo(pgtable);
+	}
+#endif
+	return pgtable;
 }
 
 static int __init setup_userpte(char *arg)
@@ -52,6 +69,9 @@ early_param("userpte", setup_userpte);
 
 void ___pte_free_tlb(struct mmu_gather *tlb, struct page *pte)
 {
+#ifdef CONFIG_HTMM
+	free_pginfo_pte(pte);
+#endif
 	pgtable_pte_page_dtor(pte);
 	paravirt_release_pte(page_to_pfn(pte));
 	paravirt_tlb_remove_table(tlb, pte);
diff --git a/drivers/dma/ioat/dma.c b/drivers/dma/ioat/dma.c
index e2070df6cad2..0b846c605d4b 100644
--- a/drivers/dma/ioat/dma.c
+++ b/drivers/dma/ioat/dma.c
@@ -584,11 +584,11 @@ desc_get_errstat(struct ioatdma_chan *ioat_chan, struct ioat_ring_ent *desc)
 }
 
 /**
- * __cleanup - reclaim used descriptors
+ * __ioat_cleanup - reclaim used descriptors
  * @ioat_chan: channel (ring) to clean
  * @phys_complete: zeroed (or not) completion address (from status)
  */
-static void __cleanup(struct ioatdma_chan *ioat_chan, dma_addr_t phys_complete)
+static void __ioat_cleanup(struct ioatdma_chan *ioat_chan, dma_addr_t phys_complete)
 {
 	struct ioatdma_device *ioat_dma = ioat_chan->ioat_dma;
 	struct ioat_ring_ent *desc;
@@ -675,7 +675,7 @@ static void ioat_cleanup(struct ioatdma_chan *ioat_chan)
 	spin_lock_bh(&ioat_chan->cleanup_lock);
 
 	if (ioat_cleanup_preamble(ioat_chan, &phys_complete))
-		__cleanup(ioat_chan, phys_complete);
+		__ioat_cleanup(ioat_chan, phys_complete);
 
 	if (is_ioat_halted(*ioat_chan->completion)) {
 		u32 chanerr = readl(ioat_chan->reg_base + IOAT_CHANERR_OFFSET);
@@ -712,7 +712,7 @@ static void ioat_restart_channel(struct ioatdma_chan *ioat_chan)
 
 	ioat_quiesce(ioat_chan, 0);
 	if (ioat_cleanup_preamble(ioat_chan, &phys_complete))
-		__cleanup(ioat_chan, phys_complete);
+		__ioat_cleanup(ioat_chan, phys_complete);
 
 	__ioat_restart_chan(ioat_chan);
 }
@@ -786,7 +786,7 @@ static void ioat_eh(struct ioatdma_chan *ioat_chan)
 
 	/* cleanup so tail points to descriptor that caused the error */
 	if (ioat_cleanup_preamble(ioat_chan, &phys_complete))
-		__cleanup(ioat_chan, phys_complete);
+		__ioat_cleanup(ioat_chan, phys_complete);
 
 	chanerr = readl(ioat_chan->reg_base + IOAT_CHANERR_OFFSET);
 	pci_read_config_dword(pdev, IOAT_PCI_CHANERR_INT_OFFSET, &chanerr_int);
@@ -943,7 +943,7 @@ void ioat_timer_event(struct timer_list *t)
 		/* timer restarted in ioat_cleanup_preamble
 		 * and IOAT_COMPLETION_ACK cleared
 		 */
-		__cleanup(ioat_chan, phys_complete);
+		__ioat_cleanup(ioat_chan, phys_complete);
 		goto unlock_out;
 	}
 
diff --git a/include/asm-generic/pgalloc.h b/include/asm-generic/pgalloc.h
index 02932efad3ab..57b47f9df2c8 100644
--- a/include/asm-generic/pgalloc.h
+++ b/include/asm-generic/pgalloc.h
@@ -99,6 +99,9 @@ static inline pgtable_t pte_alloc_one(struct mm_struct *mm)
 static inline void pte_free(struct mm_struct *mm, struct page *pte_page)
 {
 	pgtable_pte_page_dtor(pte_page);
+#ifdef CONFIG_HTMM
+	free_pginfo_pte(pte_page);
+#endif
 	__free_page(pte_page);
 }
 
diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index cd8b8bd5ec4d..4457fe123494 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -617,6 +617,9 @@ struct cftype {
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lock_class_key	lockdep_key;
 #endif
+#ifdef CONFIG_HTMM /* struct cftype */
+	int numa_node_id;
+#endif
 };
 
 /*
diff --git a/include/linux/cleanup.h b/include/linux/cleanup.h
new file mode 100644
index 000000000000..c2d09bc4f976
--- /dev/null
+++ b/include/linux/cleanup.h
@@ -0,0 +1,250 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __LINUX_GUARDS_H
+#define __LINUX_GUARDS_H
+
+#include <linux/compiler.h>
+
+/*
+ * DEFINE_FREE(name, type, free):
+ *	simple helper macro that defines the required wrapper for a __free()
+ *	based cleanup function. @free is an expression using '_T' to access the
+ *	variable. @free should typically include a NULL test before calling a
+ *	function, see the example below.
+ *
+ * __free(name):
+ *	variable attribute to add a scoped based cleanup to the variable.
+ *
+ * no_free_ptr(var):
+ *	like a non-atomic xchg(var, NULL), such that the cleanup function will
+ *	be inhibited -- provided it sanely deals with a NULL value.
+ *
+ *	NOTE: this has __must_check semantics so that it is harder to accidentally
+ *	leak the resource.
+ *
+ * return_ptr(p):
+ *	returns p while inhibiting the __free().
+ *
+ * Ex.
+ *
+ * DEFINE_FREE(kfree, void *, if (_T) kfree(_T))
+ *
+ * void *alloc_obj(...)
+ * {
+ *	struct obj *p __free(kfree) = kmalloc(...);
+ *	if (!p)
+ *		return NULL;
+ *
+ *	if (!init_obj(p))
+ *		return NULL;
+ *
+ *	return_ptr(p);
+ * }
+ *
+ * NOTE: the DEFINE_FREE()'s @free expression includes a NULL test even though
+ * kfree() is fine to be called with a NULL value. This is on purpose. This way
+ * the compiler sees the end of our alloc_obj() function as:
+ *
+ *	tmp = p;
+ *	p = NULL;
+ *	if (p)
+ *		kfree(p);
+ *	return tmp;
+ *
+ * And through the magic of value-propagation and dead-code-elimination, it
+ * eliminates the actual cleanup call and compiles into:
+ *
+ *	return p;
+ *
+ * Without the NULL test it turns into a mess and the compiler can't help us.
+ */
+
+#define DEFINE_FREE(_name, _type, _free) \
+	static inline void __free_##_name(void *p) { _type _T = *(_type *)p; _free; }
+
+#define __free(_name)	__cleanup(__free_##_name)
+
+#define __get_and_null_ptr(p) \
+	({ __auto_type __ptr = &(p); \
+	   __auto_type __val = *__ptr; \
+	   *__ptr = NULL;  __val; })
+
+static inline __must_check
+const volatile void * __must_check_fn(const volatile void *val)
+{ return val; }
+
+#define no_free_ptr(p) \
+	((typeof(p)) __must_check_fn(__get_and_null_ptr(p)))
+
+#define return_ptr(p)	return no_free_ptr(p)
+
+
+/*
+ * DEFINE_CLASS(name, type, exit, init, init_args...):
+ *	helper to define the destructor and constructor for a type.
+ *	@exit is an expression using '_T' -- similar to FREE above.
+ *	@init is an expression in @init_args resulting in @type
+ *
+ * EXTEND_CLASS(name, ext, init, init_args...):
+ *	extends class @name to @name@ext with the new constructor
+ *
+ * CLASS(name, var)(args...):
+ *	declare the variable @var as an instance of the named class
+ *
+ * Ex.
+ *
+ * DEFINE_CLASS(fdget, struct fd, fdput(_T), fdget(fd), int fd)
+ *
+ *	CLASS(fdget, f)(fd);
+ *	if (!f.file)
+ *		return -EBADF;
+ *
+ *	// use 'f' without concern
+ */
+
+#define DEFINE_CLASS(_name, _type, _exit, _init, _init_args...)		\
+typedef _type class_##_name##_t;					\
+static inline void class_##_name##_destructor(_type *p)			\
+{ _type _T = *p; _exit; }						\
+static inline _type class_##_name##_constructor(_init_args)		\
+{ _type t = _init; return t; }
+
+#define EXTEND_CLASS(_name, ext, _init, _init_args...)			\
+typedef class_##_name##_t class_##_name##ext##_t;			\
+static inline void class_##_name##ext##_destructor(class_##_name##_t *p)\
+{ class_##_name##_destructor(p); }					\
+static inline class_##_name##_t class_##_name##ext##_constructor(_init_args) \
+{ class_##_name##_t t = _init; return t; }
+
+#define CLASS(_name, var)						\
+	class_##_name##_t var __cleanup(class_##_name##_destructor) =	\
+		class_##_name##_constructor
+
+
+/*
+ * DEFINE_GUARD(name, type, lock, unlock):
+ *	trivial wrapper around DEFINE_CLASS() above specifically
+ *	for locks.
+ *
+ * DEFINE_GUARD_COND(name, ext, condlock)
+ *	wrapper around EXTEND_CLASS above to add conditional lock
+ *	variants to a base class, eg. mutex_trylock() or
+ *	mutex_lock_interruptible().
+ *
+ * guard(name):
+ *	an anonymous instance of the (guard) class, not recommended for
+ *	conditional locks.
+ *
+ * scoped_guard (name, args...) { }:
+ *	similar to CLASS(name, scope)(args), except the variable (with the
+ *	explicit name 'scope') is declard in a for-loop such that its scope is
+ *	bound to the next (compound) statement.
+ *
+ *	for conditional locks the loop body is skipped when the lock is not
+ *	acquired.
+ *
+ * scoped_cond_guard (name, fail, args...) { }:
+ *      similar to scoped_guard(), except it does fail when the lock
+ *      acquire fails.
+ *
+ */
+
+#define DEFINE_GUARD(_name, _type, _lock, _unlock) \
+	DEFINE_CLASS(_name, _type, if (_T) { _unlock; }, ({ _lock; _T; }), _type _T); \
+	static inline void * class_##_name##_lock_ptr(class_##_name##_t *_T) \
+	{ return *_T; }
+
+#define DEFINE_GUARD_COND(_name, _ext, _condlock) \
+	EXTEND_CLASS(_name, _ext, \
+		     ({ void *_t = _T; if (_T && !(_condlock)) _t = NULL; _t; }), \
+		     class_##_name##_t _T) \
+	static inline void * class_##_name##_ext##_lock_ptr(class_##_name##_t *_T) \
+	{ return class_##_name##_lock_ptr(_T); }
+
+#define guard(_name) \
+	CLASS(_name, __UNIQUE_ID(guard))
+
+#define __guard_ptr(_name) class_##_name##_lock_ptr
+
+#define scoped_guard(_name, args...)					\
+	for (CLASS(_name, scope)(args),					\
+	     *done = NULL; __guard_ptr(_name)(&scope) && !done; done = (void *)1)
+
+#define scoped_cond_guard(_name, _fail, args...) \
+	for (CLASS(_name, scope)(args), \
+	     *done = NULL; !done; done = (void *)1) \
+		if (!__guard_ptr(_name)(&scope)) _fail; \
+		else
+
+/*
+ * Additional helper macros for generating lock guards with types, either for
+ * locks that don't have a native type (eg. RCU, preempt) or those that need a
+ * 'fat' pointer (eg. spin_lock_irqsave).
+ *
+ * DEFINE_LOCK_GUARD_0(name, lock, unlock, ...)
+ * DEFINE_LOCK_GUARD_1(name, type, lock, unlock, ...)
+ * DEFINE_LOCK_GUARD_1_COND(name, ext, condlock)
+ *
+ * will result in the following type:
+ *
+ *   typedef struct {
+ *	type *lock;		// 'type := void' for the _0 variant
+ *	__VA_ARGS__;
+ *   } class_##name##_t;
+ *
+ * As above, both _lock and _unlock are statements, except this time '_T' will
+ * be a pointer to the above struct.
+ */
+
+#define __DEFINE_UNLOCK_GUARD(_name, _type, _unlock, ...)		\
+typedef struct {							\
+	_type *lock;							\
+	__VA_ARGS__;							\
+} class_##_name##_t;							\
+									\
+static inline void class_##_name##_destructor(class_##_name##_t *_T)	\
+{									\
+	if (_T->lock) { _unlock; }					\
+}									\
+									\
+static inline void *class_##_name##_lock_ptr(class_##_name##_t *_T)	\
+{									\
+	return _T->lock;						\
+}
+
+
+#define __DEFINE_LOCK_GUARD_1(_name, _type, _lock)			\
+static inline class_##_name##_t class_##_name##_constructor(_type *l)	\
+{									\
+	class_##_name##_t _t = { .lock = l }, *_T = &_t;		\
+	_lock;								\
+	return _t;							\
+}
+
+#define __DEFINE_LOCK_GUARD_0(_name, _lock)				\
+static inline class_##_name##_t class_##_name##_constructor(void)	\
+{									\
+	class_##_name##_t _t = { .lock = (void*)1 },			\
+			 *_T __maybe_unused = &_t;			\
+	_lock;								\
+	return _t;							\
+}
+
+#define DEFINE_LOCK_GUARD_1(_name, _type, _lock, _unlock, ...)		\
+__DEFINE_UNLOCK_GUARD(_name, _type, _unlock, __VA_ARGS__)		\
+__DEFINE_LOCK_GUARD_1(_name, _type, _lock)
+
+#define DEFINE_LOCK_GUARD_0(_name, _lock, _unlock, ...)			\
+__DEFINE_UNLOCK_GUARD(_name, void, _unlock, __VA_ARGS__)		\
+__DEFINE_LOCK_GUARD_0(_name, _lock)
+
+#define DEFINE_LOCK_GUARD_1_COND(_name, _ext, _condlock)		\
+	EXTEND_CLASS(_name, _ext,					\
+		     ({ class_##_name##_t _t = { .lock = l }, *_T = &_t;\
+		        if (_T->lock && !(_condlock)) _T->lock = NULL;	\
+			_t; }),						\
+		     typeof_member(class_##_name##_t, lock) l)		\
+	static inline void * class_##_name##_ext##_lock_ptr(class_##_name##_t *_T) \
+	{ return class_##_name##_lock_ptr(_T); }
+
+
+#endif /* __LINUX_GUARDS_H */
diff --git a/include/linux/compiler-clang.h b/include/linux/compiler-clang.h
index 3c4de9b6c6e3..31cc343864fb 100644
--- a/include/linux/compiler-clang.h
+++ b/include/linux/compiler-clang.h
@@ -5,6 +5,15 @@
 
 /* Compiler specific definitions for Clang compiler */
 
+/*
+ * Clang prior to 17 is being silly and considers many __cleanup() variables
+ * as unused (because they are, their sole purpose is to go out of scope).
+ *
+ * https://github.com/llvm/llvm-project/commit/877210faa447f4cc7db87812f8ed80e398fedd61
+ */
+#undef __cleanup
+#define __cleanup(func) __maybe_unused __attribute__((__cleanup__(func)))
+
 /* same as gcc, this was present in clang-2.6 so we can assume it works
  * with any version that can compile the kernel
  */
diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index afc678d7fc86..340900649de3 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -902,6 +902,7 @@ struct ftrace_graph_ent {
  */
 struct ftrace_graph_ret {
 	unsigned long func; /* Current function */
+	unsigned long retval;
 	int depth;
 	/* Number of functions that overran the depth limit for current task */
 	unsigned int overrun;
diff --git a/include/linux/htmm.h b/include/linux/htmm.h
new file mode 100644
index 000000000000..57229408d2c0
--- /dev/null
+++ b/include/linux/htmm.h
@@ -0,0 +1,224 @@
+#ifndef _LINUX_HTMM_H
+#define _LINUX_HTMM_H
+#include <linux/mm.h>
+#include <linux/errname.h>
+#include <uapi/linux/perf_event.h>
+
+#define DEFERRED_SPLIT_ISOLATED 1
+
+#define BUFFER_SIZE 32 /* 128: 1MB */
+#define HTMM_CPUS num_online_cpus()
+#define CPU_NODE first_node(node_states[N_CPU])
+#define MAX_MIGRATION_RATE_IN_MBPS 2048 /* 2048MB per sec */
+
+/* pebs events */
+#define DRAM_LLC_LOAD_MISS 0x1d3
+#define REMOTE_DRAM_LLC_LOAD_MISS 0x2d3
+#define NVM_LLC_LOAD_MISS 0x80d1
+#define ALL_STORES 0x82d0
+#define ALL_LOADS 0x81d0
+#define STLB_MISS_STORES 0x12d0
+#define STLB_MISS_LOADS 0x11d0
+
+/* tmm option */
+#define HTMM_NO_MIG 0x0 /* unused */
+#define HTMM_BASELINE 0x1 /* unused */
+#define HTMM_HUGEPAGE_OPT 0x2 /* only used */
+#define HTMM_HUGEPAGE_OPT_V2 0x3 /* unused */
+
+/**/
+#define DRAM_ACCESS_LATENCY 80
+#define NVM_ACCESS_LATENCY 270
+#define CXL_ACCESS_LATENCY 170
+#define DELTA_CYCLES (NVM_ACCESS_LATENCY - DRAM_ACCESS_LATENCY)
+
+/* only prime numbers */
+static const unsigned int pebs_period_list[] = {
+	199, // 200 - min
+	293, // 300
+	401, // 400
+	499, // 500
+	599, // 600
+	701, // 700
+	797, // 800
+	907, // 900
+	997, // 1000
+	1201, // 1200
+	1399, // 1400
+	1601, // 1600
+	1801, // 1800
+	1999, // 2000
+	2503, // 2500
+	3001, // 3000
+	3499, // 3500
+	4001, // 4000
+	4507, // 4507
+	4999, // 5000
+	6007, // 6000
+	7001, // 7000
+	7993, // 8000
+	9001, // 9000
+	10007, // 10000
+	12007, // 12000
+	13999, // 14000
+	16001, // 16000
+	17989, // 18000
+	19997, // 20000 - max
+};
+
+/* this is for store instructions */
+static const unsigned int pebs_inst_period_list[] = {
+	100003, // 0.1M
+	300007, // 0.3M
+	600011, // 0.6M
+	1000003, // 1.0M
+	1500003, // 1.5M
+};
+
+struct htmm_event {
+	struct perf_event_header header;
+	__u64 ip;
+	__u32 pid, tid;
+	__u64 addr;
+};
+
+enum events {
+	DRAMREAD = 0,
+	NVMREAD = 1,
+	MEMWRITE = 2,
+	TLB_MISS_LOADS = 3,
+	TLB_MISS_STORES = 4,
+	CXLREAD = 5, // emulated by remote DRAM node
+	N_HTMMEVENTS
+};
+
+/* htmm_core.c */
+extern void htmm_mm_init(struct mm_struct *mm);
+extern void htmm_mm_exit(struct mm_struct *mm);
+extern void __prep_transhuge_page_for_htmm(struct mm_struct *mm, struct page *page);
+extern void prep_transhuge_page_for_htmm(struct vm_area_struct *vma,
+					 struct page *page);
+extern void clear_transhuge_pginfo(struct page *page);
+extern void copy_transhuge_pginfo(struct page *page, struct page *newpage);
+extern pginfo_t *get_compound_pginfo(struct page *page, unsigned long address);
+
+extern void check_transhuge_cooling(void *arg, struct page *page, bool locked);
+extern void check_base_cooling(pginfo_t *pginfo, struct page *page, bool locked);
+extern int set_page_coolstatus(struct page *page, pte_t *pte, struct mm_struct *mm);
+
+extern void set_lru_adjusting(struct mem_cgroup *memcg, bool inc_thres);
+
+extern void update_pginfo(pid_t pid, unsigned long address, enum events e);
+
+extern bool deferred_split_huge_page_for_htmm(struct page *page);
+extern unsigned long
+deferred_split_scan_for_htmm(struct mem_cgroup_per_node *pn,
+			     struct list_head *split_list);
+extern void putback_split_pages(struct list_head *split_list, struct lruvec *lruvec);
+
+extern bool check_split_huge_page(struct mem_cgroup *memcg, struct page *meta, bool hot);
+extern bool move_page_to_deferred_split_queue(struct mem_cgroup *memcg, struct page *page);
+
+extern void move_page_to_active_lru(struct page *page);
+extern void move_page_to_inactive_lru(struct page *page);
+
+extern struct page *get_meta_page(struct page *page);
+extern unsigned int get_accesses_from_idx(unsigned int idx);
+extern unsigned int get_idx(unsigned long num);
+extern int get_skew_idx(unsigned long num);
+extern void uncharge_htmm_pte(pte_t *pte, struct mem_cgroup *memcg);
+extern void uncharge_htmm_page(struct page *page, struct mem_cgroup *memcg);
+extern void charge_htmm_page(struct page *page, struct mem_cgroup *memcg);
+
+extern void set_lru_split_pid(pid_t pid);
+extern void adjust_active_threshold(pid_t pid);
+extern void set_lru_cooling_pid(pid_t pid);
+
+/* htmm_sampler.c */
+extern int ksamplingd_init(pid_t pid, int node);
+extern void ksamplingd_exit(void);
+
+static inline unsigned long get_sample_period(unsigned long cur)
+{
+	ulong len = ARRAY_SIZE(pebs_period_list);
+	if (cur < 0)
+		return 0;
+	return pebs_period_list[cur < len ? cur : len - 1];
+}
+
+static inline unsigned long get_sample_inst_period(unsigned long cur)
+{
+	ulong len = ARRAY_SIZE(pebs_inst_period_list);
+	if (cur < 0)
+		return 0;
+	return pebs_inst_period_list[cur < len ? cur : len - 1];
+}
+#if 1
+static inline void increase_sample_period(unsigned long *llc_period,
+					  unsigned long *inst_period)
+{
+	unsigned long p;
+	p = *llc_period;
+	if (++p < ARRAY_SIZE(pebs_period_list))
+		*llc_period = p;
+
+	p = *inst_period;
+	if (++p < ARRAY_SIZE(pebs_inst_period_list))
+		*inst_period = p;
+}
+
+static inline void decrease_sample_period(unsigned long *llc_period,
+					  unsigned long *inst_period)
+{
+	unsigned long p;
+	p = *llc_period;
+	if (p > 0)
+		*llc_period = p - 1;
+
+	p = *inst_period;
+	if (p > 0)
+		*inst_period = p - 1;
+}
+#else
+static inline unsigned int increase_sample_period(unsigned int cur,
+						  unsigned int next)
+{
+	ulong len = ARRAY_SIZE(pebs_period_list);
+	do {
+		cur++;
+	} while (pebs_period_list[cur] < next && cur < len);
+
+	return cur < len ? cur : len - 1;
+}
+
+static inline unsigned int decrease_sample_period(unsigned int cur,
+						  unsigned int next)
+{
+	do {
+		cur--;
+	} while (pebs_period_list[cur] > next && cur > 0);
+
+	return cur;
+}
+#endif
+
+/* htmm_migrater.c */
+#define HTMM_MIN_FREE_PAGES 256 * 10 // 10MB
+extern unsigned long get_nr_lru_pages_node(struct mem_cgroup *memcg, pg_data_t *pgdat);
+extern void add_memcg_to_kmigraterd(struct mem_cgroup *memcg, int nid);
+extern void del_memcg_from_kmigraterd(struct mem_cgroup *memcg, int nid);
+extern unsigned long get_memcg_demotion_watermark(unsigned long max_nr_pages);
+extern unsigned long get_memcg_promotion_watermark(unsigned long max_nr_pages);
+extern void kmigraterd_wakeup(int nid);
+extern int kmigraterd_init(void);
+extern void kmigraterd_stop(void);
+extern bool __isolate_lru_page_prepare(struct page *page, isolate_mode_t mode);
+
+struct htmm_rmap_walk_arg {
+	void *arg;
+	bool unmap_clean;
+};
+extern bool try_to_unmap_clean(struct page_vma_mapped_walk *pvmw,
+			       struct page *page);
+
+#endif
diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h
index f123e15d966e..5b78b9d1ac76 100644
--- a/include/linux/huge_mm.h
+++ b/include/linux/huge_mm.h
@@ -327,6 +327,10 @@ static inline struct list_head *page_deferred_list(struct page *page)
 	return &page[2].deferred_list;
 }
 
+#ifdef CONFIG_MEMCG
+extern struct deferred_split *get_deferred_split_queue(struct page *page);
+#endif
+
 #else /* CONFIG_TRANSPARENT_HUGEPAGE */
 #define HPAGE_PMD_SHIFT ({ BUILD_BUG(); 0; })
 #define HPAGE_PMD_MASK ({ BUILD_BUG(); 0; })
diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h
index 94df87cb69c3..fd6f53ce3c2d 100644
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@ -141,7 +141,16 @@ struct mem_cgroup_per_node {
 	struct lruvec_stats			lruvec_stats;
 
 	unsigned long		lru_zone_size[MAX_NR_ZONES][NR_LRU_LISTS];
-
+#ifdef CONFIG_HTMM /* struct mem_cgroup_per_node */
+	unsigned long max_nr_base_pages; /* Set by "max_at_node" param */
+	struct list_head kmigraterd_list;
+	bool need_cooling;
+	bool need_adjusting;
+	bool need_adjusting_all;
+	bool need_demotion;
+	struct deferred_split deferred_split_queue;
+	struct list_head deferred_list;
+#endif
 	struct mem_cgroup_reclaim_iter	iter;
 
 	struct shrinker_info __rcu	*shrinker_info;
@@ -353,6 +362,48 @@ struct mem_cgroup {
 	struct deferred_split deferred_split_queue;
 #endif
 
+#ifdef CONFIG_HTMM /* struct mem_cgroup */
+	bool htmm_enabled;
+	unsigned long max_nr_dram_pages; /* the maximum number of pages */
+	unsigned long nr_active_pages; /* updated by need_lru_cooling() */
+	/* stat for sampled accesses */
+	unsigned long nr_sampled; /* the total number of sampled accesses */
+	unsigned long nr_sampled_for_split; /* nr_sampled for split decision */
+	unsigned long nr_dram_sampled; /* accesses to DRAM: n(i) */
+	unsigned long prev_dram_sampled; /* accesses to DRAM n(i-1) */
+	unsigned long max_dram_sampled; /* accesses to DRAM (estimated) */
+	unsigned long prev_max_dram_sampled; /* accesses to DRAM (estimated) */
+	unsigned long nr_max_sampled; /* the calibrated number of accesses to both DRAM and NVM */
+	/* thresholds */
+	unsigned int active_threshold; /* hot */
+	unsigned int warm_threshold;
+	unsigned int bp_active_threshold; /* expected threshold */
+	/* split */
+	unsigned int split_threshold;
+	unsigned int split_active_threshold;
+	unsigned int nr_split;
+	unsigned int nr_split_tail_idx;
+	/* used to calculated avg_samples_hp. see check_transhuge_cooling() */
+	unsigned int sum_util;
+	unsigned int num_util;
+	/*  */
+	unsigned long access_map[21];
+	/* histograms. exponential scale */
+	/* "hotness_map" is used to determine the hot page threshold.
+	 * "ebp_hotness_map" is used to accurately determine
+	 * the expected DRAM hit ratio when the system only uses 4KB (base) pages.
+	 */
+	unsigned long hotness_hg[16]; // page access histogram
+	unsigned long ebp_hotness_hg[16]; // expected bage page
+	/* lock for histogram */
+	spinlock_t access_lock;
+	/* etc */
+	bool cooled;
+	bool split_happen;
+	bool need_split;
+	unsigned int cooling_clock;
+	unsigned long nr_alloc;
+#endif /* CONFIG_HTMM */
 	struct mem_cgroup_per_node *nodeinfo[];
 };
 
@@ -1768,4 +1819,7 @@ static inline struct mem_cgroup *mem_cgroup_from_obj(void *p)
 
 #endif /* CONFIG_MEMCG_KMEM */
 
+#ifdef CONFIG_HTMM
+extern int mem_cgroup_per_node_htmm_init(void);
+#endif
 #endif /* _LINUX_MEMCONTROL_H */
diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 4091692bed8c..c0d728df70e9 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -142,6 +142,7 @@ extern void numa_default_policy(void);
 extern void numa_policy_init(void);
 extern void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new);
 extern void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new);
+extern void check_toptier_balanced(void);
 
 extern int huge_node(struct vm_area_struct *vma,
 				unsigned long addr, gfp_t gfp_flags,
@@ -181,17 +182,39 @@ extern void mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol);
 /* Check if a vma is migratable */
 extern bool vma_migratable(struct vm_area_struct *vma);
 
-extern int mpol_misplaced(struct page *, struct vm_area_struct *, unsigned long);
+extern int mpol_misplaced(struct page *, struct vm_area_struct *, unsigned long, int);
 extern void mpol_put_task_policy(struct task_struct *);
 
 extern bool numa_demotion_enabled;
+extern bool numa_promotion_tiered_enabled;
+
+#ifdef CONFIG_HTMM
+extern unsigned int htmm_sample_period;
+extern unsigned int htmm_inst_sample_period;
+extern unsigned int htmm_split_period;
+extern unsigned int htmm_thres_hot;
+extern unsigned int htmm_cooling_period;
+extern unsigned int htmm_adaptation_period;
+extern unsigned int ksampled_min_sample_ratio;
+extern unsigned int ksampled_max_sample_ratio;
+extern unsigned int htmm_demotion_period_in_ms;
+extern unsigned int htmm_promotion_period_in_ms;
+extern unsigned int htmm_thres_split;
+extern unsigned int htmm_nowarm;
+extern unsigned int htmm_util_weight;
+extern unsigned int htmm_gamma;
+extern unsigned int htmm_mode;
+extern bool htmm_cxl_mode;
+extern bool htmm_skip_cooling;
+extern unsigned int htmm_thres_cooling_alloc;
+extern unsigned int ksampled_soft_cpu_quota;
+#endif
 
 static inline bool mpol_is_preferred_many(struct mempolicy *pol)
 {
 	return  (pol->mode == MPOL_PREFERRED_MANY);
 }
 
-
 #else
 
 struct mempolicy {};
@@ -287,7 +310,7 @@ static inline int mpol_parse_str(char *str, struct mempolicy **mpol)
 #endif
 
 static inline int mpol_misplaced(struct page *page, struct vm_area_struct *vma,
-				 unsigned long address)
+				 unsigned long address, int flags)
 {
 	return -1; /* no node preference */
 }
@@ -301,7 +324,12 @@ static inline nodemask_t *policy_nodemask_current(gfp_t gfp)
 	return NULL;
 }
 
+static inline void check_toptier_balanced(void)
+{
+}
+
 #define numa_demotion_enabled	false
+#define numa_promotion_tiered_enabled	false
 
 static inline bool mpol_is_preferred_many(struct mempolicy *pol)
 {
diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index c8077e936691..a76334655521 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -51,6 +51,20 @@ extern int migrate_pages(struct list_head *l, new_page_t new, free_page_t free,
 extern struct page *alloc_migration_target(struct page *page, unsigned long private);
 extern int isolate_movable_page(struct page *page, isolate_mode_t mode);
 
+#ifdef CONFIG_NOMAD
+struct nomad_context;
+extern int nomad_demotion_migrate_pages(struct list_head *l, new_page_t new,
+				  free_page_t free, unsigned long private,
+				  enum migrate_mode mode, int reason,
+				  unsigned int *nr_succeeded);
+extern int nomad_transit_pages(struct list_head *l, new_page_t new,
+			       free_page_t free, unsigned long private,
+			       enum migrate_mode mode, int reason,
+			       unsigned int *nr_succeeded,
+			       unsigned int *nr_migrated,
+			       struct nomad_context *contxt);
+#endif
+
 extern void migrate_page_states(struct page *newpage, struct page *page);
 extern void migrate_page_copy(struct page *newpage, struct page *page);
 extern int migrate_huge_page_move_mapping(struct address_space *mapping,
@@ -173,6 +187,7 @@ int migrate_vma_setup(struct migrate_vma *args);
 void migrate_vma_pages(struct migrate_vma *migrate);
 void migrate_vma_finalize(struct migrate_vma *migrate);
 int next_demotion_node(int node);
+int next_promotion_node(int node);
 
 #else /* CONFIG_MIGRATION disabled: */
 
@@ -181,6 +196,12 @@ static inline int next_demotion_node(int node)
 	return NUMA_NO_NODE;
 }
 
+static inline int next_promotion_node(int node)
+
+{
+	return NUMA_NO_NODE;
+}
+
 #endif /* CONFIG_MIGRATION */
 
 #endif /* _LINUX_MIGRATE_H */
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 5692055f202c..7a2045460670 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -3239,6 +3239,10 @@ static inline bool debug_guardpage_enabled(void) { return false; }
 static inline bool page_is_guard(struct page *page) { return false; }
 #endif /* CONFIG_DEBUG_PAGEALLOC */
 
+#ifdef CONFIG_MIGRATION
+extern int demote_scale_factor;
+#endif
+
 #if MAX_NUMNODES > 1
 void __init setup_nr_node_ids(void);
 #else
@@ -3304,5 +3308,9 @@ static inline int seal_check_future_write(int seals, struct vm_area_struct *vma)
 	return 0;
 }
 
+#ifdef CONFIG_HTMM
+extern void free_pginfo_pte(struct page *pte);
+#endif
+
 #endif /* __KERNEL__ */
 #endif /* _LINUX_MM_H */
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 7f8ee09c711f..18cd4544dd38 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -159,10 +159,34 @@ struct page {
 			/* For both global and memcg */
 			struct list_head deferred_list;
 		};
+#ifdef CONFIG_HTMM
+		struct { /* Third tail page of compound page */
+			unsigned long __compound_pad_1; /* compound_head */
+			unsigned long total_accesses;
+			unsigned int hot_utils;
+			unsigned int skewness_idx; /* current hotness val */
+			unsigned int idx;
+#if 0
+			unsigned long acc_accesses;	/* prev hotness val */
+#endif
+			uint32_t cooling_clock;
+		};
+		struct { /* Fourth~ tail pages of compound page */
+			unsigned long ___compound_pad_1; /* compound_head */
+			pginfo_t compound_pginfo[4]; /* 32 bytes */
+		};
+#endif
 		struct {	/* Page table pages */
 			unsigned long _pt_pad_1;	/* compound_head */
 			pgtable_t pmd_huge_pte; /* protected by page->ptl */
+#ifdef CONFIG_HTMM
+			union {
+				pginfo_t *pginfo;
+				unsigned long _pt_pad_2;
+			};
+#else
 			unsigned long _pt_pad_2;	/* mapping */
+#endif
 			union {
 				struct mm_struct *pt_mm; /* x86 pgds only */
 				atomic_t pt_frag_refcount; /* powerpc */
@@ -580,8 +604,15 @@ struct mm_struct {
 #ifdef CONFIG_IOMMU_SUPPORT
 		u32 pasid;
 #endif
+
+#ifdef CONFIG_HTMM
+		bool htmm_enabled;
+#endif
 	} __randomize_layout;
 
+#ifdef CONFIG_NOMAD
+	unsigned long cpu_trap_nr;
+#endif
 	/*
 	 * The mm_cpumask needs to be at the end of mm_struct, because it
 	 * is dynamically sized based on nr_cpu_ids.
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 8b8349ffa1cd..d73a1c8f8602 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -276,6 +276,8 @@ enum lru_list {
 
 #define for_each_evictable_lru(lru) for (lru = 0; lru <= LRU_ACTIVE_FILE; lru++)
 
+#define for_each_active_lru(lru) for (lru = 1; lru <= LRU_ACTIVE_FILE; lru += 2)
+
 static inline bool is_file_lru(enum lru_list lru)
 {
 	return (lru == LRU_INACTIVE_FILE || lru == LRU_ACTIVE_FILE);
@@ -330,6 +332,7 @@ enum zone_watermarks {
 	WMARK_MIN,
 	WMARK_LOW,
 	WMARK_HIGH,
+	WMARK_DEMOTE,
 	NR_WMARK
 };
 
@@ -354,6 +357,7 @@ enum zone_watermarks {
 #define min_wmark_pages(z) (z->_watermark[WMARK_MIN] + z->watermark_boost)
 #define low_wmark_pages(z) (z->_watermark[WMARK_LOW] + z->watermark_boost)
 #define high_wmark_pages(z) (z->_watermark[WMARK_HIGH] + z->watermark_boost)
+#define demote_wmark_pages(z) (z->_watermark[WMARK_DEMOTE] + z->watermark_boost)
 #define wmark_pages(z, i) (z->_watermark[i] + z->watermark_boost)
 
 /* Fields and list protected by pagesets local_lock in page_alloc.c */
@@ -884,6 +888,13 @@ typedef struct pglist_data {
 	struct deferred_split deferred_split_queue;
 #endif
 
+#ifdef CONFIG_HTMM /* struct pglist_data */
+	struct cftype *memcg_htmm_file; /* max, terminate. */
+	struct task_struct *kmigraterd;
+	struct list_head kmigraterd_head;
+	spinlock_t kmigraterd_lock;
+	wait_queue_head_t kmigraterd_wait;
+#endif
 	/* Fields commonly accessed by the page reclaim scanner */
 
 	/*
@@ -937,6 +948,7 @@ bool zone_watermark_ok(struct zone *z, unsigned int order,
 		unsigned int alloc_flags);
 bool zone_watermark_ok_safe(struct zone *z, unsigned int order,
 		unsigned long mark, int highest_zoneidx);
+bool pgdat_toptier_balanced(pg_data_t *pgdat, int order, int zone_idx);
 /*
  * Memory initialization context, use to differentiate memory added by
  * the platform statically or via memory hotplug interface.
@@ -960,6 +972,8 @@ static inline struct pglist_data *lruvec_pgdat(struct lruvec *lruvec)
 #endif
 }
 
+extern unsigned long lruvec_lru_size(struct lruvec *lruvec, enum lru_list lru, int zone_idx);
+
 #ifdef CONFIG_HAVE_MEMORYLESS_NODES
 int local_memory_node(int node_id);
 #else
@@ -1063,6 +1077,8 @@ int min_free_kbytes_sysctl_handler(struct ctl_table *, int, void *, size_t *,
 		loff_t *);
 int watermark_scale_factor_sysctl_handler(struct ctl_table *, int, void *,
 		size_t *, loff_t *);
+int demote_scale_factor_sysctl_handler(struct ctl_table *, int, void __user *,
+		size_t *, loff_t *);
 extern int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES];
 int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int, void *,
 		size_t *, loff_t *);
diff --git a/include/linux/node.h b/include/linux/node.h
index 8e5a29897936..30ad88bd3130 100644
--- a/include/linux/node.h
+++ b/include/linux/node.h
@@ -181,4 +181,10 @@ static inline void register_hugetlbfs_with_node(node_registration_func_t reg,
 
 #define to_node(device) container_of(device, struct node, dev)
 
+static inline bool node_is_toptier(int node)
+{
+	// ideally, toptier nodes should be the memory with CPU.
+	// for now, just assume the first memory node is the toptier memory
+	return first_memory_node == node;
+}
 #endif /* _LINUX_NODE_H_ */
diff --git a/include/linux/nomad.h b/include/linux/nomad.h
new file mode 100644
index 000000000000..0eb6996487a0
--- /dev/null
+++ b/include/linux/nomad.h
@@ -0,0 +1,102 @@
+#ifndef _LINUX_NOMAD_H
+#define _LINUX_NOMAD_H
+#include <linux/rmap.h>
+#include <linux/types.h>
+#include <linux/pgtable.h>
+
+enum nomad_stat_async_promotion {
+	NOMAD_MODULE_NOT_ENABLED,
+	NOMAD_SYNC_SUCCESS,
+	NOMAD_SYNC_FAIL,
+	NOMAD_ASYNC_QUEUED_SUCCESS,
+	NOMAD_ASYNC_QUEUED_FAIL,
+};
+
+struct nomad_remap_status {
+	bool use_new_page;
+	pte_t old_pteval;
+	enum ttu_flags flags;
+	struct page *new_page;
+};
+
+struct demote_shadow_page_context;
+// TODO(lingfeng): when rmmod, how to safely reset the function pointer is not considered yet
+struct async_promote_ctrl {
+	int initialized;
+	/**
+	* @brief return the number of pages to be queued, if should not queue, return 0
+	*
+	*/
+	int (*queue_page_fault)(struct page *page, pte_t *ptep, int target_nid);
+
+	int (*link_shadow_page)(struct page *newpage, struct page *oldpage);
+	struct page *(*demote_shadow_page_find)(
+		struct page *page, struct demote_shadow_page_context *contxt);
+	bool (*demote_shadow_page_breakup)(
+		struct page *page, struct demote_shadow_page_context *contxt);
+	struct page *(*release_shadow_page)(struct page *page, void *private,
+					    bool in_fault);
+	unsigned long (*reclaim_page)(int nid, int nr_to_reclaim);
+};
+
+void async_mod_glob_init_set(void);
+bool async_mod_glob_inited(void);
+
+extern bool nomad_try_to_migrate(struct page *, enum ttu_flags flags,
+				 struct demote_shadow_page_context *context);
+extern bool nomad_try_to_remap(struct page *page,
+			       struct nomad_remap_status *arg);
+
+extern struct async_promote_ctrl async_mod_glob_ctrl;
+
+#define _PAGE_BIT_ORIGINALLY_WRITABLE 52 /* used for non-exclusive mapping */
+#define _PAGE_ORIGINALLY_WRITABLE (_AT(u64, 1) << _PAGE_BIT_ORIGINALLY_WRITABLE)
+
+static inline int pte_orig_writable(pte_t pte)
+{
+	return (native_pte_val(pte) & _PAGE_ORIGINALLY_WRITABLE) != 0;
+}
+
+static inline pte_t pte_mk_orig_writable(pte_t pte)
+{
+	return pte_set_flags(pte, _PAGE_ORIGINALLY_WRITABLE);
+}
+
+static inline pte_t pte_clear_orig_writable(pte_t pte)
+{
+	return pte_clear_flags(pte, _PAGE_ORIGINALLY_WRITABLE);
+}
+
+// used for shadow page demotion context
+struct demote_shadow_page_context {
+	// should we use shadow page? 1 for yes, 0 for no
+	int use_shadow_page;
+	// If we find one, increment this number by one. If shadow page is
+	// destroyed, then decrement by one. This is used for checking correctness.
+	int shadow_page_ref_num;
+	int traversed_mapping_num;
+	void *flags;
+	// Sometimes pte may be write protected eventhough
+	// the VMA is writable (not set by our solution). For this
+	// we record this situation, and recover mapped page PTE to writable when it
+	// gets demoted.
+	// Potential cases where shadow relation gets broken:
+	// 1. Writing to the page, this page may be forked, we break the relation when
+	// a WP fault happens
+	// 2. Demote a page, in this case we no longer need the shadow relation.
+	// Break it, and restore the PTE.
+	// 3. Free a page in userspace. Since shadow page goes with the page
+	// rather than the PTE, we break this relation when the page ref count gets 0.
+	bool was_writable_before_shadowed;
+	bool made_writable;
+	struct page *shadow_page;
+	// this is the page to be discarded, in the remapping phase of demotion
+	struct page *old_page;
+};
+
+struct nomad_context {
+	uint64_t transactional_migrate_success_nr;
+	uint64_t transactional_migrate_fail_nr;
+};
+
+#endif
diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index fbfd3fad48f2..1f09738c9a73 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -138,8 +138,19 @@ enum pageflags {
 #ifdef CONFIG_64BIT
 	PG_arch_2,
 #endif
+#ifdef CONFIG_NUMA_BALANCING
+	PG_demoted,
+#ifdef CONFIG_NOMAD
+	PG_numa_queued,
+	PG_shadowed,
+#endif
+#endif
 #ifdef CONFIG_KASAN_HW_TAGS
 	PG_skip_kasan_poison,
+#endif
+#ifdef CONFIG_HTMM
+	PG_htmm,
+	PG_needsplit,
 #endif
 	__NR_PAGEFLAGS,
 
@@ -457,12 +468,35 @@ TESTCLEARFLAG(Young, young, PF_ANY)
 PAGEFLAG(Idle, idle, PF_ANY)
 #endif
 
+#if defined(CONFIG_NUMA_BALANCING) && defined(CONFIG_64BIT)
+TESTPAGEFLAG(Demoted, demoted, PF_NO_TAIL)
+SETPAGEFLAG(Demoted, demoted, PF_NO_TAIL)
+TESTCLEARFLAG(Demoted, demoted, PF_NO_TAIL)
+#ifdef CONFIG_NOMAD
+TESTPAGEFLAG(PromQueued, numa_queued, PF_NO_TAIL)
+SETPAGEFLAG(PromQueued, numa_queued, PF_NO_TAIL)
+TESTCLEARFLAG(PromQueued, numa_queued, PF_NO_TAIL)
+CLEARPAGEFLAG(PromQueued, numa_queued, PF_NO_TAIL)
+TESTPAGEFLAG(Shadowed, shadowed, PF_NO_TAIL)
+SETPAGEFLAG(Shadowed, shadowed, PF_NO_TAIL)
+TESTCLEARFLAG(Shadowed, shadowed, PF_NO_TAIL)
+CLEARPAGEFLAG(Shadowed, shadowed, PF_NO_TAIL)
+#endif
+#endif
+
 #ifdef CONFIG_KASAN_HW_TAGS
 PAGEFLAG(SkipKASanPoison, skip_kasan_poison, PF_HEAD)
 #else
 PAGEFLAG_FALSE(SkipKASanPoison)
 #endif
 
+#ifdef CONFIG_HTMM
+PAGEFLAG(Htmm, htmm, PF_ANY)
+
+PAGEFLAG(NeedSplit, needsplit, PF_HEAD)
+TESTCLEARFLAG(NeedSplit, needsplit, PF_HEAD)
+#endif
+
 /*
  * PageReported() is used to track reported free pages within the Buddy
  * allocator. We can use the non-atomic version of the test and set
diff --git a/include/linux/page_ext.h b/include/linux/page_ext.h
index fabb2e1e087f..151257e30739 100644
--- a/include/linux/page_ext.h
+++ b/include/linux/page_ext.h
@@ -23,6 +23,9 @@ enum page_ext_flags {
 	PAGE_EXT_YOUNG,
 	PAGE_EXT_IDLE,
 #endif
+#if defined(CONFIG_NUMA_BALANCING) && !defined(CONFIG_64BIT)
+	PAGE_EXT_DEMOTED,
+#endif
 };
 
 /*
diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 200995c5210e..e99d558c67ff 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1646,4 +1646,10 @@ extern void __weak arch_perf_update_userpage(struct perf_event *event,
 extern __weak u64 arch_perf_get_page_size(struct mm_struct *mm, unsigned long addr);
 #endif
 
+#ifdef CONFIG_HTMM
+extern int htmm__perf_event_init(struct perf_event *event, unsigned long nr_pages);
+extern int htmm__perf_event_open(struct perf_event_attr *attr_ptr, pid_t pid,
+				 int cpu, int group_fd, unsigned long flags);
+#endif
+
 #endif /* _LINUX_PERF_EVENT_H */
diff --git a/include/linux/rmap.h b/include/linux/rmap.h
index c29d9c13378b..1cc637987510 100644
--- a/include/linux/rmap.h
+++ b/include/linux/rmap.h
@@ -193,7 +193,11 @@ static inline void page_dup_rmap(struct page *page, bool compound)
  */
 int page_referenced(struct page *, int is_locked,
 			struct mem_cgroup *memcg, unsigned long *vm_flags);
-
+#ifdef CONFIG_HTMM
+int cooling_page(struct page *page, struct mem_cgroup *memcg);
+int page_check_hotness(struct page *page, struct mem_cgroup *memcg);
+int get_pginfo_idx(struct page *page);
+#endif
 void try_to_migrate(struct page *page, enum ttu_flags flags);
 void try_to_unmap(struct page *, enum ttu_flags flags);
 
@@ -246,7 +250,8 @@ int page_mkclean(struct page *);
  */
 void page_mlock(struct page *page);
 
-void remove_migration_ptes(struct page *old, struct page *new, bool locked);
+void remove_migration_ptes(struct page *old, struct page *new, bool locked,
+			   bool unmap_clean);
 
 /*
  * Called by memory-failure.c to kill processes.
@@ -298,12 +303,27 @@ static inline void try_to_unmap(struct page *page, enum ttu_flags flags)
 {
 }
 
+#ifdef CONFIG_HTMM
+static inline int cooling_page(struct page *page, struct mem_cgroup *memcg)
+{
+	return false;
+}
+
+static inline int page_check_hotness(struct page *page, struct mem_cgroup *memcg)
+{
+	return false;
+}
+static int get_pginfo_idx(struct page *page)
+{
+	return -1;
+}
+#endif
+
 static inline int page_mkclean(struct page *page)
 {
 	return 0;
 }
 
-
 #endif	/* CONFIG_MMU */
 
 #endif	/* _LINUX_RMAP_H */
diff --git a/include/linux/sched/numa_balancing.h b/include/linux/sched/numa_balancing.h
index 3988762efe15..c13ba820c07d 100644
--- a/include/linux/sched/numa_balancing.h
+++ b/include/linux/sched/numa_balancing.h
@@ -8,12 +8,14 @@
  */
 
 #include <linux/sched.h>
+#include <linux/page-flags.h>
 
 #define TNF_MIGRATED	0x01
 #define TNF_NO_GROUP	0x02
 #define TNF_SHARED	0x04
 #define TNF_FAULT_LOCAL	0x08
 #define TNF_MIGRATE_FAIL 0x10
+#define TNF_DEMOTED	0x40
 
 #ifdef CONFIG_NUMA_BALANCING
 extern void task_numa_fault(int last_node, int node, int pages, int flags);
@@ -21,7 +23,53 @@ extern pid_t task_numa_group_id(struct task_struct *p);
 extern void set_numabalancing_state(bool enabled);
 extern void task_numa_free(struct task_struct *p, bool final);
 extern bool should_numa_migrate_memory(struct task_struct *p, struct page *page,
-					int src_nid, int dst_cpu);
+					int src_nid, int dst_cpu, int flags);
+#ifdef CONFIG_64BIT
+static inline bool page_is_demoted(struct page *page)
+{
+	return PageDemoted(page);
+}
+
+static inline void set_page_demoted(struct page *page)
+{
+	SetPageDemoted(page);
+}
+
+static inline bool test_and_clear_page_demoted(struct page *page)
+{
+	return TestClearPageDemoted(page);
+}
+#else /* !CONFIG_64BIT */
+static inline bool page_is_demoted(struct page *page)
+{
+	struct page_ext *page_ext = lookup_page_ext(page);
+
+	if (unlikely(!page_ext))
+		return false;
+
+	return test_bit(PAGE_EXT_DEMOTED, &page_ext->flags);
+}
+
+static inline void set_page_demoted(struct page *page)
+{
+	struct page_ext *page_ext = lookup_page_ext(page);
+
+	if (unlikely(!page_ext))
+		return false;
+
+	return set_bit(PAGE_EXT_DEMOTED, &page_ext->flags);
+}
+
+static inline bool test_and_clear_page_demoted(struct page *page)
+{
+	struct page_ext *page_ext = lookup_page_ext(page);
+
+	if (unlikely(!page_ext))
+		return false;
+
+	return test_and_clear_bit(PAGE_EXT_DEMOTED, &page_ext->flags);
+}
+#endif /* !CONFIG_64BIT */
 #else
 static inline void task_numa_fault(int last_node, int node, int pages,
 				   int flags)
@@ -38,10 +86,21 @@ static inline void task_numa_free(struct task_struct *p, bool final)
 {
 }
 static inline bool should_numa_migrate_memory(struct task_struct *p,
-				struct page *page, int src_nid, int dst_cpu)
+				struct page *page, int src_nid, int dst_cpu, int flags)
 {
 	return true;
 }
+static inline bool page_is_demoted(struct page *page)
+{
+	return false;
+}
+static inline void set_page_demoted(struct page *page)
+{
+}
+static inline bool test_and_clear_page_demoted(struct page *page)
+{
+	return false;
+}
 #endif
 
 #endif /* _LINUX_SCHED_NUMA_BALANCING_H */
diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 304f431178fd..d4e4b19a3285 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -35,6 +35,12 @@ enum sched_tunable_scaling {
 	SCHED_TUNABLESCALING_END,
 };
 
+#define NUMA_BALANCING_DISABLED		0x0
+#define NUMA_BALANCING_NORMAL		0x1
+#define NUMA_BALANCING_TIERED_MEMORY	0x2
+
+extern int sysctl_numa_balancing_mode;
+
 /*
  *  control realtime throttling:
  *
diff --git a/include/linux/swap.h b/include/linux/swap.h
index 4efd267e2937..197fdb9169be 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -390,6 +390,8 @@ extern unsigned long shrink_all_memory(unsigned long nr_pages);
 extern int vm_swappiness;
 extern int remove_mapping(struct address_space *mapping, struct page *page);
 
+extern unsigned int move_pages_to_lru(struct lruvec *lruvec, struct list_head *list);
+
 extern unsigned long reclaim_pages(struct list_head *page_list);
 #ifdef CONFIG_NUMA
 extern int node_reclaim_mode;
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index b8037a46ff41..cb1ddc4501cd 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -945,6 +945,10 @@ asmlinkage long sys_rt_tgsigqueueinfo(pid_t tgid, pid_t  pid, int sig,
 asmlinkage long sys_perf_event_open(
 		struct perf_event_attr __user *attr_uptr,
 		pid_t pid, int cpu, int group_fd, unsigned long flags);
+// #ifdef CONFIG_HTMM
+asmlinkage long sys_htmm_start(pid_t pid, int node);
+asmlinkage long sys_htmm_end(pid_t pid);
+// #endif
 asmlinkage long sys_accept4(int, struct sockaddr __user *, int __user *, int);
 asmlinkage long sys_recvmmsg(int fd, struct mmsghdr __user *msg,
 			     unsigned int vlen, unsigned flags,
diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index a185cc75ff52..4c197493c159 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -35,6 +35,8 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		PGSTEAL_DIRECT,
 		PGDEMOTE_KSWAPD,
 		PGDEMOTE_DIRECT,
+		PGDEMOTE_FILE,
+		PGDEMOTE_ANON,
 		PGSCAN_KSWAPD,
 		PGSCAN_DIRECT,
 		PGSCAN_DIRECT_THROTTLE,
@@ -56,13 +58,42 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		NUMA_HINT_FAULTS,
 		NUMA_HINT_FAULTS_LOCAL,
 		NUMA_PAGE_MIGRATE,
+		PGPROMOTE_CANDIDATE,		/* candidates get selected for promotion */
+		PGPROMOTE_CANDIDATE_DEMOTED,	/* promotion candidate that got demoted earlier */
+		PGPROMOTE_CANDIDATE_ANON,	/* promotion candidate that are anon */
+		PGPROMOTE_CANDIDATE_FILE,	/* promotion candidate that are file */
+		PGPROMOTE_TRIED,		/* tried to migrate via NUMA balancing */
+		PGPROMOTE_FILE,			/* successfully promoted file pages  */
+		PGPROMOTE_ANON,			/* successfully promoted anon pages  */
 #endif
 #ifdef CONFIG_MIGRATION
 		PGMIGRATE_SUCCESS, PGMIGRATE_FAIL,
+		PGMIGRATE_DST_NODE_FULL_FAIL,	/* failed as the target node is full */
+		PGMIGRATE_NUMA_ISOLATE_FAIL,	/* failed in isolating numa page */
+		PGMIGRATE_NOMEM_FAIL,		/* failed as no memory left */
+		PGMIGRATE_REFCOUNT_FAIL,	/* failed in ref count */
 		THP_MIGRATION_SUCCESS,
 		THP_MIGRATION_FAIL,
 		THP_MIGRATION_SPLIT,
 #endif
+		PEBS_NR_SAMPLED,
+		PEBS_NR_SAMPLED_FMEM,
+		PEBS_NR_SAMPLED_SMEM,
+		//        | Samping                | Classification                           | Migration
+		// -------+------------------------+------------------------------------------+--------------------------
+		// TPP    | PTEA_SCAN_NS           | LRU_ROTATE_NS - PTEA_SCAN_NS - DEMOTE_NS | DEMOTE_NS + HINT_FAULT_NS
+		// Nomad  | PTEA_SCAN_NS           | LRU_ROTATE_NS - PTEA_SCAN_NS - DEMOTE_NS | DEMOTE_NS + HINT_FAULT_NS
+		// Memtis | SAMPLING_NS - PTEXT_NS | LRU_ROTATE_NS + PTEXT_NS                 | DEMOTE_NS + PROMOTE_NS
+		PTEA_SCAN_NS,	// Only record the rmap overhead
+		PTEA_SCANNED,	// Record rmap walked PTEs
+		LRU_ROTATE_NS,  // Overhead spent on selecting reclaimation candidates
+				// (include PTEA_SCAN_NS and DEMOTE_NS in TPP/Nomad)
+		DEMOTE_NS,	// Cost of demoting reclaimation candidates
+		HINT_FAULT_NS,	// Software-only overhead spent on NUMA hinting faults
+				// (include triggering overhead)
+		PROMOTE_NS,	// Memtis promotion candidates migration cost
+		SAMPLING_NS,	// Memtis ksamplingd cost (include PTEXT_NS)
+		PTEXT_NS,	// Memtis pagetable extension maintance cost
 #ifdef CONFIG_COMPACTION
 		COMPACTMIGRATE_SCANNED, COMPACTFREE_SCANNED,
 		COMPACTISOLATED,
@@ -113,6 +144,15 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		BALLOON_MIGRATE,
 #endif
 #endif
+#ifdef CONFIG_HTMM
+		HTMM_NR_PROMOTED,
+		HTMM_NR_DEMOTED,
+		HTMM_MISSED_DRAMREAD,
+		HTMM_MISSED_NVMREAD,
+		HTMM_MISSED_WRITE,
+		HTMM_ALLOC_DRAM,
+		HTMM_ALLOC_NVM,
+#endif
 #ifdef CONFIG_DEBUG_TLBFLUSH
 		NR_TLB_REMOTE_FLUSH,	/* cpu tried to flush others' tlbs */
 		NR_TLB_REMOTE_FLUSH_RECEIVED,/* cpu received ipi for flush */
diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index d6a6cf53b127..3a46268eb55c 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -11,6 +11,9 @@
 #include <linux/mmdebug.h>
 
 extern int sysctl_stat_interval;
+extern int sysctl_clearvmevents_handler(struct ctl_table *table, int write,
+	void __user *buffer, size_t *length, loff_t *ppos);
+
 
 #ifdef CONFIG_NUMA
 #define ENABLE_NUMA_STAT   1
@@ -83,6 +86,8 @@ static inline void count_vm_events(enum vm_event_item item, long delta)
 
 extern void all_vm_events(unsigned long *);
 
+extern void clear_all_vm_events(void);
+
 extern void vm_events_fold_cpu(int cpu);
 
 #else
@@ -103,6 +108,9 @@ static inline void __count_vm_events(enum vm_event_item item, long delta)
 static inline void all_vm_events(unsigned long *ret)
 {
 }
+static inline void clear_all_vm_events(void)
+{
+}
 static inline void vm_events_fold_cpu(int cpu)
 {
 }
diff --git a/include/trace/events/htmm.h b/include/trace/events/htmm.h
new file mode 100644
index 000000000000..c59c8c97e6e2
--- /dev/null
+++ b/include/trace/events/htmm.h
@@ -0,0 +1,62 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM htmm
+
+#if !defined(_TRACE_HTMM_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_HTMM_H
+
+#include <linux/tracepoint.h>
+
+TRACE_EVENT(access_info,
+
+	TP_PROTO(unsigned int nr_access, unsigned int nr_util),
+
+	TP_ARGS(nr_access, nr_util),
+
+	TP_STRUCT__entry(
+		__field(unsigned int, nr_access)
+		__field(unsigned int, nr_util)
+	),
+
+	TP_fast_assign(
+		__entry->nr_access = nr_access;
+		__entry->nr_util = nr_util;
+	),
+
+	TP_printk("nr_access: %u nr_util: %u\n",
+		__entry->nr_access,
+		__entry->nr_util)
+);
+
+TRACE_EVENT(base_access_info,
+
+	TP_PROTO(unsigned long addr, unsigned int clock,
+		     unsigned int nr_access, unsigned int nr_util),
+
+	TP_ARGS(addr, clock, nr_access, nr_util),
+
+	TP_STRUCT__entry(
+		__field(unsigned long, addr)
+		__field(unsigned int, clock)
+		__field(unsigned int, nr_access)
+		__field(unsigned int, nr_util)
+	),
+
+	TP_fast_assign(
+		__entry->addr = addr;
+		__entry->clock = clock;
+		__entry->nr_access = nr_access;
+		__entry->nr_util = nr_util;
+	),
+
+	TP_printk("addr: %lu clock: %u nr_access: %u nr_util: %u\n",
+		__entry->addr,
+		__entry->clock,
+		__entry->nr_access,
+		__entry->nr_util)
+);
+
+#endif
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h
index 116ed4d5d0f8..594df049a2c3 100644
--- a/include/trace/events/mmflags.h
+++ b/include/trace/events/mmflags.h
@@ -87,12 +87,34 @@
 #define IF_HAVE_PG_ARCH_2(flag,string)
 #endif
 
+#if defined(CONFIG_NUMA_BALANCING) && defined(CONFIG_64BIT)
+#define IF_HAVE_PG_DEMOTED(flag, string) ,{1UL << flag, string}
+#else
+#define IF_HAVE_PG_DEMOTED(flag, string)
+#endif
+
+#ifdef CONFIG_NOMAD
+#define IF_HAVE_PG_NUMA_QUEUED(flag, string) ,{1UL << flag, string}
+#define IF_HAVE_PG_SHADOWED(flag, string) ,{1UL << flag, string}
+#else
+#define IF_HAVE_PG_NUMA_QUEUED(flag, string)
+#define IF_HAVE_PG_SHADOWED(flag, string)
+#endif
+
 #ifdef CONFIG_KASAN_HW_TAGS
 #define IF_HAVE_PG_SKIP_KASAN_POISON(flag,string) ,{1UL << flag, string}
 #else
 #define IF_HAVE_PG_SKIP_KASAN_POISON(flag,string)
 #endif
 
+#ifdef CONFIG_HTMM
+#define IF_HAVE_PG_HTMM(flag,string) ,{1UL << flag, string}
+#define IF_HAVE_PG_NEEDSPLIT(flag,string) ,{1UL << flag, string}
+#else
+#define IF_HAVE_PG_HTMM(flag,string)
+#define IF_HAVE_PG_NEEDSPLIT(flag,string)
+#endif
+
 #define __def_pageflag_names						\
 	{1UL << PG_locked,		"locked"	},		\
 	{1UL << PG_waiters,		"waiters"	},		\
@@ -121,7 +143,12 @@ IF_HAVE_PG_HWPOISON(PG_hwpoison,	"hwpoison"	)		\
 IF_HAVE_PG_IDLE(PG_young,		"young"		)		\
 IF_HAVE_PG_IDLE(PG_idle,		"idle"		)		\
 IF_HAVE_PG_ARCH_2(PG_arch_2,		"arch_2"	)		\
-IF_HAVE_PG_SKIP_KASAN_POISON(PG_skip_kasan_poison, "skip_kasan_poison")
+IF_HAVE_PG_DEMOTED(PG_demoted,		"demoted"	)		\
+IF_HAVE_PG_NUMA_QUEUED(PG_numa_queued,  "promqueued"	)		\
+IF_HAVE_PG_SHADOWED(PG_shadowed,        "shadowed"	)               \
+IF_HAVE_PG_SKIP_KASAN_POISON(PG_skip_kasan_poison, "skip_kasan_poison")	\
+IF_HAVE_PG_HTMM(PG_htmm,		"htmm"		)		\
+IF_HAVE_PG_NEEDSPLIT(PG_needsplit,	"needsplit"	)
 
 #define show_page_flags(flags)						\
 	(flags) ? __print_flags(flags, "|",				\
diff --git a/kernel/cgroup/cgroup.c b/kernel/cgroup/cgroup.c
index be467aea457e..fdaedb57d814 100644
--- a/kernel/cgroup/cgroup.c
+++ b/kernel/cgroup/cgroup.c
@@ -4067,6 +4067,9 @@ static int cgroup_add_file(struct cgroup_subsys_state *css, struct cgroup *cgrp,
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	key = &cft->lockdep_key;
+#ifdef CONFIG_HTMM
+	pr_debug("%s: cft->name=%s\n", __func__, cft->name);
+#endif
 #endif
 	kn = __kernfs_create_file(cgrp->kn, cgroup_file_name(cgrp, cft, name),
 				  cgroup_file_mode(cft),
diff --git a/kernel/events/Makefile b/kernel/events/Makefile
index 3c022e33c109..ad24d4c2a0df 100644
--- a/kernel/events/Makefile
+++ b/kernel/events/Makefile
@@ -7,4 +7,5 @@ obj-y := core.o ring_buffer.o callchain.o
 
 obj-$(CONFIG_HAVE_HW_BREAKPOINT) += hw_breakpoint.o
 obj-$(CONFIG_UPROBES) += uprobes.o
+obj-y += memtis.o
 
diff --git a/kernel/events/core.c b/kernel/events/core.c
index 80d9c8fcc30a..b5ad3ddd0598 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -4888,8 +4888,7 @@ static void free_event_rcu(struct rcu_head *head)
 	kmem_cache_free(perf_event_cache, event);
 }
 
-static void ring_buffer_attach(struct perf_event *event,
-			       struct perf_buffer *rb);
+void ring_buffer_attach(struct perf_event *event, struct perf_buffer *rb);
 
 static void detach_sb_event(struct perf_event *event)
 {
@@ -5993,7 +5992,7 @@ static int perf_event_index(struct perf_event *event)
 	return event->pmu->event_idx(event);
 }
 
-static void perf_event_init_userpage(struct perf_event *event)
+void perf_event_init_userpage(struct perf_event *event)
 {
 	struct perf_event_mmap_page *userpg;
 	struct perf_buffer *rb;
@@ -6111,8 +6110,7 @@ static vm_fault_t perf_mmap_fault(struct vm_fault *vmf)
 	return ret;
 }
 
-static void ring_buffer_attach(struct perf_event *event,
-			       struct perf_buffer *rb)
+void ring_buffer_attach(struct perf_event *event, struct perf_buffer *rb)
 {
 	struct perf_buffer *old_rb = NULL;
 	unsigned long flags;
@@ -12302,10 +12300,20 @@ perf_check_permission(struct perf_event_attr *attr, struct task_struct *task)
 SYSCALL_DEFINE5(perf_event_open,
 		struct perf_event_attr __user *, attr_uptr,
 		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
+{
+	struct perf_event_attr attr;
+	int err = perf_copy_attr(attr_uptr, &attr);
+	if (err)
+		return err;
+	return __perf_event_open(&attr, pid, cpu, group_fd, flags);
+}
+
+int __perf_event_open(struct perf_event_attr *attr_ptr, pid_t pid, int cpu,
+		      int group_fd, unsigned long flags)
 {
 	struct perf_event *group_leader = NULL, *output_event = NULL;
 	struct perf_event *event, *sibling;
-	struct perf_event_attr attr;
+	struct perf_event_attr attr = *attr_ptr;
 	struct perf_event_context *ctx, *gctx;
 	struct file *event_file = NULL;
 	struct fd group = {NULL, 0};
@@ -12321,9 +12329,9 @@ SYSCALL_DEFINE5(perf_event_open,
 	if (flags & ~PERF_FLAG_ALL)
 		return -EINVAL;
 
-	err = perf_copy_attr(attr_uptr, &attr);
-	if (err)
-		return err;
+	// err = perf_copy_attr(attr_uptr, &attr);
+	// if (err)
+	// 	return err;
 
 	/* Do we allow access to perf_event_open(2) ? */
 	err = security_perf_event_open(&attr, PERF_SECURITY_OPEN);
diff --git a/kernel/events/internal.h b/kernel/events/internal.h
index 5150d5f84c03..5ed08f3eba4b 100644
--- a/kernel/events/internal.h
+++ b/kernel/events/internal.h
@@ -90,6 +90,8 @@ static inline bool rb_has_aux(struct perf_buffer *rb)
 	return !!rb->aux_nr_pages;
 }
 
+int __perf_event_open(struct perf_event_attr *attr_ptr, pid_t pid, int cpu,
+		      int group_fd, unsigned long flags);
 void perf_event_aux_event(struct perf_event *event, unsigned long head,
 			  unsigned long size, u64 flags);
 
diff --git a/kernel/events/memtis.c b/kernel/events/memtis.c
new file mode 100644
index 000000000000..32b95ecaef04
--- /dev/null
+++ b/kernel/events/memtis.c
@@ -0,0 +1,77 @@
+#include <linux/perf_event.h>
+#include <linux/ring_buffer.h>
+#include <linux/syscalls.h>
+#include "internal.h"
+
+extern void ring_buffer_attach(struct perf_event *event,
+			       struct perf_buffer *rb);
+extern void perf_event_init_userpage(struct perf_event *event);
+
+int htmm__perf_event_open(struct perf_event_attr *attr, pid_t pid, int cpu,
+			  int group_fd, unsigned long flags)
+{
+	return __perf_event_open(attr, pid, cpu, group_fd, flags);
+}
+
+/* allocates perf_buffer instead of calling perf_mmap() */
+int htmm__perf_event_init(struct perf_event *event, unsigned long nr_pages)
+{
+	struct perf_buffer *rb = NULL;
+	int ret = 0, flags = 0;
+
+	if (event->cpu == -1 && event->attr.inherit)
+		return -EINVAL;
+
+	ret = security_perf_event_read(event);
+	if (ret)
+		return ret;
+
+	if (nr_pages != 0 && !is_power_of_2(nr_pages))
+		return -EINVAL;
+
+	WARN_ON_ONCE(event->ctx->parent_ctx);
+	mutex_lock(&event->mmap_mutex);
+
+	WARN_ON(event->rb);
+
+	rb = rb_alloc(nr_pages,
+		      event->attr.watermark ? event->attr.wakeup_watermark : 0,
+		      event->cpu, flags);
+	if (!rb) {
+		ret = -ENOMEM;
+		goto unlock;
+	}
+
+	ring_buffer_attach(event, rb);
+	perf_event_init_userpage(event);
+	perf_event_update_userpage(event);
+
+unlock:
+	if (!ret) {
+		atomic_inc(&event->mmap_count);
+	}
+	mutex_unlock(&event->mmap_mutex);
+	return ret;
+}
+
+#ifdef CONFIG_HTMM
+#include <linux/htmm.h>
+SYSCALL_DEFINE2(htmm_start, pid_t, pid, int, node)
+{
+	return ksamplingd_init(pid, node);
+}
+SYSCALL_DEFINE1(htmm_end, pid_t, pid)
+{
+	ksamplingd_exit();
+	return 0;
+}
+#else
+SYSCALL_DEFINE2(htmm_start, pid_t, pid, int, node)
+{
+	return 0;
+}
+SYSCALL_DEFINE1(htmm_end, pid_t, pid)
+{
+	return 0;
+}
+#endif
diff --git a/kernel/exit.c b/kernel/exit.c
index 80efdfda6662..c61ad3490fbc 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -65,6 +65,9 @@
 #include <linux/compat.h>
 #include <linux/io_uring.h>
 #include <linux/sysfs.h>
+#ifdef CONFIG_HTMM
+#include <linux/htmm.h>
+#endif
 
 #include <linux/uaccess.h>
 #include <asm/unistd.h>
diff --git a/kernel/fork.c b/kernel/fork.c
index 753e641f617b..7fbfe94c069f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -97,6 +97,9 @@
 #include <linux/scs.h>
 #include <linux/io_uring.h>
 #include <linux/bpf.h>
+#ifdef CONFIG_HTMM
+#include <linux/htmm.h>
+#endif
 
 #include <asm/pgalloc.h>
 #include <linux/uaccess.h>
@@ -1072,6 +1075,10 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	init_tlb_flush_pending(mm);
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	mm->pmd_huge_pte = NULL;
+#endif
+#ifdef CONFIG_HTMM
+	if (p != &init_task)
+		htmm_mm_init(mm);
 #endif
 	mm_init_uprobes_state(mm);
 	hugetlb_count_init(mm);
@@ -1096,6 +1103,9 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 fail_nocontext:
 	mm_free_pgd(mm);
 fail_nopgd:
+#ifdef CONFIG_HTMM
+	htmm_mm_exit(mm);
+#endif
 	free_mm(mm);
 	return NULL;
 }
@@ -1122,6 +1132,9 @@ static inline void __mmput(struct mm_struct *mm)
 	uprobe_clear_state(mm);
 	exit_aio(mm);
 	ksm_exit(mm);
+#ifdef CONFIG_HTMM
+	htmm_mm_exit(mm);
+#endif
 	khugepaged_exit(mm); /* must run before exit_mmap */
 	exit_mmap(mm);
 	mm_put_huge_zero_page(mm);
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index b43da6201b9a..705064231bd0 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4274,9 +4274,29 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 }
 
 DEFINE_STATIC_KEY_FALSE(sched_numa_balancing);
+int sysctl_numa_balancing_mode;
+bool numa_promotion_tiered_enabled;
 
 #ifdef CONFIG_NUMA_BALANCING
 
+/*
+ * If there is only one toptier node available, pages on that
+ * node can not be promotrd to anywhere. In that case, downgrade
+ * to numa_promotion_tiered_enabled mode
+ */
+static void check_numa_promotion_mode(void)
+{
+	int node, toptier_node_count = 0;
+
+	for_each_online_node(node) {
+		if (node_is_toptier(node))
+			++toptier_node_count;
+	}
+	if (toptier_node_count == 1) {
+		numa_promotion_tiered_enabled = true;
+	}
+}
+
 void set_numabalancing_state(bool enabled)
 {
 	if (enabled)
@@ -4289,20 +4309,22 @@ void set_numabalancing_state(bool enabled)
 int sysctl_numa_balancing(struct ctl_table *table, int write,
 			  void *buffer, size_t *lenp, loff_t *ppos)
 {
-	struct ctl_table t;
 	int err;
-	int state = static_branch_likely(&sched_numa_balancing);
 
 	if (write && !capable(CAP_SYS_ADMIN))
 		return -EPERM;
 
-	t = *table;
-	t.data = &state;
-	err = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);
+	err = proc_dointvec_minmax(table, write, buffer, lenp, ppos);
 	if (err < 0)
 		return err;
-	if (write)
-		set_numabalancing_state(state);
+	if (write) {
+		if (sysctl_numa_balancing_mode & NUMA_BALANCING_NORMAL)
+			check_numa_promotion_mode();
+		else if (sysctl_numa_balancing_mode & NUMA_BALANCING_TIERED_MEMORY)
+			numa_promotion_tiered_enabled = true;
+
+		set_numabalancing_state(*(int *)table->data);
+	}
 	return err;
 }
 #endif
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 94fcd585eb7f..11ce27588727 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -21,6 +21,8 @@
  *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra
  */
 #include "sched.h"
+#include <trace/events/sched.h>
+#include <linux/mempolicy.h>
 
 /*
  * Targeted preemption latency for CPU-bound tasks:
@@ -1424,15 +1426,32 @@ static inline unsigned long group_weight(struct task_struct *p, int nid,
 }
 
 bool should_numa_migrate_memory(struct task_struct *p, struct page * page,
-				int src_nid, int dst_cpu)
+				int src_nid, int dst_cpu, int flags)
 {
 	struct numa_group *ng = deref_curr_numa_group(p);
 	int dst_nid = cpu_to_node(dst_cpu);
 	int last_cpupid, this_cpupid;
 
+	count_vm_numa_event(PGPROMOTE_CANDIDATE);
+
+	if (numa_demotion_enabled && (flags & TNF_DEMOTED))
+		count_vm_numa_event(PGPROMOTE_CANDIDATE_DEMOTED);
+
+	if (page_is_file_lru(page))
+		count_vm_numa_event(PGPROMOTE_CANDIDATE_FILE);
+	else
+		count_vm_numa_event(PGPROMOTE_CANDIDATE_ANON);
+
 	this_cpupid = cpu_pid_to_cpupid(dst_cpu, current->pid);
 	last_cpupid = page_cpupid_xchg_last(page, this_cpupid);
 
+	/*
+	 * The pages in non-toptier memory node should be migrated
+	 * according to hot/cold instead of accessing CPU node.
+	 */
+	if (numa_promotion_tiered_enabled && !node_is_toptier(src_nid))
+		return true;
+
 	/*
 	 * Allow first faults or private faults to migrate immediately early in
 	 * the lifetime of a task. The magic number 4 is based on waiting for
@@ -11321,6 +11340,7 @@ void trigger_load_balance(struct rq *rq)
 		raise_softirq(SCHED_SOFTIRQ);
 
 	nohz_balancer_kick(rq);
+	check_toptier_balanced();
 }
 
 static void rq_online_fair(struct rq *rq)
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5061093d9baa..be0c8e75612d 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -52,6 +52,8 @@
 #include <linux/kthread.h>
 #include <linux/membarrier.h>
 #include <linux/migrate.h>
+#include <linux/mempolicy.h>
+#include <linux/mm_inline.h>
 #include <linux/mmu_context.h>
 #include <linux/nmi.h>
 #include <linux/proc_fs.h>
@@ -1461,7 +1463,9 @@ static inline void assert_clock_updated(struct rq *rq)
 	 * The only reason for not seeing a clock update since the
 	 * last rq_pin_lock() is if we're currently skipping updates.
 	 */
+#ifndef CONFIG_HTMM /* for lock stat analysis */
 	SCHED_WARN_ON(rq->clock_update_flags < RQCF_ACT_SKIP);
+#endif
 }
 
 static inline u64 rq_clock(struct rq *rq)
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 4554e80c4272..a9a4acbae71d 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -113,11 +113,12 @@
 static int sixty = 60;
 #endif
 
+static int __maybe_unused three = 3;
 static unsigned long zero_ul;
 static unsigned long one_ul = 1;
 static unsigned long long_max = LONG_MAX;
-#ifdef CONFIG_PRINTK
 static int ten_thousand = 10000;
+#ifdef CONFIG_PRINTK
 #endif
 #ifdef CONFIG_PERF_EVENTS
 static int six_hundred_forty_kb = 640 * 1024;
@@ -188,6 +189,8 @@ static int min_extfrag_threshold;
 static int max_extfrag_threshold = 1000;
 #endif
 
+static int sysctl_clear_vm_events;
+
 #endif /* CONFIG_SYSCTL */
 
 #if defined(CONFIG_BPF_SYSCALL) && defined(CONFIG_SYSCTL)
@@ -1807,12 +1810,12 @@ static struct ctl_table kern_table[] = {
 #ifdef CONFIG_NUMA_BALANCING
 	{
 		.procname	= "numa_balancing",
-		.data		= NULL, /* filled in by handler */
-		.maxlen		= sizeof(unsigned int),
+		.data		= &sysctl_numa_balancing_mode,
+		.maxlen		= sizeof(int),
 		.mode		= 0644,
 		.proc_handler	= sysctl_numa_balancing,
 		.extra1		= SYSCTL_ZERO,
-		.extra2		= SYSCTL_ONE,
+		.extra2		= &three,
 	},
 #endif /* CONFIG_NUMA_BALANCING */
 	{
@@ -2935,6 +2938,13 @@ static struct ctl_table vm_table[] = {
 	},
 
 #endif /* CONFIG_COMPACTION */
+	{
+		.procname	= "clear_all_vm_events",
+		.data		= &sysctl_clear_vm_events,
+		.maxlen		= sizeof(int),
+		.mode		= 0200,
+		.proc_handler	= sysctl_clearvmevents_handler,
+	},
 	{
 		.procname	= "min_free_kbytes",
 		.data		= &min_free_kbytes,
@@ -2960,6 +2970,15 @@ static struct ctl_table vm_table[] = {
 		.extra1		= SYSCTL_ONE,
 		.extra2		= SYSCTL_THREE_THOUSAND,
 	},
+	{
+		.procname       = "demote_scale_factor",
+		.data           = &demote_scale_factor,
+		.maxlen         = sizeof(demote_scale_factor),
+		.mode           = 0644,
+		.proc_handler   = demote_scale_factor_sysctl_handler,
+		.extra1         = SYSCTL_ONE,
+		.extra2         = &ten_thousand,
+	},
 	{
 		.procname	= "percpu_pagelist_high_fraction",
 		.data		= &percpu_pagelist_high_fraction,
diff --git a/kernel/trace/fgraph.c b/kernel/trace/fgraph.c
index b8a0d1d564fb..6ef645a1e465 100644
--- a/kernel/trace/fgraph.c
+++ b/kernel/trace/fgraph.c
@@ -229,12 +229,13 @@ static struct notifier_block ftrace_suspend_notifier = {
  * Send the trace to the ring-buffer.
  * @return the original return address.
  */
-unsigned long ftrace_return_to_handler(unsigned long frame_pointer)
+unsigned long ftrace_return_to_handler(unsigned long frame_pointer, unsigned long retval)
 {
 	struct ftrace_graph_ret trace;
 	unsigned long ret;
 
 	ftrace_pop_return_trace(&trace, &ret, frame_pointer);
+	trace.retval = retval;
 	trace.rettime = trace_clock_local();
 	ftrace_graph_return(&trace);
 	/*
diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 449a8bd873cf..4a163746f821 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -839,6 +839,7 @@ static __always_inline bool ftrace_hash_empty(struct ftrace_hash *hash)
 #define TRACE_GRAPH_PRINT_TAIL          0x100
 #define TRACE_GRAPH_SLEEP_TIME          0x200
 #define TRACE_GRAPH_GRAPH_TIME          0x400
+#define TRACE_GRAPH_PRINT_RETVAL        0x800
 #define TRACE_GRAPH_PRINT_FILL_SHIFT	28
 #define TRACE_GRAPH_PRINT_FILL_MASK	(0x3 << TRACE_GRAPH_PRINT_FILL_SHIFT)
 
diff --git a/kernel/trace/trace_entries.h b/kernel/trace/trace_entries.h
index cd41e863b51c..d798cb17546f 100644
--- a/kernel/trace/trace_entries.h
+++ b/kernel/trace/trace_entries.h
@@ -93,16 +93,17 @@ FTRACE_ENTRY_PACKED(funcgraph_exit, ftrace_graph_ret_entry,
 	F_STRUCT(
 		__field_struct(	struct ftrace_graph_ret,	ret	)
 		__field_packed(	unsigned long,	ret,		func	)
+		__field_packed(	unsigned long,	ret,		retval	)
 		__field_packed(	int,		ret,		depth	)
 		__field_packed(	unsigned int,	ret,		overrun	)
 		__field_packed(	unsigned long long, ret,	calltime)
 		__field_packed(	unsigned long long, ret,	rettime	)
 	),
 
-	F_printk("<-- %ps (%d) (start: %llx  end: %llx) over: %d",
+	F_printk("<-- %ps (%d) (start: %llx  end: %llx) over: %d retval: %lx",
 		 (void *)__entry->func, __entry->depth,
 		 __entry->calltime, __entry->rettime,
-		 __entry->depth)
+		 __entry->depth, __entry->retval)
 );
 
 /*
diff --git a/kernel/trace/trace_functions_graph.c b/kernel/trace/trace_functions_graph.c
index 6b5ff3ba4251..720b5fa7f4c5 100644
--- a/kernel/trace/trace_functions_graph.c
+++ b/kernel/trace/trace_functions_graph.c
@@ -58,6 +58,8 @@ static struct tracer_opt trace_opts[] = {
 	{ TRACER_OPT(funcgraph-irqs, TRACE_GRAPH_PRINT_IRQS) },
 	/* Display function name after trailing } */
 	{ TRACER_OPT(funcgraph-tail, TRACE_GRAPH_PRINT_TAIL) },
+	/* Display function return value */
+	{ TRACER_OPT(funcgraph-retval, TRACE_GRAPH_PRINT_RETVAL) },
 	/* Include sleep time (scheduled out) between entry and return */
 	{ TRACER_OPT(sleep-time, TRACE_GRAPH_SLEEP_TIME) },
 
@@ -619,6 +621,43 @@ print_graph_duration(struct trace_array *tr, unsigned long long duration,
 	trace_seq_puts(s, "|  ");
 }
 
+static void print_graph_retval(struct trace_seq *s, unsigned long retval,
+				bool leaf, void *func)
+{
+	unsigned long err_code = 0;
+
+	if (retval == 0)
+		goto done;
+
+	/* Guess whether the retval looks like an error code */
+	if ((retval & BIT(7)) && (retval >> 8) == 0)
+		err_code = (unsigned long)(s8)retval;
+	else if ((retval & BIT(15)) && (retval >> 16) == 0)
+		err_code = (unsigned long)(s16)retval;
+	else if ((retval & BIT(31)) && (((u64)retval) >> 32) == 0)
+		err_code = (unsigned long)(s32)retval;
+	else
+		err_code = retval;
+
+	if (!IS_ERR_VALUE(err_code))
+		err_code = 0;
+
+done:
+	if (leaf) {
+		if (err_code != 0)
+			trace_seq_printf(s, "%ps(); /* => %lx %ld */\n",
+				func, retval, err_code);
+		else
+			trace_seq_printf(s, "%ps(); /* => %lx */\n", func, retval);
+	} else {
+		if (err_code != 0)
+			trace_seq_printf(s, "} /* %ps => %lx %ld */\n",
+				func, retval, err_code);
+		else
+			trace_seq_printf(s, "} /* %ps => %lx */\n", func, retval);
+	}
+}
+
 /* Case of a leaf function on its call entry */
 static enum print_line_t
 print_graph_entry_leaf(struct trace_iterator *iter,
@@ -663,7 +702,10 @@ print_graph_entry_leaf(struct trace_iterator *iter,
 	for (i = 0; i < call->depth * TRACE_GRAPH_INDENT; i++)
 		trace_seq_putc(s, ' ');
 
-	trace_seq_printf(s, "%ps();\n", (void *)call->func);
+	if (flags & TRACE_GRAPH_PRINT_RETVAL)
+		print_graph_retval(s, graph_ret->retval, true, (void *)call->func);
+	else
+		trace_seq_printf(s, "%ps();\n", (void *)call->func);
 
 	print_graph_irq(iter, graph_ret->func, TRACE_GRAPH_RET,
 			cpu, iter->ent->pid, flags);
@@ -942,17 +984,25 @@ print_graph_return(struct ftrace_graph_ret *trace, struct trace_seq *s,
 		trace_seq_putc(s, ' ');
 
 	/*
-	 * If the return function does not have a matching entry,
-	 * then the entry was lost. Instead of just printing
-	 * the '}' and letting the user guess what function this
-	 * belongs to, write out the function name. Always do
-	 * that if the funcgraph-tail option is enabled.
+	 * Always write out the function name and its return value if the
+	 * function-retval option is enabled.
 	 */
-	if (func_match && !(flags & TRACE_GRAPH_PRINT_TAIL))
-		trace_seq_puts(s, "}\n");
-	else
-		trace_seq_printf(s, "} /* %ps */\n", (void *)trace->func);
-
+	if (flags & TRACE_GRAPH_PRINT_RETVAL) {
+		print_graph_retval(s, trace->retval, false, (void *)trace->func);
+	} else {
+		/*
+		 * If the return function does not have a matching entry,
+		 * then the entry was lost. Instead of just printing
+		 * the '}' and letting the user guess what function this
+		 * belongs to, write out the function name. Always do
+		 * that if the funcgraph-tail option is enabled.
+		 */
+		if (func_match && !(flags & TRACE_GRAPH_PRINT_TAIL))
+			trace_seq_puts(s, "}\n");
+		else
+			trace_seq_printf(s, "} /* %ps */\n", (void *)trace->func);
+	}
+ 
 	/* Overrun */
 	if (flags & TRACE_GRAPH_PRINT_OVERRUN)
 		trace_seq_printf(s, " (Overruns: %u)\n",
diff --git a/mm/Kconfig b/mm/Kconfig
index c048dea7e342..fe24ff4c6b80 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -899,4 +899,10 @@ config SECRETMEM
 
 source "mm/damon/Kconfig"
 
+source "mm/nomad/Kconfig"
+
+source "mm/memtis/Kconfig"
+
+source "mm/demeter/Kconfig"
+
 endmenu
diff --git a/mm/Makefile b/mm/Makefile
index fc60a40ce954..e19775248978 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -130,3 +130,6 @@ obj-$(CONFIG_PAGE_REPORTING) += page_reporting.o
 obj-$(CONFIG_IO_MAPPING) += io-mapping.o
 obj-$(CONFIG_HAVE_BOOTMEM_INFO_NODE) += bootmem_info.o
 obj-$(CONFIG_GENERIC_IOREMAP) += ioremap.o
+obj-$(CONFIG_NOMAD) += nomad/
+obj-$(CONFIG_HTMM) += memtis/
+obj-$(CONFIG_DEMETER) += demeter/
diff --git a/mm/demeter/Kconfig b/mm/demeter/Kconfig
new file mode 100644
index 000000000000..9d4581b89902
--- /dev/null
+++ b/mm/demeter/Kconfig
@@ -0,0 +1,7 @@
+config DEMETER
+        tristate "Heterogeneous memory agent"
+        default m
+        select PRIME_NUMBERS
+        help
+          Enable heterogeneous memory guest agent to rebalance memory across different memory media.
+
diff --git a/mm/demeter/Makefile b/mm/demeter/Makefile
new file mode 100644
index 000000000000..950c43e44edf
--- /dev/null
+++ b/mm/demeter/Makefile
@@ -0,0 +1,5 @@
+obj-$(CONFIG_DEMETER) += demeter_balloon.o
+
+demeter_balloon-objs += balloon.o
+
+CFLAGS_balloon.o += -include balloon-compact.h -Wno-gcc-compat -Wno-declaration-after-statement
diff --git a/mm/demeter/balloon-compact.h b/mm/demeter/balloon-compact.h
new file mode 100644
index 000000000000..2d6bf118c2e4
--- /dev/null
+++ b/mm/demeter/balloon-compact.h
@@ -0,0 +1,63 @@
+#ifndef BALLOON_COMPACT_H
+#define BALLOON_COMPACT_H
+#include <linux/virtio_config.h>
+
+#undef __cleanup
+#define __cleanup(func) __maybe_unused __attribute__((__cleanup__(func)))
+
+#define __guard_ptr(_name) class_##_name##_lock_ptr
+
+#define scoped_guard(_name, args...)					\
+	for (CLASS(_name, scope)(args),					\
+	     *done = NULL; __guard_ptr(_name)(&scope) && !done; done = (void *)1)
+
+#define CLASS(_name, var)						\
+	class_##_name##_t var __cleanup(class_##_name##_destructor) =	\
+		class_##_name##_constructor
+
+
+#define __DEFINE_UNLOCK_GUARD(_name, _type, _unlock, ...)		\
+typedef struct {							\
+	_type *lock;							\
+	__VA_ARGS__;							\
+} class_##_name##_t;							\
+									\
+static inline void class_##_name##_destructor(class_##_name##_t *_T)	\
+{									\
+	if (_T->lock) { _unlock; }					\
+}									\
+									\
+static inline void *class_##_name##_lock_ptr(class_##_name##_t *_T)	\
+{									\
+	return _T->lock;						\
+}
+
+#define __DEFINE_LOCK_GUARD_1(_name, _type, _lock)			\
+static inline class_##_name##_t class_##_name##_constructor(_type *l)	\
+{									\
+	class_##_name##_t _t = { .lock = l }, *_T = &_t;		\
+	_lock;								\
+	return _t;							\
+}
+
+#define DEFINE_LOCK_GUARD_1(_name, _type, _lock, _unlock, ...)		\
+__DEFINE_UNLOCK_GUARD(_name, _type, _unlock, __VA_ARGS__)		\
+__DEFINE_LOCK_GUARD_1(_name, _type, _lock)
+
+DEFINE_LOCK_GUARD_1(spinlock_irqsave, spinlock_t,
+		    spin_lock_irqsave(_T->lock, _T->flags),
+		    spin_unlock_irqrestore(_T->lock, _T->flags),
+		    unsigned long flags)
+
+void virtio_reset_device(struct virtio_device *dev)
+{
+	dev->config->reset(dev);
+}
+
+#define last_node(src) __last_node(&(src))
+static inline unsigned int __last_node(const nodemask_t *srcp)
+{
+	return min_t(unsigned int, MAX_NUMNODES, find_last_bit(srcp->bits, MAX_NUMNODES));
+}
+
+#endif
diff --git a/mm/demeter/balloon.c b/mm/demeter/balloon.c
new file mode 100644
index 000000000000..6eda97c8e0ba
--- /dev/null
+++ b/mm/demeter/balloon.c
@@ -0,0 +1,798 @@
+#include <linux/virtio.h>
+#include <linux/virtio_balloon.h>
+#include <linux/swap.h>
+#include <linux/workqueue.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/balloon_compaction.h>
+#include <linux/oom.h>
+#include <linux/wait.h>
+#include <linux/mm.h>
+#include <linux/page_reporting.h>
+#include <linux/sched/clock.h>
+
+#define TRY(exp)                                                            \
+	({                                                                  \
+		__typeof__((exp)) __err = (exp);                            \
+		if ((u64)(__err) >= (u64)(-MAX_ERRNO)) {                    \
+			pr_err("%s:%d failed with error %lld:\n", __FILE__, \
+			       __LINE__, (s64)__err);                       \
+			dump_stack();                                       \
+			return (s64)(__err);                                \
+		}                                                           \
+		__err;                                                      \
+	})
+
+struct virtio_balloon_config_extended {
+	struct virtio_balloon_config base;
+	/* Number of heterogeneous pages host wants Guest to give up. */
+	__le32 num_hetero_pages;
+	/* Number of pages we've actually got in balloon. */
+	__le32 actual_hetero;
+};
+
+/*
+ * Balloon device works in 4K page units.  So each page is pointed to by
+ * multiple balloon pages.  All memory counters in this driver are in balloon
+ * page units.
+ */
+#define VIRTIO_BALLOON_PAGES_PER_PAGE \
+	(unsigned int)(PAGE_SIZE >> VIRTIO_BALLOON_PFN_SHIFT)
+#define VIRTIO_BALLOON_ARRAY_PFNS_MAX 256
+/* Maximum number of (4k) pages to deflate on OOM notifications. */
+#define VIRTIO_BALLOON_OOM_NR_PAGES 256
+#define VIRTIO_BALLOON_OOM_NOTIFY_PRIORITY 80
+
+static const struct virtio_device_id id_table[] = {
+	{ VIRTIO_ID_BALLOON, VIRTIO_DEV_ANY_ID },
+	{ 0 },
+};
+
+enum virtio_balloon_feature {
+	F_TELL_HOST = VIRTIO_BALLOON_F_MUST_TELL_HOST,
+	F_STATS = VIRTIO_BALLOON_F_STATS_VQ,
+	F_OOM = VIRTIO_BALLOON_F_DEFLATE_ON_OOM,
+	F_REPORT = VIRTIO_BALLOON_F_REPORTING,
+#define VIRTIO_BALLOON_F_HETERO_MEM \
+	6 /* Additional inflate/deflate queue for heterogeneous memory*/
+	F_HETERO = VIRTIO_BALLOON_F_HETERO_MEM,
+};
+
+// clang-format off
+static unsigned int features[] = {
+	F_TELL_HOST,
+	F_STATS,
+	F_OOM,
+	F_REPORT,
+	F_HETERO,
+};
+// clang-format on
+
+enum virtio_balloon_vq {
+	Q_INFLATE,
+	Q_DEFLATE,
+	Q_STATS,
+	// Q_FREE_PAGE,
+	Q_REPORTING,
+	Q_HETERO_INFLATE,
+	Q_HETERO_DEFLATE,
+	Q_MAX
+};
+
+enum virtio_balloon_inner_idx {
+	I_NORMAL,
+	I_HETERO,
+	I_MAX,
+};
+
+enum virtio_balloon_stats_tag {
+	T_SWAP_IN = VIRTIO_BALLOON_S_SWAP_IN,
+	T_SWAP_OUT = VIRTIO_BALLOON_S_SWAP_OUT,
+	T_MAJFLT = VIRTIO_BALLOON_S_MAJFLT,
+	T_MINFLT = VIRTIO_BALLOON_S_MINFLT,
+	T_MEMFREE = VIRTIO_BALLOON_S_MEMFREE,
+	T_MEMTOT = VIRTIO_BALLOON_S_MEMTOT,
+	T_AVAIL = VIRTIO_BALLOON_S_AVAIL,
+	T_CACHES = VIRTIO_BALLOON_S_CACHES,
+	T_HTLB_PGALLOC = VIRTIO_BALLOON_S_HTLB_PGALLOC,
+	T_HTLB_PGFAIL = VIRTIO_BALLOON_S_HTLB_PGFAIL,
+	T_NORMAL_ACCESS,
+	T_NORMAL_FREE,
+	T_NORMAL_TOTAL,
+	T_HETERO_ACCESS,
+	T_HETERO_FREE,
+	T_HETERO_TOTAL,
+	T_MAX,
+};
+
+typedef struct balloon_dev_info page_tracker_t;
+static void (*page_tracker_track)(page_tracker_t *tracker,
+				  struct page *page) = balloon_page_enqueue;
+static struct page *(*page_tracker_untrack)(page_tracker_t *tracker) =
+	balloon_page_dequeue;
+static inline void page_tracker_init(page_tracker_t *tracker)
+{
+	balloon_devinfo_init(tracker);
+}
+
+struct virtio_balloon {
+	struct virtio_device *vdev;
+	struct virtqueue *vqs[Q_MAX];
+	struct work_struct work[Q_MAX];
+	struct virtio_balloon_tracepoints {
+		u64 total_elapsed, work_elapsed;
+	} tracepoints[Q_MAX];
+	// make sure no new work are queued when stopping the device
+	spinlock_t queue_work;
+	atomic_t should_exit;
+	struct notifier_block oom_notification;
+	wait_queue_head_t ack;
+	struct virtio_balloon_inner {
+		struct mutex lock;
+		// The actual size of pages in the balloon
+		u32 len;
+		// All the pages we have returned to the host
+		page_tracker_t tracking;
+		// Temporary storage for communicating with the host
+		u32 pfns[VIRTIO_BALLOON_ARRAY_PFNS_MAX];
+	} inner[I_MAX];
+	struct virtio_balloon_stat_vec {
+		u32 len;
+		struct virtio_balloon_stat items[T_MAX];
+	} stats;
+};
+
+static u32 vb_config_read_target(struct virtio_balloon *vb, u32 idx)
+{
+	u32 target = 0;
+	switch (idx) {
+	case I_NORMAL:
+		virtio_cread_le(vb->vdev, struct virtio_balloon_config_extended,
+				base.num_pages, &target);
+		break;
+	case I_HETERO:
+		virtio_cread_le(vb->vdev, struct virtio_balloon_config_extended,
+				num_hetero_pages, &target);
+		break;
+	default:
+		dev_err(&vb->vdev->dev,
+			"%s failure: requested sub-ballon does not exit\n",
+			__func__);
+		BUG();
+	}
+	return target;
+}
+
+static void vb_config_write_actual(struct virtio_balloon *vb, u32 idx,
+				   u32 actual)
+{
+	switch (idx) {
+	case I_NORMAL:
+		virtio_cwrite_le(vb->vdev,
+				 struct virtio_balloon_config_extended,
+				 base.actual, &actual);
+		break;
+	case I_HETERO:
+		virtio_cwrite_le(vb->vdev,
+				 struct virtio_balloon_config_extended,
+				 actual_hetero, &actual);
+		break;
+	default:
+		dev_err(&vb->vdev->dev,
+			"%s failure: requested sub-ballon does not exit\n",
+			__func__);
+		BUG();
+	}
+}
+
+static void vb_callback_ack(struct virtqueue *vq)
+{
+	struct virtio_balloon *vb = vq->vdev->priv;
+	wake_up(&vb->ack);
+}
+
+static void vb_callback_stats_request(struct virtqueue *vq)
+{
+	struct virtio_balloon *vb = vq->vdev->priv;
+
+	scoped_guard(spinlock_irqsave, &vb->queue_work)
+	{
+		if (atomic_read(&vb->should_exit)) {
+			return;
+		}
+		queue_work(system_freezable_wq, &vb->work[Q_STATS]);
+	}
+}
+
+static bool vb_acked(struct virtio_balloon *vb, u32 feature)
+{
+	return virtio_has_feature(vb->vdev, feature);
+}
+
+static int vb_send_buf(struct virtio_balloon *vb, u32 qidx, void *buf, u32 len)
+{
+	struct virtqueue *vq = vb->vqs[qidx];
+	struct scatterlist sg;
+	sg_init_one(&sg, buf, len);
+	TRY(virtqueue_add_outbuf(vq, &sg, 1, vb, GFP_KERNEL));
+	virtqueue_kick(vq);
+	return 0;
+}
+
+static void *vb_recv_buf(struct virtio_balloon *vb, u32 qidx, u32 *len)
+{
+	struct virtqueue *vq = vb->vqs[qidx];
+	u32 _len;
+	// no data should be associated with used buffer for all balloon vq
+	return virtqueue_get_buf(vq, len ? len : &_len);
+}
+
+static s64 vb_inner_diff_from_target(struct virtio_balloon *vb, u32 idx)
+{
+	BUG_ON(idx != I_NORMAL && idx != I_HETERO);
+	struct virtio_balloon_inner *inner = &vb->inner[idx];
+	mutex_lock(&inner->lock);
+	s64 target = vb_config_read_target(vb, idx);
+	s64 diff = target - inner->len;
+	mutex_unlock(&inner->lock);
+	// dev_info(&vb->vdev->dev, "%s: idx=%u, target=%lld, has=%u, diff=%lld\n",
+	// 	 __func__, idx, target, inner->len, diff);
+	return diff;
+}
+
+static struct page *vb_inner_page_alloc(struct virtio_balloon *vb, u64 idx)
+{
+	int nid = NUMA_NO_NODE;
+	switch (idx) {
+	case I_NORMAL:
+		nid = first_node(node_states[N_MEMORY]);
+		break;
+	case I_HETERO:
+		nid = last_node(node_states[N_MEMORY]);
+		break;
+	default:
+		dev_err(&vb->vdev->dev,
+			"%s failure: requested sub-ballon does not exit\n",
+			__func__);
+		BUG();
+	}
+	return alloc_pages_node(nid,
+				balloon_mapping_gfp_mask() | __GFP_NOMEMALLOC |
+					__GFP_NORETRY | __GFP_NOWARN,
+				0);
+}
+
+static u32 vb_inner_inflate(struct virtio_balloon *vb, u32 idx, u32 todo)
+{
+	BUG_ON(idx != I_NORMAL && idx != I_HETERO);
+	u32 qidx = idx == I_NORMAL ? Q_INFLATE : Q_HETERO_INFLATE;
+
+	struct virtio_balloon_inner *inner = &vb->inner[idx];
+	todo = min(todo, (u32)ARRAY_SIZE(vb->inner[idx].pfns));
+
+	// allocate pages without holding the lock
+	struct list_head pages = LIST_HEAD_INIT(pages);
+	while (todo-- > 0) {
+		struct page *page = vb_inner_page_alloc(vb, idx);
+		if (!page) {
+			dev_info_ratelimited(
+				&vb->vdev->dev,
+				"%s failure: Out of puff! Can't get pages\n",
+				__func__);
+			msleep(200);
+			break;
+		}
+		list_add(&page->lru, &pages);
+	}
+
+	mutex_lock(&inner->lock);
+
+	u32 done = 0;
+	struct page *page, *next;
+	list_for_each_entry_safe(page, next, &pages, lru) {
+		page_tracker_track(&inner->tracking, page);
+		inner->pfns[done++] = page_to_pfn(page);
+	}
+	vb_send_buf(vb, qidx, inner->pfns, sizeof(*inner->pfns) * done);
+	wait_event(vb->ack, vb_recv_buf(vb, qidx, NULL));
+	inner->len += done;
+	vb_config_write_actual(vb, idx, inner->len);
+
+	mutex_unlock(&inner->lock);
+	return done;
+}
+
+static u32 vb_inner_deflate(struct virtio_balloon *vb, u32 idx, u32 todo)
+{
+	BUG_ON(idx != I_NORMAL && idx != I_HETERO);
+	u32 qidx = idx == I_NORMAL ? Q_DEFLATE : Q_HETERO_DEFLATE;
+	struct virtio_balloon_inner *inner = &vb->inner[idx];
+	todo = min(todo, inner->len);
+	mutex_lock(&inner->lock);
+	u32 done = 0;
+	struct list_head pages = LIST_HEAD_INIT(pages);
+	while (done < todo) {
+		struct page *page = page_tracker_untrack(&inner->tracking);
+		if (!page)
+			break;
+		inner->pfns[done++] = page_to_pfn(page);
+		list_add(&page->lru, &pages);
+	}
+	vb_send_buf(vb, qidx, inner->pfns, sizeof(*inner->pfns) * done);
+	wait_event(vb->ack, vb_recv_buf(vb, qidx, NULL));
+	inner->len -= done;
+	vb_config_write_actual(vb, idx, inner->len);
+
+	struct page *page, *next;
+	list_for_each_entry_safe(page, next, &pages, lru) {
+		list_del(&page->lru);
+		put_page(page);
+	}
+
+	mutex_unlock(&inner->lock);
+	return done;
+}
+
+int vb_stat_push(struct virtio_balloon *vb, u16 tag, u64 val)
+{
+	struct virtio_balloon_stat_vec *vec = &vb->stats;
+	if (vec->len >= ARRAY_SIZE(vec->items))
+		return -EINVAL;
+	vec->items[vec->len++] =
+		(struct virtio_balloon_stat){ .tag = tag, .val = val };
+	return 0;
+}
+
+void vb_stat_clear(struct virtio_balloon *vb)
+{
+	struct virtio_balloon_stat_vec *vec = &vb->stats;
+	vec->len = 0;
+}
+
+static void vb_stats(struct virtio_balloon *vb)
+{
+	unsigned long events[NR_VM_EVENT_ITEMS] = {};
+	all_vm_events(events);
+	struct sysinfo global = {}, normal = {}, hetero = {};
+	si_meminfo(&global);
+	si_meminfo_node(&normal, first_node(node_states[N_MEMORY]));
+	si_meminfo_node(&hetero, last_node(node_states[N_MEMORY]));
+
+	// clang-format off
+	u64 items[T_MAX] = {
+		[T_SWAP_IN]       = events[PSWPIN],
+		[T_SWAP_OUT]      = events[PSWPOUT],
+		[T_MAJFLT]        = events[PGMAJFAULT],
+		[T_MINFLT]        = events[PGFAULT],
+		[T_MEMFREE]       = global.freeram * global.mem_unit,
+		[T_MEMTOT]        = global.totalram * global.mem_unit,
+		[T_AVAIL]         = si_mem_available() << PAGE_SHIFT,
+		[T_CACHES]        = global_node_page_state(NR_FILE_PAGES) << PAGE_SHIFT,
+		[T_HTLB_PGALLOC]  = events[HTLB_BUDDY_PGALLOC],
+		[T_HTLB_PGFAIL]   = events[HTLB_BUDDY_PGALLOC_FAIL],
+		// FIXME: DRAM access accounting
+		// [T_NORMAL_ACCESS] = events[DRAM_ACCESS],
+		[T_NORMAL_FREE]   = normal.freeram * normal.mem_unit,
+		[T_NORMAL_TOTAL]  = normal.totalram * normal.mem_unit,
+		// FIXME: PMEM access accounting
+		// [T_HETERO_ACCESS] = events[PMEM_ACCESS],
+		[T_HETERO_FREE]   = hetero.freeram * hetero.mem_unit,
+		[T_HETERO_TOTAL]  = hetero.totalram * hetero.mem_unit,
+	};
+	// clang-format on
+
+	vb_stat_clear(vb);
+	for (u64 i = 0; i < ARRAY_SIZE(items); ++i) {
+		vb_stat_push(vb, i, items[i]);
+	}
+
+	vb_send_buf(vb, Q_STATS, vb->stats.items,
+		    sizeof(*vb->stats.items) * ARRAY_SIZE(vb->stats.items));
+}
+
+static void vb_stats_initial(struct virtio_balloon *vb)
+{
+	if (!vb_acked(vb, F_STATS))
+		return;
+	vb_stats(vb);
+	dev_info(&vb->vdev->dev, "%s done\n", __func__);
+}
+
+static void vb_work_fn_inflate(struct work_struct *work)
+{
+	struct virtio_balloon *vb =
+		container_of(work, struct virtio_balloon, work[Q_INFLATE]);
+	struct virtio_balloon_tracepoints *t = &vb->tracepoints[Q_INFLATE];
+
+	u64 chunk_begin = local_clock(),
+	    *work_elapsed = &t->work_elapsed,
+	    *total_elapsed = &t->total_elapsed;
+
+	s64 todo = vb_inner_diff_from_target(vb, I_NORMAL);
+	if (todo <= 0)
+		return;
+	u32 done = vb_inner_inflate(vb, I_NORMAL, todo);
+
+	*work_elapsed += local_clock() - chunk_begin;
+
+	if (done < todo) {
+		queue_work(system_freezable_wq, &vb->work[Q_INFLATE]);
+	} else {
+		dev_info(&vb->vdev->dev, "%s: took %llu ms\n", __func__,
+			 *work_elapsed / 1000 / 1000);
+		*total_elapsed += *work_elapsed;
+		*work_elapsed = 0;
+	}
+}
+
+static void vb_work_fn_deflate(struct work_struct *work)
+{
+	struct virtio_balloon *vb =
+		container_of(work, struct virtio_balloon, work[Q_DEFLATE]);
+	struct virtio_balloon_tracepoints *t = &vb->tracepoints[Q_DEFLATE];
+
+	u64 chunk_begin = local_clock(),
+	    *work_elapsed = &t->work_elapsed,
+	    *total_elapsed = &t->total_elapsed;
+
+	s64 todo = -vb_inner_diff_from_target(vb, I_NORMAL);
+	if (todo <= 0)
+		return;
+	u32 done = vb_inner_deflate(vb, I_NORMAL, todo);
+
+	*work_elapsed += local_clock() - chunk_begin;
+
+	if (done < todo) {
+		queue_work(system_freezable_wq, &vb->work[Q_DEFLATE]);
+	} else {
+		dev_info(&vb->vdev->dev, "%s: took %llu ms\n", __func__,
+			 *work_elapsed / 1000 / 1000);
+		*total_elapsed += *work_elapsed;
+		*work_elapsed = 0;
+	}
+}
+
+static void vb_work_fn_stats(struct work_struct *work)
+{
+	struct virtio_balloon *vb =
+		container_of(work, struct virtio_balloon, work[Q_STATS]);
+
+	// We can only reach here by the user buffer notification callback.
+	// So we first need to remove that buffer
+	vb_recv_buf(vb, Q_STATS, NULL);
+	vb_stats(vb);
+}
+
+static void vb_work_fn_reporting(struct work_struct *work)
+{
+	struct virtio_balloon *vb =
+		container_of(work, struct virtio_balloon, work[Q_REPORTING]);
+
+	// TODO
+}
+
+static void vb_work_fn_hetero_inflate(struct work_struct *work)
+{
+	struct virtio_balloon *vb = container_of(work, struct virtio_balloon,
+						 work[Q_HETERO_INFLATE]);
+	struct virtio_balloon_tracepoints *t =
+		&vb->tracepoints[Q_HETERO_INFLATE];
+	u64 chunk_begin = local_clock(),
+	    *work_elapsed = &t->work_elapsed,
+	    *total_elapsed = &t->total_elapsed;
+	s64 todo = vb_inner_diff_from_target(vb, I_HETERO);
+	if (todo <= 0)
+		return;
+	u32 done = vb_inner_inflate(vb, I_HETERO, todo);
+	*work_elapsed += local_clock() - chunk_begin;
+	if (done < todo) {
+		queue_work(system_freezable_wq, &vb->work[Q_HETERO_INFLATE]);
+	} else {
+		dev_info(&vb->vdev->dev, "%s: took %llu ms\n", __func__,
+			 *work_elapsed / 1000 / 1000);
+		*total_elapsed += *work_elapsed;
+		*work_elapsed = 0;
+	}
+}
+
+static void vb_work_fn_hetero_deflate(struct work_struct *work)
+{
+	struct virtio_balloon *vb = container_of(work, struct virtio_balloon,
+						 work[Q_HETERO_DEFLATE]);
+	struct virtio_balloon_tracepoints *t =
+		&vb->tracepoints[Q_HETERO_DEFLATE];
+	u64 chunk_begin = local_clock(),
+	    *work_elapsed = &t->work_elapsed,
+	    *total_elapsed = &t->total_elapsed;
+	s64 todo = -vb_inner_diff_from_target(vb, I_HETERO);
+	if (todo <= 0)
+		return;
+	u32 done = vb_inner_deflate(vb, I_HETERO, todo);
+	*work_elapsed += local_clock() - chunk_begin;
+	if (done < todo) {
+		queue_work(system_freezable_wq, &vb->work[Q_HETERO_DEFLATE]);
+	} else {
+		dev_info(&vb->vdev->dev, "%s: took %llu ms\n", __func__,
+			 *work_elapsed / 1000 / 1000);
+		*total_elapsed += *work_elapsed;
+		*work_elapsed = 0;
+	}
+}
+
+static void vb_work_queue(struct virtio_balloon *vb)
+{
+	// clang-format off
+	struct work_struct *works[Q_MAX] = {
+		[Q_INFLATE] = &vb->work[Q_INFLATE],
+		[Q_DEFLATE] = &vb->work[Q_DEFLATE],
+		// CAVEAT: driven by the host's used buffer notification
+		// [Q_STATS] = vb_acked(vb, F_STATS) ? &vb->work[Q_STATS] : NULL,
+		[Q_HETERO_INFLATE] = vb_acked(vb, F_HETERO) ? &vb->work[Q_HETERO_INFLATE] : NULL,
+		[Q_HETERO_DEFLATE] = vb_acked(vb, F_HETERO) ? &vb->work[Q_HETERO_DEFLATE] : NULL,
+	};
+	// clang-format on
+	scoped_guard(spinlock_irqsave, &vb->queue_work)
+	{
+		if (atomic_read(&vb->should_exit)) {
+			return;
+		}
+		for (u64 i = 0; i < ARRAY_SIZE(works); ++i) {
+			if (!works[i] || !works[i]->func) {
+				continue;
+			}
+			queue_work(system_freezable_wq, works[i]);
+		}
+	}
+	dev_info(&vb->vdev->dev, "%s done\n", __func__);
+}
+
+static void vb_work_stop(struct virtio_balloon *vb)
+{
+	scoped_guard(spinlock_irqsave, &vb->queue_work)
+	{
+		atomic_set(&vb->should_exit, 1);
+		for (u64 i = 0; i < ARRAY_SIZE(vb->work); ++i) {
+			struct work_struct *work = &vb->work[i];
+			if (!work->func)
+				continue;
+			cancel_work_sync(work);
+		}
+	}
+}
+
+static int vb_work_init(struct virtio_balloon *vb)
+{
+	dev_info(&vb->vdev->dev, "%s started\n", __func__);
+
+	// clang-format off
+	struct work_struct *works[Q_MAX] = {
+		[Q_INFLATE]   = &vb->work[Q_INFLATE],
+		[Q_DEFLATE]   = &vb->work[Q_DEFLATE],
+		[Q_STATS]     = vb_acked(vb, F_STATS) ? &vb->work[Q_STATS] : NULL,
+		[Q_REPORTING] = vb_acked(vb, F_REPORT) ? &vb->work[Q_REPORTING] : NULL,
+		[Q_HETERO_INFLATE] = vb_acked(vb, F_HETERO) ? &vb->work[Q_HETERO_INFLATE] : NULL,
+		[Q_HETERO_DEFLATE] = vb_acked(vb, F_HETERO) ? &vb->work[Q_HETERO_DEFLATE] : NULL,
+	};
+	work_func_t fns[Q_MAX] = {
+		[Q_INFLATE]   = vb_work_fn_inflate,
+		[Q_DEFLATE]   = vb_work_fn_deflate,
+		[Q_STATS]     = vb_acked(vb, F_STATS) ? vb_work_fn_stats : NULL,
+		[Q_REPORTING] = vb_acked(vb, F_REPORT) ? vb_work_fn_reporting : NULL,
+		[Q_HETERO_INFLATE] = vb_acked(vb, F_HETERO) ? vb_work_fn_hetero_inflate : NULL,
+		[Q_HETERO_DEFLATE] = vb_acked(vb, F_HETERO) ? vb_work_fn_hetero_deflate : NULL,
+	};
+	// clang-format on
+	for (u64 i = 0; i < ARRAY_SIZE(works); ++i) {
+		if (!works[i] || !fns[i]) {
+			continue;
+		}
+		INIT_WORK(works[i], fns[i]);
+	}
+	spin_lock_init(&vb->queue_work);
+	dev_info(&vb->vdev->dev, "%s done\n", __func__);
+	return 0;
+}
+
+static int vb_vqs_init(struct virtio_balloon *vb)
+{
+	dev_info(&vb->vdev->dev, "%s started\n", __func__);
+
+	// clang-format off
+	vq_callback_t *callbacks[Q_MAX] = {
+		[Q_INFLATE]   = vb_callback_ack,
+		[Q_DEFLATE]   = vb_callback_ack,
+		[Q_STATS]     = vb_acked(vb, F_STATS) ? vb_callback_stats_request : NULL,
+		[Q_REPORTING] = vb_acked(vb, F_REPORT) ? vb_callback_ack : NULL,
+		[Q_HETERO_INFLATE] = vb_acked(vb, F_HETERO) ? vb_callback_ack : NULL,
+		[Q_HETERO_DEFLATE] = vb_acked(vb, F_HETERO) ? vb_callback_ack : NULL,
+	};
+	char const *names[Q_MAX] = {
+		[Q_INFLATE]   = "inflate",
+		[Q_DEFLATE]   = "deflate",
+		[Q_STATS]     = vb_acked(vb, F_STATS) ? "stats" : NULL,
+		[Q_REPORTING] = vb_acked(vb, F_REPORT) ? "reporting" : NULL,
+		[Q_HETERO_INFLATE] = vb_acked(vb, F_HETERO) ? "hetero-inflate" : NULL,
+		[Q_HETERO_DEFLATE] = vb_acked(vb, F_HETERO) ? "hetero-deflate" : NULL,
+	};
+	// clang-format on
+
+	TRY(virtio_find_vqs(vb->vdev, ARRAY_SIZE(vb->vqs), vb->vqs, callbacks,
+			    names, NULL));
+
+	dev_info(&vb->vdev->dev, "%s done\n", __func__);
+	return 0;
+}
+static void vb_vqs_drop(struct virtio_balloon *vb)
+{
+	vb->vdev->config->del_vqs(vb->vdev);
+}
+
+static int vb_oom(struct notifier_block *nb, unsigned long _0, void *freed)
+{
+	struct virtio_balloon *vb =
+		container_of(nb, struct virtio_balloon, oom_notification);
+
+	u32 idx = vb_acked(vb, F_HETERO) ? I_HETERO : I_NORMAL;
+	pr_warn_ratelimited("%s: deflate %d pages via %s vq\n", __func__,
+			    VIRTIO_BALLOON_OOM_NR_PAGES,
+			    idx == I_NORMAL ? "normal" : "hetero");
+	vb_inner_deflate(vb, idx, VIRTIO_BALLOON_OOM_NR_PAGES);
+	return NOTIFY_OK;
+}
+
+static int vb_init(struct virtio_balloon *vb, struct virtio_device *vdev)
+{
+	memset(vb, 0, sizeof(*vb));
+	vb->vdev = vdev;
+	vdev->priv = vb;
+	dev_info(&vdev->dev, "%s started: vb=0x%px\n", __func__, vb);
+
+	atomic_set(&vb->should_exit, 0);
+
+	TRY(vb_vqs_init(vb));
+	TRY(vb_work_init(vb));
+
+	int err = 0;
+	if (vb_acked(vb, F_OOM)) {
+		vb->oom_notification.notifier_call = vb_oom;
+		vb->oom_notification.priority =
+			VIRTIO_BALLOON_OOM_NOTIFY_PRIORITY;
+		err = register_oom_notifier(&vb->oom_notification);
+		if (err)
+			goto err_oom;
+	}
+	init_waitqueue_head(&vb->ack);
+	for (u64 i = 0; i < ARRAY_SIZE(vb->inner); ++i) {
+		struct virtio_balloon_inner *inner = &vb->inner[i];
+		mutex_init(&inner->lock);
+		page_tracker_init(&inner->tracking);
+	}
+	virtio_device_ready(vdev);
+	dev_info(&vdev->dev, "virtio-balloon device registered\n");
+	// stats queue require an initial stat item to kick-start
+	vb_stats_initial(vb);
+	// inflate/deflation starts as soon as balloon is ready
+	vb_work_queue(vb);
+
+	dev_info(&vb->vdev->dev, "%s done\n", __func__);
+	return 0;
+
+err_oom:
+	if (vb_acked(vb, F_OOM))
+		unregister_oom_notifier(&vb->oom_notification);
+err_vqs_drop:
+	vb_vqs_drop(vb);
+	return err;
+}
+
+static void vb_stop(struct virtio_balloon *vb)
+{
+	if (vb_acked(vb, F_REPORT)) {
+		// TODO: page_reporting_unregister()
+	}
+	if (vb_acked(vb, F_OOM)) {
+		unregister_oom_notifier(&vb->oom_notification);
+	}
+	vb_work_stop(vb);
+	for (u64 i = 0; i < ARRAY_SIZE(vb->inner); ++i) {
+		struct virtio_balloon_inner *inner = &vb->inner[i];
+		vb_inner_deflate(vb, i, inner->len);
+	}
+}
+
+static void vb_reset(struct virtio_balloon *vb)
+{
+	virtio_reset_device(vb->vdev);
+}
+
+static void vb_drop(struct virtio_balloon *vb)
+{
+	vb_stop(vb);
+	// TODO: mutex_destroy
+	// for (u64 i = 0; i < ARRAY_SIZE(vb->inner); ++i) {
+	// 	struct virtio_balloon_inner *inner = &vb->inner[i];
+	// 	mutex_destroy(&inner->lock);
+	// }
+	vb_reset(vb);
+	vb_vqs_drop(vb);
+	return;
+}
+
+static int validate(struct virtio_device *vdev)
+{
+	BUILD_BUG_ON(PAGE_SHIFT != VIRTIO_BALLOON_PFN_SHIFT);
+
+	if (!vdev->config->get) {
+		dev_err(&vdev->dev, "%s failure: config access disabled\n",
+			__func__);
+		return -EINVAL;
+	}
+
+	if (virtio_has_feature(vdev, F_HETERO) &&
+	    num_node_state(N_MEMORY) < 2) {
+		dev_err(&vdev->dev,
+			"%s failure: no heterogeneous memory presents\n",
+			__func__);
+		return -EINVAL;
+	}
+
+	__virtio_clear_bit(vdev, VIRTIO_F_ACCESS_PLATFORM);
+	return 0;
+}
+
+static int probe(struct virtio_device *vdev)
+{
+	if (!vdev) {
+		return -EINVAL;
+	}
+	struct virtio_balloon *vb = kvzalloc(sizeof(*vb), GFP_KERNEL);
+	if (!vb) {
+		return -ENOMEM;
+	}
+
+	TRY(vb_init(vb, vdev));
+	return 0;
+}
+
+static void config_changed(struct virtio_device *vdev)
+{
+	struct virtio_balloon *vb = vdev->priv;
+	vb_work_queue(vb);
+}
+
+static void remove(struct virtio_device *vdev)
+{
+	struct virtio_balloon *vb = vdev->priv;
+
+	vb_drop(vb);
+	kvfree(vb);
+}
+
+static int freeze(struct virtio_device *vdev)
+{
+	return -EINVAL;
+}
+
+static int restore(struct virtio_device *vdev)
+{
+	return -EINVAL;
+}
+
+static struct virtio_driver virtio_balloon_driver = {
+	.feature_table = features,
+	.feature_table_size = ARRAY_SIZE(features),
+	.driver.name = KBUILD_MODNAME,
+	.driver.owner = THIS_MODULE,
+	.id_table = id_table,
+	.validate = validate,
+	.probe = probe,
+	.remove = remove,
+	.config_changed = config_changed,
+};
+
+module_virtio_driver(virtio_balloon_driver);
+MODULE_DEVICE_TABLE(virtio, id_table);
+MODULE_AUTHOR("Junliang Hu <jlhu@cse.cuhk.edu.hk>");
+MODULE_DESCRIPTION("Enhanced Virtio balloon driver");
+MODULE_LICENSE("GPL");
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 98ff57c8eda6..2049789ef953 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -499,13 +499,23 @@ pmd_t maybe_pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma)
 }
 
 #ifdef CONFIG_MEMCG
-static inline struct deferred_split *get_deferred_split_queue(struct page *page)
+struct deferred_split *get_deferred_split_queue(struct page *page)
 {
 	struct mem_cgroup *memcg = page_memcg(compound_head(page));
 	struct pglist_data *pgdat = NODE_DATA(page_to_nid(page));
 
 	if (memcg)
+#ifdef CONFIG_HTMM
+	{
+		if (memcg->htmm_enabled) {
+			struct mem_cgroup_per_node *pn = memcg->nodeinfo[page_to_nid(page)];
+			return &pn->deferred_split_queue;
+		} else
+			return &memcg->deferred_split_queue;
+	}
+#else
 		return &memcg->deferred_split_queue;
+#endif
 	else
 		return &pgdat->deferred_split_queue;
 }
@@ -657,6 +667,12 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf,
 		spin_unlock(vmf->ptl);
 		count_vm_event(THP_FAULT_ALLOC);
 		count_memcg_event_mm(vma->vm_mm, THP_FAULT_ALLOC);
+#ifdef CONFIG_HTMM
+		if (page != NULL && node_is_toptier(page_to_nid(page)))
+			count_vm_events(HTMM_ALLOC_DRAM, HPAGE_PMD_NR);
+		else
+			count_vm_events(HTMM_ALLOC_NVM, HPAGE_PMD_NR);
+#endif
 	}
 
 	return 0;
@@ -779,7 +795,11 @@ vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 		count_vm_event(THP_FAULT_FALLBACK);
 		return VM_FAULT_FALLBACK;
 	}
+#ifdef CONFIG_HTMM
+	prep_transhuge_page_for_htmm(vma, page);
+#else
 	prep_transhuge_page(page);
+#endif
 	return __do_huge_pmd_anonymous_page(vmf, page, gfp);
 }
 
@@ -1610,6 +1630,9 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,
 
 		if (pmd_present(orig_pmd)) {
 			page = pmd_page(orig_pmd);
+#ifdef CONFIG_HTMM
+			uncharge_htmm_page(page, get_mem_cgroup_from_mm(vma->vm_mm));
+#endif
 			page_remove_rmap(page, true);
 			VM_BUG_ON_PAGE(page_mapcount(page) < 0, page);
 			VM_BUG_ON_PAGE(!PageHead(page), page);
@@ -1766,16 +1789,24 @@ int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 	}
 #endif
 
-	/*
-	 * Avoid trapping faults against the zero page. The read-only
-	 * data is likely to be read-cached on the local CPU and
-	 * local/remote hits to the zero page are not interesting.
-	 */
-	if (prot_numa && is_huge_zero_pmd(*pmd))
-		goto unlock;
+	if (prot_numa) {
+		struct page *page;
+		/*
+		 * Avoid trapping faults against the zero page. The read-only
+		 * data is likely to be read-cached on the local CPU and
+		 * local/remote hits to the zero page are not interesting.
+		 */
+		if (is_huge_zero_pmd(*pmd))
+			goto unlock;
 
-	if (prot_numa && pmd_protnone(*pmd))
-		goto unlock;
+		if (pmd_protnone(*pmd))
+			goto unlock;
+
+		/* skip scanning toptier node */
+		page = pmd_page(*pmd);
+		if (numa_promotion_tiered_enabled && node_is_toptier(page_to_nid(page)))
+			goto unlock;
+	}
 
 	/*
 	 * In case prot_numa, we are under mmap_read_lock(mm). It's critical
@@ -2107,7 +2138,45 @@ static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,
 			atomic_inc(&page[i]._mapcount);
 		pte_unmap(pte);
 	}
+#ifdef CONFIG_HTMM
+	/* pginfo-s managed by the huge page should be copied into pte->pginfo*/
+	if (PageHtmm(&page[3])) {
+		struct mem_cgroup *memcg = get_mem_cgroup_from_mm(mm);
+		pte_t *pte = pte_offset_map(&_pmd, haddr);
+
+		SetPageHtmm(&page[0]);
 
+		for (i = 0, addr = haddr; i < HPAGE_PMD_NR;
+		     i++, addr += PAGE_SIZE) {
+			pginfo_t *pte_pginfo, *tail_pginfo;
+
+			pte_pginfo = get_pginfo_from_pte(&pte[i]);
+			tail_pginfo = get_compound_pginfo(page, addr);
+			if (!pte_pginfo || !tail_pginfo) {
+				printk("split - pginfo - none...\n");
+				goto skip_copy_pginfo;
+			}
+
+			pte_pginfo->nr_accesses = tail_pginfo->nr_accesses;
+			pte_pginfo->total_accesses =
+				tail_pginfo->total_accesses;
+			pte_pginfo->cooling_clock = tail_pginfo->cooling_clock;
+
+			if (get_idx(pte_pginfo->total_accesses) >=
+			    (memcg->active_threshold - 1))
+				SetPageActive(&page[i]);
+			else
+				ClearPageActive(&page[i]);
+
+			spin_lock(&memcg->access_lock);
+			memcg->hotness_hg[get_idx(pte_pginfo->total_accesses)]++;
+			spin_unlock(&memcg->access_lock);
+			/* Htmm flag will be cleared later */
+			/* ClearPageHtmm(&page[i]); */
+		}
+	}
+skip_copy_pginfo:
+#endif
 	if (!pmd_migration) {
 		/*
 		 * Set PG_double_map before dropping compound_mapcount to avoid
@@ -2303,7 +2372,7 @@ static void unmap_page(struct page *page)
 	VM_WARN_ON_ONCE_PAGE(page_mapped(page), page);
 }
 
-static void remap_page(struct page *page, unsigned int nr)
+static void remap_page(struct page *page, unsigned int nr, bool unmap_clean)
 {
 	int i;
 
@@ -2311,10 +2380,10 @@ static void remap_page(struct page *page, unsigned int nr)
 	if (!PageAnon(page))
 		return;
 	if (PageTransHuge(page)) {
-		remove_migration_ptes(page, page, true);
+		remove_migration_ptes(page, page, true, unmap_clean);
 	} else {
 		for (i = 0; i < nr; i++)
-			remove_migration_ptes(page + i, page + i, true);
+			remove_migration_ptes(page + i, page + i, true, unmap_clean);
 	}
 }
 
@@ -2343,7 +2412,13 @@ static void __split_huge_page_tail(struct page *head, int tail,
 		struct lruvec *lruvec, struct list_head *list)
 {
 	struct page *page_tail = head + tail;
-
+#ifdef CONFIG_HTMM
+	bool htmm_tail = PageHtmm(page_tail) ? true : false;
+	bool htmm_active_tail = PageHtmm(head) && PageActive(page_tail);
+#else
+	bool htmm_tail = false;
+	bool htmm_active_tail = false;
+#endif
 	VM_BUG_ON_PAGE(atomic_read(&page_tail->_mapcount) != -1, page_tail);
 
 	/*
@@ -2365,12 +2440,24 @@ static void __split_huge_page_tail(struct page *head, int tail,
 			 (1L << PG_unevictable) |
 #ifdef CONFIG_64BIT
 			 (1L << PG_arch_2) |
+#endif
+#ifdef CONFIG_HTMM
+	// (1L << PG_htmm) |
 #endif
 			 (1L << PG_dirty)));
 
 	/* ->mapping in first tail page is compound_mapcount */
-	VM_BUG_ON_PAGE(tail > 2 && page_tail->mapping != TAIL_MAPPING,
-			page_tail);
+	//VM_BUG_ON_PAGE(tail > 2 && !PageHtmm(head) && !htmm_tail && page_tail->mapping != TAIL_MAPPING,
+	//			page_tail);
+
+#ifdef CONFIG_HTMM
+	if (htmm_tail)
+		clear_transhuge_pginfo(page_tail);
+	if (htmm_active_tail)
+		SetPageActive(page_tail);
+	else
+		ClearPageActive(page_tail);
+#endif
 	page_tail->mapping = head->mapping;
 	page_tail->index = head->index + tail;
 
@@ -2412,6 +2499,8 @@ static void __split_huge_page(struct page *page, struct list_head *list,
 	struct address_space *swap_cache = NULL;
 	unsigned long offset = 0;
 	unsigned int nr = thp_nr_pages(head);
+	LIST_HEAD(pages_to_free);
+	int nr_pages_to_free = 0;
 	int i;
 
 	/* complete memcg works before add pages to LRU */
@@ -2447,7 +2536,9 @@ static void __split_huge_page(struct page *page, struct list_head *list,
 					head + i, 0);
 		}
 	}
-
+#ifdef CONFIG_HTMM
+	ClearPageHtmm(head);
+#endif
 	ClearPageCompound(head);
 	unlock_page_lruvec(lruvec);
 	/* Caller disabled irqs, so they are still disabled here */
@@ -2470,7 +2561,7 @@ static void __split_huge_page(struct page *page, struct list_head *list,
 	}
 	local_irq_enable();
 
-	remap_page(head, nr);
+	remap_page(head, nr, PageAnon(head));
 
 	if (PageSwapCache(head)) {
 		swp_entry_t entry = { .val = page_private(head) };
@@ -2484,6 +2575,34 @@ static void __split_huge_page(struct page *page, struct list_head *list,
 			continue;
 		unlock_page(subpage);
 
+		/*
+		 * If a tail page has only two references left, one inherited
+		 * from the isolation of its head and the other from
+		 * lru_add_page_tail() which we are about to drop, it means this
+		 * tail page was concurrently zapped. Then we can safely free it
+		 * and save page reclaim or migration the trouble of trying it.
+		 */
+		if (list && page_ref_freeze(subpage, 2)) {
+			VM_BUG_ON_PAGE(PageLRU(subpage), subpage);
+			VM_BUG_ON_PAGE(PageCompound(subpage), subpage);
+			VM_BUG_ON_PAGE(page_mapped(subpage), subpage);
+
+			ClearPageActive(subpage);
+			ClearPageUnevictable(subpage);
+			list_move(&subpage->lru, &pages_to_free);
+			nr_pages_to_free++;
+			continue;
+		}
+
+		/*
+		 * If a tail page has only one reference left, it will be freed
+		 * by the call to free_page_and_swap_cache below. Since zero
+		 * subpages are no longer remapped, there will only be one
+		 * reference left in cases outside of reclaim or migration.
+		 */
+		if (page_ref_count(subpage) == 1)
+			nr_pages_to_free++;
+
 		/*
 		 * Subpages may be freed if there wasn't any mapping
 		 * like if add_to_swap() is running on a lru page that
@@ -2493,6 +2612,12 @@ static void __split_huge_page(struct page *page, struct list_head *list,
 		 */
 		put_page(subpage);
 	}
+
+	if (!nr_pages_to_free)
+		return;
+
+	mem_cgroup_uncharge_list(&pages_to_free);
+	free_unref_page_list(&pages_to_free);
 }
 
 int total_mapcount(struct page *page)
@@ -2718,7 +2843,22 @@ int split_huge_page_to_list(struct page *page, struct list_head *list)
 				filemap_nr_thps_dec(mapping);
 			}
 		}
+#ifdef CONFIG_HTMM
+		{
+			struct mem_cgroup *memcg = page_memcg(head);
+			unsigned int idx;
+
+			spin_lock(&memcg->access_lock);
+			idx = head[3].idx;
 
+			if (memcg->hotness_hg[idx] < HPAGE_PMD_NR)
+				memcg->hotness_hg[idx] = 0;
+			else
+				memcg->hotness_hg[idx] -= HPAGE_PMD_NR;
+
+			spin_unlock(&memcg->access_lock);
+		}
+#endif
 		__split_huge_page(page, list, end);
 		ret = 0;
 	} else {
@@ -2727,7 +2867,20 @@ int split_huge_page_to_list(struct page *page, struct list_head *list)
 		if (mapping)
 			xa_unlock(&mapping->i_pages);
 		local_irq_enable();
-		remap_page(head, thp_nr_pages(head));
+		remap_page(head, thp_nr_pages(head), false);
+#if 0 //def CONFIG_HTMM
+		{
+		    struct mem_cgroup *memcg = page_memcg(head);
+		    if (memcg->htmm_enabled) {
+			spin_lock(&ds_queue->split_queue_lock);
+			if (!list_empty(page_deferred_list(head))) {
+			    ds_queue->split_queue_len--;
+			    list_del(page_deferred_list(head));
+			}
+			spin_unlock(&ds_queue->split_queue_lock);
+		    }
+		}
+#endif
 		ret = -EBUSY;
 	}
 
@@ -2823,8 +2976,68 @@ static unsigned long deferred_split_scan(struct shrinker *shrink,
 #ifdef CONFIG_MEMCG
 	if (sc->memcg)
 		ds_queue = &sc->memcg->deferred_split_queue;
-#endif
+#ifdef CONFIG_HTMM
+	if (sc->memcg) {
+		int nid, nr_nonempty = 0;
+
+		for_each_node_state (nid, N_MEMORY) {
+			struct mem_cgroup_per_node *pn =
+				sc->memcg->nodeinfo[nid];
+			struct deferred_split *pn_ds_queue =
+				&pn->deferred_split_queue;
+
+			if (list_empty(&pn_ds_queue->split_queue))
+				continue;
+
+			spin_lock_irqsave(&pn_ds_queue->split_queue_lock,
+					  flags);
+			list_for_each_safe (pos, next,
+					    &pn_ds_queue->split_queue) {
+				page = list_entry((void *)pos, struct page,
+						  deferred_list);
+				page = compound_head(page);
+				if (get_page_unless_zero(page))
+					list_move(page_deferred_list(page),
+						  &list);
+				else {
+					list_del_init(page_deferred_list(page));
+					pn_ds_queue->split_queue_len--;
+				}
+				if (!--sc->nr_to_scan)
+					break;
+			}
+			spin_unlock_irqrestore(&pn_ds_queue->split_queue_lock,
+					       flags);
+			list_for_each_safe (pos, next, &list) {
+				page = list_entry((void *)pos, struct page,
+						  deferred_list);
+				if (!trylock_page(page))
+					goto next;
+				/* split_huge_page() removes page from list on success */
+				if (!split_huge_page(page))
+					split++;
+				unlock_page(page);
+			next:
+				put_page(page);
+			}
+
+			spin_lock_irqsave(&pn_ds_queue->split_queue_lock,
+					  flags);
+			list_splice_tail(&list, &pn_ds_queue->split_queue);
+			spin_unlock_irqrestore(&pn_ds_queue->split_queue_lock,
+					       flags);
 
+			if (!list_empty(&pn_ds_queue->split_queue))
+				nr_nonempty++;
+		}
+
+		if (!split && !nr_nonempty)
+			return SHRINK_STOP;
+	}
+	return split;
+#endif
+#endif
+#ifndef CONFIG_HTMM
 	spin_lock_irqsave(&ds_queue->split_queue_lock, flags);
 	/* Take pin on all head pages to avoid freeing them under us */
 	list_for_each_safe(pos, next, &ds_queue->split_queue) {
@@ -2865,6 +3078,7 @@ static unsigned long deferred_split_scan(struct shrinker *shrink,
 	if (!split && list_empty(&ds_queue->split_queue))
 		return SHRINK_STOP;
 	return split;
+#endif
 }
 
 static struct shrinker deferred_split_shrinker = {
@@ -3215,6 +3429,11 @@ void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)
 	else
 		page_add_file_rmap(new, true);
 	set_pmd_at(mm, mmun_start, pvmw->pmd, pmde);
+#ifdef CONFIG_HTMM
+	{
+		check_transhuge_cooling(NULL, new, true);
+	}
+#endif
 	if ((vma->vm_flags & VM_LOCKED) && !PageDoubleMap(new))
 		mlock_vma_page(new);
 	update_mmu_cache_pmd(vma, address, pvmw->pmd);
diff --git a/mm/internal.h b/mm/internal.h
index cf3cb933eba3..5961b78e316a 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -12,6 +12,16 @@
 #include <linux/pagemap.h>
 #include <linux/tracepoint-defs.h>
 
+#ifdef CONFIG_NOMAD
+#include <linux/nomad.h>
+#endif
+
+#ifdef CONFIG_HTMM
+#include <linux/htmm.h>
+#endif
+
+#include "profile.h"
+
 /*
  * The set of flags that only affect watermark checking and reclaim
  * behaviour. This is used by the MM to obey the caller constraints
diff --git a/mm/khugepaged.c b/mm/khugepaged.c
index 203792e70ac1..e078ea07db38 100644
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@ -858,7 +858,6 @@ static int khugepaged_find_target_node(void)
 				target_node = nid;
 				break;
 			}
-
 	last_khugepaged_target_node = target_node;
 	return target_node;
 }
@@ -1060,10 +1059,9 @@ static bool __collapse_huge_page_swapin(struct mm_struct *mm,
 	return true;
 }
 
-static void collapse_huge_page(struct mm_struct *mm,
-				   unsigned long address,
-				   struct page **hpage,
-				   int node, int referenced, int unmapped)
+static int collapse_huge_page(struct mm_struct *mm, unsigned long address,
+			      struct page **hpage, int node, int referenced,
+			      int unmapped)
 {
 	LIST_HEAD(compound_pagelist);
 	pmd_t *pmd, _pmd;
@@ -1075,6 +1073,7 @@ static void collapse_huge_page(struct mm_struct *mm,
 	struct vm_area_struct *vma;
 	struct mmu_notifier_range range;
 	gfp_t gfp;
+	int ret = 0;
 
 	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 
@@ -1088,7 +1087,35 @@ static void collapse_huge_page(struct mm_struct *mm,
 	 * that. We will recheck the vma after taking it again in write mode.
 	 */
 	mmap_read_unlock(mm);
+#ifdef CONFIG_HTMM
+	/* check whether there is enough free space in target memory node */
+	if (node_is_toptier(node)) {
+		struct mem_cgroup *memcg = get_mem_cgroup_from_mm(mm);
+		unsigned long max_nr_pages, cur_nr_pages;
+		pg_data_t *pgdat;
+
+		if (!memcg || !memcg->htmm_enabled)
+			goto normal_exec;
+
+		pgdat = NODE_DATA(node);
+		cur_nr_pages = get_nr_lru_pages_node(memcg, pgdat);
+		max_nr_pages = memcg->nodeinfo[node]->max_nr_base_pages;
+
+		if (max_nr_pages == ULONG_MAX)
+			goto normal_exec;
+		else if (cur_nr_pages + HPAGE_PMD_NR <= max_nr_pages)
+			goto normal_exec;
+		else {
+			result = SCAN_ALLOC_HUGE_PAGE_FAIL;
+			//ret = 1;
+			goto out_nolock;
+		}
+	}
+normal_exec:
 	new_page = khugepaged_alloc_page(hpage, gfp, node);
+#else
+	new_page = khugepaged_alloc_page(hpage, gfp, node);
+#endif
 	if (!new_page) {
 		result = SCAN_ALLOC_HUGE_PAGE_FAIL;
 		goto out_nolock;
@@ -1223,7 +1250,7 @@ static void collapse_huge_page(struct mm_struct *mm,
 	if (!IS_ERR_OR_NULL(*hpage))
 		mem_cgroup_uncharge(*hpage);
 	trace_mm_collapse_huge_page(mm, isolated, result);
-	return;
+	return ret;
 }
 
 static int khugepaged_scan_pmd(struct mm_struct *mm,
@@ -1240,6 +1267,9 @@ static int khugepaged_scan_pmd(struct mm_struct *mm,
 	spinlock_t *ptl;
 	int node = NUMA_NO_NODE, unmapped = 0;
 	bool writable = false;
+#ifdef CONFIG_HTMM
+	pginfo_t *pginfo;
+#endif
 
 	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 
@@ -1295,7 +1325,16 @@ static int khugepaged_scan_pmd(struct mm_struct *mm,
 		}
 		if (pte_write(pteval))
 			writable = true;
+#ifdef CONFIG_HTMM
+		if (mm->htmm_enabled) {
+			pginfo = get_pginfo_from_pte(_pte);
+			if (!pginfo)
+				goto out_unmap;
 
+			if (!pginfo->may_hot)
+				goto out_unmap;
+		}
+#endif
 		page = vm_normal_page(vma, _address, pteval);
 		if (unlikely(!page)) {
 			result = SCAN_PAGE_NULL;
@@ -1374,8 +1413,8 @@ static int khugepaged_scan_pmd(struct mm_struct *mm,
 	if (ret) {
 		node = khugepaged_find_target_node();
 		/* collapse_huge_page will return with the mmap_lock released */
-		collapse_huge_page(mm, address, hpage, node,
-				referenced, unmapped);
+		ret += collapse_huge_page(mm, address, hpage, node, referenced,
+					  unmapped);
 	}
 out:
 	trace_mm_khugepaged_scan_pmd(mm, page, writable, referenced,
@@ -1676,9 +1715,8 @@ static void retract_page_tables(struct address_space *mapping, pgoff_t pgoff)
  *    + restore gaps in the page cache;
  *    + unlock and free huge page;
  */
-static void collapse_file(struct mm_struct *mm,
-		struct file *file, pgoff_t start,
-		struct page **hpage, int node)
+static int collapse_file(struct mm_struct *mm, struct file *file, pgoff_t start,
+			 struct page **hpage, int node)
 {
 	struct address_space *mapping = file->f_mapping;
 	gfp_t gfp;
@@ -1688,15 +1726,42 @@ static void collapse_file(struct mm_struct *mm,
 	XA_STATE_ORDER(xas, &mapping->i_pages, start, HPAGE_PMD_ORDER);
 	int nr_none = 0, result = SCAN_SUCCEED;
 	bool is_shmem = shmem_file(file);
-	int nr;
+	int nr, ret = 0;
 
 	VM_BUG_ON(!IS_ENABLED(CONFIG_READ_ONLY_THP_FOR_FS) && !is_shmem);
 	VM_BUG_ON(start & (HPAGE_PMD_NR - 1));
 
 	/* Only allocate from the target node */
 	gfp = alloc_hugepage_khugepaged_gfpmask() | __GFP_THISNODE;
-
+#ifdef CONFIG_HTMM
+	/* check whether there is enough free space in target memory node */
+	if (node_is_toptier(node)) {
+		struct mem_cgroup *memcg = get_mem_cgroup_from_mm(mm);
+		unsigned long max_nr_pages, cur_nr_pages;
+		pg_data_t *pgdat;
+
+		if (!memcg || !memcg->htmm_enabled)
+			goto normal_exec_file;
+
+		pgdat = NODE_DATA(node);
+		cur_nr_pages = get_nr_lru_pages_node(memcg, pgdat);
+		max_nr_pages = memcg->nodeinfo[node]->max_nr_base_pages;
+
+		if (max_nr_pages == ULONG_MAX)
+			goto normal_exec_file;
+		else if (cur_nr_pages + HPAGE_PMD_NR <= max_nr_pages)
+			goto normal_exec_file;
+		else {
+			result = SCAN_ALLOC_HUGE_PAGE_FAIL;
+			//ret = 1;
+			goto out;
+		}
+	}
+normal_exec_file:
+	new_page = khugepaged_alloc_page(hpage, gfp, node);
+#else
 	new_page = khugepaged_alloc_page(hpage, gfp, node);
+#endif
 	if (!new_page) {
 		result = SCAN_ALLOC_HUGE_PAGE_FAIL;
 		goto out;
@@ -1979,6 +2044,7 @@ static void collapse_file(struct mm_struct *mm,
 		*hpage = NULL;
 
 		khugepaged_pages_collapsed++;
+		ret = 2;
 	} else {
 		struct page *page;
 
@@ -2026,10 +2092,11 @@ static void collapse_file(struct mm_struct *mm,
 	if (!IS_ERR_OR_NULL(*hpage))
 		mem_cgroup_uncharge(*hpage);
 	/* TODO: tracepoints */
+	return ret;
 }
 
-static void khugepaged_scan_file(struct mm_struct *mm,
-		struct file *file, pgoff_t start, struct page **hpage)
+static int khugepaged_scan_file(struct mm_struct *mm, struct file *file,
+				pgoff_t start, struct page **hpage)
 {
 	struct page *page = NULL;
 	struct address_space *mapping = file->f_mapping;
@@ -2097,15 +2164,16 @@ static void khugepaged_scan_file(struct mm_struct *mm,
 			result = SCAN_EXCEED_NONE_PTE;
 		} else {
 			node = khugepaged_find_target_node();
-			collapse_file(mm, file, start, hpage, node);
+			return 1 + collapse_file(mm, file, start, hpage, node);
 		}
 	}
 
 	/* TODO: tracepoints */
+	return 1;
 }
 #else
-static void khugepaged_scan_file(struct mm_struct *mm,
-		struct file *file, pgoff_t start, struct page **hpage)
+static int khugepaged_scan_file(struct mm_struct *mm, struct file *file,
+				pgoff_t start, struct page **hpage)
 {
 	BUILD_BUG();
 }
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index 6f969ba0d688..2b9b2a4539ba 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -1307,7 +1307,7 @@ void mem_cgroup_update_lru_size(struct lruvec *lruvec, enum lru_list lru,
 	if (WARN_ONCE(size < 0,
 		"%s(%p, %d, %d): lru_size %ld\n",
 		__func__, lruvec, lru, nr_pages, size)) {
-		VM_BUG_ON(1);
+		//VM_BUG_ON(1);
 		*lru_size = 0;
 	}
 
@@ -5151,6 +5151,18 @@ static int alloc_mem_cgroup_per_node_info(struct mem_cgroup *memcg, int node)
 	pn->usage_in_excess = 0;
 	pn->on_tree = false;
 	pn->memcg = memcg;
+#ifdef CONFIG_HTMM /* alloc_mem_cgroup_per_node_info() */
+	pn->max_nr_base_pages = ULONG_MAX;
+	INIT_LIST_HEAD(&pn->kmigraterd_list);
+	pn->need_cooling = false;
+	pn->need_adjusting = false;
+	pn->need_adjusting_all = false;
+	pn->need_demotion = false;
+	spin_lock_init(&pn->deferred_split_queue.split_queue_lock);
+	INIT_LIST_HEAD(&pn->deferred_split_queue.split_queue);
+	INIT_LIST_HEAD(&pn->deferred_list);
+	pn->deferred_split_queue.split_queue_len = 0;
+#endif
 
 	memcg->nodeinfo[node] = pn;
 	return 0;
@@ -5171,8 +5183,12 @@ static void __mem_cgroup_free(struct mem_cgroup *memcg)
 {
 	int node;
 
-	for_each_node(node)
+	for_each_node (node) {
+#ifdef CONFIG_HTMM
+		del_memcg_from_kmigraterd(memcg, node);
+#endif
 		free_mem_cgroup_per_node_info(memcg, node);
+	}
 	free_percpu(memcg->vmstats_percpu);
 	kfree(memcg);
 }
@@ -5240,6 +5256,43 @@ static struct mem_cgroup *mem_cgroup_alloc(void)
 	spin_lock_init(&memcg->deferred_split_queue.split_queue_lock);
 	INIT_LIST_HEAD(&memcg->deferred_split_queue.split_queue);
 	memcg->deferred_split_queue.split_queue_len = 0;
+#endif
+#ifdef CONFIG_HTMM /* mem_cgroup_alloc() */
+	memcg->htmm_enabled = false;
+	memcg->max_nr_dram_pages = ULONG_MAX;
+	memcg->nr_active_pages = 0;
+	memcg->nr_sampled = 0;
+	memcg->nr_dram_sampled = 0;
+	memcg->prev_dram_sampled = 0;
+	memcg->max_dram_sampled = 0;
+	memcg->prev_max_dram_sampled = 0;
+	memcg->nr_max_sampled = 0;
+	/* thresholds */
+	memcg->active_threshold = htmm_thres_hot;
+	memcg->warm_threshold = htmm_thres_hot;
+	memcg->bp_active_threshold = htmm_thres_hot;
+
+	/* split */
+	memcg->split_threshold = 21;
+	memcg->split_active_threshold = 16;
+	memcg->nr_split = 0;
+	memcg->nr_split_tail_idx = 0;
+	memcg->sum_util = 0;
+	memcg->num_util = 0;
+
+	for (i = 0; i < 21; i++)
+		memcg->access_map[i] = 0;
+	for (i = 0; i < 16; i++) {
+		memcg->hotness_hg[i] = 0;
+		memcg->ebp_hotness_hg[i] = 0;
+	}
+
+	spin_lock_init(&memcg->access_lock);
+	memcg->cooled = false;
+	memcg->split_happen = false;
+	memcg->need_split = false;
+	memcg->cooling_clock = 0;
+	memcg->nr_alloc = 0;
 #endif
 	idr_replace(&mem_cgroup_idr, memcg, memcg->id.id);
 	return memcg;
@@ -6776,6 +6829,9 @@ int __mem_cgroup_charge(struct page *page, struct mm_struct *mm,
 
 	memcg = get_mem_cgroup_from_mm(mm);
 	ret = charge_memcg(page, memcg, gfp_mask);
+#ifdef CONFIG_HTMM
+	//charge_htmm_page(page, memcg);
+#endif
 	css_put(&memcg->css);
 
 	return ret;
@@ -6958,7 +7014,9 @@ void __mem_cgroup_uncharge(struct page *page)
 	/* Don't touch page->lru of any random page, pre-check: */
 	if (!page_memcg(page))
 		return;
-
+#ifdef CONFIG_HTMM
+		//uncharge_htmm_page(page, page_memcg(page));
+#endif
 	uncharge_gather_clear(&ug);
 	uncharge_page(page, &ug);
 	uncharge_batch(&ug);
diff --git a/mm/memory.c b/mm/memory.c
index 99d15abe4a06..84f8bb321b34 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1369,6 +1369,9 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 				    likely(!(vma->vm_flags & VM_SEQ_READ)))
 					mark_page_accessed(page);
 			}
+#ifdef CONFIG_HTMM
+			uncharge_htmm_pte(pte, get_mem_cgroup_from_mm(vma->vm_mm));
+#endif
 			rss[mm_counter(page)]--;
 			page_remove_rmap(page, false);
 			if (unlikely(page_mapcount(page) < 0))
@@ -2981,6 +2984,11 @@ static inline void wp_page_reuse(struct vm_fault *vmf)
 	flush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));
 	entry = pte_mkyoung(vmf->orig_pte);
 	entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+#ifdef CONFIG_NOMAD
+	if (pte_orig_writable(entry)) {
+		entry = pte_clear_orig_writable(entry);
+	}
+#endif
 	if (ptep_set_access_flags(vma, vmf->address, vmf->pte, entry, 1))
 		update_mmu_cache(vma, vmf->address, vmf->pte);
 	pte_unmap_unlock(vmf->pte, vmf->ptl);
@@ -3074,6 +3082,11 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
 		entry = mk_pte(new_page, vma->vm_page_prot);
 		entry = pte_sw_mkyoung(entry);
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+#ifdef CONFIG_NOMAD
+		if (pte_orig_writable(entry)) {
+			entry = pte_clear_orig_writable(entry);
+		}
+#endif
 
 		/*
 		 * Clear the pte entry and flush it first, before updating the
@@ -3271,6 +3284,14 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 {
 	struct vm_area_struct *vma = vmf->vma;
 
+#ifdef CONFIG_NOMAD
+	struct page *shadow_page = NULL;
+	struct {
+		atomic64_t *wp_counter_ptr;
+		atomic64_t *cow_counter_ptr;
+	} counters = { .wp_counter_ptr = NULL, .cow_counter_ptr = NULL };
+#endif
+
 	if (userfaultfd_pte_wp(vma, *vmf->pte)) {
 		pte_unmap_unlock(vmf->pte, vmf->ptl);
 		return handle_userfault(vmf, VM_UFFD_WP);
@@ -3308,6 +3329,25 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 	if (PageAnon(vmf->page)) {
 		struct page *page = vmf->page;
 
+#ifdef CONFIG_NOMAD
+		/**
+		 * whether this is a write to a shadow page write protected
+		 * page or a COW page, we should release the shadow page, if any
+		 * 1. for a WP page because of shadow paging, definitely we release it
+		 * 2. for a COW page, not sure about why. but at least it doesn't make
+		 * correctness worse.
+		*/
+		if (async_mod_glob_ctrl.initialized) {
+			if (async_mod_glob_ctrl.release_shadow_page) {
+				shadow_page =
+					async_mod_glob_ctrl.release_shadow_page(
+						page, &counters, true);
+			} else {
+				BUG();
+			}
+		}
+#endif
+
 		/* PageKsm() doesn't necessarily raise the page refcount */
 		if (PageKsm(page) || page_count(page) != 1)
 			goto copy;
@@ -3324,6 +3364,13 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 		 */
 		unlock_page(page);
 		wp_page_reuse(vmf);
+#ifdef CONFIG_NOMAD
+		if (counters.wp_counter_ptr) {
+			atomic64_inc(counters.wp_counter_ptr);
+		}
+		if (shadow_page)
+			put_page(shadow_page);
+#endif
 		return VM_FAULT_WRITE;
 	} else if (unlikely((vma->vm_flags & (VM_WRITE|VM_SHARED)) ==
 					(VM_WRITE|VM_SHARED))) {
@@ -3842,13 +3889,26 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 		put_page(page);
 		return handle_userfault(vmf, VM_UFFD_MISSING);
 	}
-
+#ifdef CONFIG_HTMM
+	do {
+		struct mem_cgroup *memcg = get_mem_cgroup_from_mm(vma->vm_mm);
+		if (!memcg) {
+			ClearPageActive(page);
+		}
+	} while (0);
+	if (page != NULL && node_is_toptier(page_to_nid(page)))
+		count_vm_event(HTMM_ALLOC_DRAM);
+	else
+		count_vm_event(HTMM_ALLOC_NVM);
+#endif
 	inc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);
 	page_add_new_anon_rmap(page, vma, vmf->address, false);
 	lru_cache_add_inactive_or_unevictable(page, vma);
 setpte:
 	set_pte_at(vma->vm_mm, vmf->address, vmf->pte, entry);
-
+#ifdef CONFIG_HTMM
+	set_page_coolstatus(page, vmf->pte, vma->vm_mm);
+#endif
 	/* No need to invalidate - it was non-present before */
 	update_mmu_cache(vma, vmf->address, vmf->pte);
 unlock:
@@ -4384,11 +4444,12 @@ int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
 		*flags |= TNF_FAULT_LOCAL;
 	}
 
-	return mpol_misplaced(page, vma, addr);
+	return mpol_misplaced(page, vma, addr, *flags);
 }
 
 static vm_fault_t do_numa_page(struct vm_fault *vmf)
 {
+	guard(vmstat_stopwatch)(HINT_FAULT_NS);
 	struct vm_area_struct *vma = vmf->vma;
 	struct page *page = NULL;
 	int page_nid = NUMA_NO_NODE;
@@ -4442,6 +4503,32 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)
 
 	last_cpupid = page_cpupid_last(page);
 	page_nid = page_to_nid(page);
+
+#ifdef CONFIG_NOMAD
+	if (async_mod_glob_ctrl.queue_page_fault) {
+		int nid = 0;
+		if (page_nid == numa_node_id()) {
+			count_vm_numa_event(NUMA_HINT_FAULTS_LOCAL);
+			flags |= TNF_FAULT_LOCAL;
+		}
+		nid = mpol_misplaced(page, vma, vmf->address, flags);
+		async_mod_glob_ctrl.queue_page_fault(page, vmf->pte, nid);
+		goto out_map;
+	}
+#endif
+
+	/* Only migrate pages that are active on non-toptier node */
+	if (numa_promotion_tiered_enabled &&
+		!node_is_toptier(page_nid) &&
+		!PageActive(page)) {
+		count_vm_numa_event(NUMA_HINT_FAULTS);
+		if (page_nid == numa_node_id())
+			count_vm_numa_event(NUMA_HINT_FAULTS_LOCAL);
+		mark_page_accessed(page);
+		pte_unmap_unlock(vmf->pte, vmf->ptl);
+		goto out;
+	}
+
 	target_nid = numa_migrate_prep(page, vma, vmf->address, page_nid,
 			&flags);
 	if (target_nid == NUMA_NO_NODE) {
@@ -4624,6 +4711,14 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 	if (!pte_present(vmf->orig_pte))
 		return do_swap_page(vmf);
 
+#ifdef CONFIG_NOMAD
+		// // expanded definition of NUMA pages, to keep two copies of read-only page
+		// // we some times tag the page as read-only
+		// if ((pte_protnone(vmf->orig_pte) || !pte_write(vmf->orig_pte)) &&
+		//     vma_is_accessible(vmf->vma))
+		// 	return do_numa_page(vmf);
+		//
+#endif
 	if (pte_protnone(vmf->orig_pte) && vma_is_accessible(vmf->vma))
 		return do_numa_page(vmf);
 
@@ -4682,6 +4777,9 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 	pgd_t *pgd;
 	p4d_t *p4d;
 	vm_fault_t ret;
+#ifdef CONFIG_NOMAD
+	mm->cpu_trap_nr += 1;
+#endif
 
 	pgd = pgd_offset(mm, address);
 	p4d = p4d_alloc(mm, pgd, address);
diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c
index 2d8e9fb4ce0b..bf98f6dcaf98 100644
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@ -37,6 +37,10 @@
 #include <linux/compaction.h>
 #include <linux/rmap.h>
 
+#ifdef CONFIG_HTMM /* include header */
+#include <linux/memcontrol.h>
+#endif
+
 #include <asm/tlbflush.h>
 
 #include "internal.h"
@@ -1147,11 +1151,14 @@ int __ref online_pages(unsigned long pfn, unsigned long nr_pages,
 
 	kswapd_run(nid);
 	kcompactd_run(nid);
-
 	writeback_set_ratelimit();
 
 	memory_notify(MEM_ONLINE, &arg);
 	mem_hotplug_done();
+#ifdef CONFIG_HTMM /* online_pages() */
+	mem_cgroup_per_node_htmm_init();
+#endif
+
 	return 0;
 
 failed_addition:
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index 818753635e42..a825c45e5a46 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -635,6 +635,7 @@ unsigned long change_prot_numa(struct vm_area_struct *vma,
 			unsigned long addr, unsigned long end)
 {
 	int nr_updated;
+	guard(vmstat_stopwatch)(HINT_FAULT_NS);
 
 	nr_updated = change_protection(vma, addr, end, PAGE_NONE, MM_CP_PROT_NUMA);
 	if (nr_updated)
@@ -1008,6 +1009,29 @@ static long do_get_mempolicy(int *policy, nodemask_t *nmask,
 	return err;
 }
 
+void check_toptier_balanced(void)
+{
+	int nid;
+	int balanced;
+
+	if (!numa_promotion_tiered_enabled)
+		return;
+
+	for_each_node_state(nid, N_MEMORY) {
+		pg_data_t *pgdat = NODE_DATA(nid);
+
+		if (!node_is_toptier(nid))
+			continue;
+
+		balanced = pgdat_toptier_balanced(pgdat, 0, ZONE_MOVABLE);
+		if (!balanced) {
+			pgdat->kswapd_order = 0;
+			pgdat->kswapd_highest_zoneidx = ZONE_NORMAL;
+			wakeup_kswapd(pgdat->node_zones + ZONE_NORMAL, 0, 1, ZONE_NORMAL);
+		}
+	}
+}
+
 #ifdef CONFIG_MIGRATION
 /*
  * page migration, thp tail pages can be passed.
@@ -2096,6 +2120,58 @@ struct page *alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,
 		goto out;
 	}
 
+#ifdef CONFIG_HTMM /* alloc_pages_vma() */
+	if (vma->vm_mm && vma->vm_mm->htmm_enabled) {
+		struct task_struct *p = current;
+		struct mem_cgroup *memcg = mem_cgroup_from_task(p);
+		unsigned long max_nr_pages;
+		int nid = pol->mode == MPOL_PREFERRED ? first_node(pol->nodes) :
+							node;
+		int orig_nid = nid;
+		unsigned int nr_pages = 1U << order;
+		pg_data_t *pgdat = NODE_DATA(nid);
+
+		if (!memcg || !memcg->htmm_enabled)
+			goto use_default_pol;
+
+		max_nr_pages = READ_ONCE(memcg->nodeinfo[nid]->max_nr_base_pages);
+		if (max_nr_pages == ULONG_MAX)
+			goto use_default_pol;
+
+		while (max_nr_pages <=
+		       (get_nr_lru_pages_node(memcg, pgdat) + nr_pages)) {
+			if (htmm_cxl_mode) {
+				nid = 1;
+				break;
+			}
+			if ((nid = next_demotion_node(nid)) == NUMA_NO_NODE) {
+				nid = first_memory_node;
+				break;
+			}
+			max_nr_pages = READ_ONCE(
+				memcg->nodeinfo[nid]->max_nr_base_pages);
+			pgdat = NODE_DATA(nid);
+		}
+
+		//nid = orig_nid;
+
+		if (orig_nid != nid) {
+			WRITE_ONCE(memcg->nodeinfo[orig_nid]->need_demotion,
+				   true);
+			kmigraterd_wakeup(orig_nid);
+		} else if (max_nr_pages <=
+			   (get_nr_lru_pages_node(memcg, pgdat) +
+			    get_memcg_demotion_watermark(max_nr_pages))) {
+			WRITE_ONCE(memcg->nodeinfo[nid]->need_demotion, true);
+			kmigraterd_wakeup(nid);
+		}
+
+		mpol_cond_put(pol);
+		page = __alloc_pages_node(nid, gfp | __GFP_THISNODE, order);
+		goto out;
+	}
+use_default_pol:
+#endif
 	if (pol->mode == MPOL_PREFERRED_MANY) {
 		page = alloc_pages_preferred_many(gfp, order, node, pol);
 		mpol_cond_put(pol);
@@ -2370,7 +2446,7 @@ static void sp_free(struct sp_node *n)
  * Return: NUMA_NO_NODE if the page is in a node that is valid for this
  * policy, or a suitable node ID to allocate a replacement page from.
  */
-int mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long addr)
+int mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long addr, int flags)
 {
 	struct mempolicy *pol;
 	struct zoneref *z;
@@ -2381,6 +2457,9 @@ int mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long
 	int polnid = NUMA_NO_NODE;
 	int ret = NUMA_NO_NODE;
 
+	if (test_and_clear_page_demoted(page))
+		flags |= TNF_DEMOTED;
+
 	pol = get_vma_policy(vma, addr);
 	if (!(pol->flags & MPOL_F_MOF))
 		goto out;
@@ -2434,7 +2513,7 @@ int mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long
 	if (pol->flags & MPOL_F_MORON) {
 		polnid = thisnid;
 
-		if (!should_numa_migrate_memory(current, page, curnid, thiscpu))
+		if (!should_numa_migrate_memory(current, page, curnid, thiscpu, flags))
 			goto out;
 	}
 
diff --git a/mm/memtis/Kconfig b/mm/memtis/Kconfig
new file mode 100644
index 000000000000..fd4484bd4d74
--- /dev/null
+++ b/mm/memtis/Kconfig
@@ -0,0 +1,11 @@
+config HTMM
+	bool "Enable hugepage-aware tiered memory management"
+	depends on MIGRATION && TRANSPARENT_HUGEPAGE && MEMCG
+	help
+	  Enable memory access sampling and dynamic placement for
+	  tiered memory systems (DRAM + NVM). This optimizes the system
+	  performance in tiered memory systems by coordinating transparent
+	  hugepage support with tiered memory management.
+
+	  If in doubt, say N.
+
diff --git a/mm/memtis/Makefile b/mm/memtis/Makefile
new file mode 100644
index 000000000000..fc1dc38404da
--- /dev/null
+++ b/mm/memtis/Makefile
@@ -0,0 +1,2 @@
+obj-$(CONFIG_HTMM) += memcontrol.o migrate.o rmap.o sysfs.o
+obj-$(CONFIG_HTMM) += sampler.o core.o migrater.o 
diff --git a/mm/memtis/core.c b/mm/memtis/core.c
new file mode 100644
index 000000000000..b0cc890ae120
--- /dev/null
+++ b/mm/memtis/core.c
@@ -0,0 +1,1417 @@
+/* memtis core functions
+ * author: Taehyung Lee (Sungkyunkwan Univ.)
+ * mail: taehyunggg@skku.edu
+ */
+#include <linux/mm.h>
+#include <linux/kernel.h>
+#include <linux/huge_mm.h>
+#include <linux/mm_inline.h>
+#include <linux/pid.h>
+#include <linux/htmm.h>
+#include <linux/mempolicy.h>
+#include <linux/migrate.h>
+#include <linux/swap.h>
+#include <linux/sched/task.h>
+#include <linux/xarray.h>
+#include <linux/math.h>
+#include <linux/random.h>
+#include <trace/events/htmm.h>
+
+#include "../internal.h"
+#include <asm/pgtable.h>
+
+void htmm_mm_init(struct mm_struct *mm)
+{
+	struct mem_cgroup *memcg = get_mem_cgroup_from_mm(mm);
+
+	if (!memcg || !memcg->htmm_enabled) {
+		mm->htmm_enabled = false;
+		return;
+	}
+	mm->htmm_enabled = true;
+}
+
+void htmm_mm_exit(struct mm_struct *mm)
+{
+	struct mem_cgroup *memcg = get_mem_cgroup_from_mm(mm);
+	if (!memcg)
+		return;
+	/* do nothing */
+}
+
+/* Hugepage uses tail pages to store access information.
+ * See struct page declaration in linux/mm_types.h */
+void __prep_transhuge_page_for_htmm(struct mm_struct *mm, struct page *page)
+{
+	int i, idx, offset;
+	struct mem_cgroup *memcg = mm ? get_mem_cgroup_from_mm(mm) : NULL;
+	pginfo_t pginfo = {
+		0,
+		0,
+		0,
+		false,
+	};
+	int hotness_factor = memcg ? get_accesses_from_idx(memcg->active_threshold + 1) : 0;
+	/* third tail page */
+	page[3].hot_utils = 0;
+	page[3].total_accesses = hotness_factor;
+	page[3].skewness_idx = 0;
+	page[3].idx = 0;
+	SetPageHtmm(&page[3]);
+
+	if (hotness_factor < 0)
+		hotness_factor = 0;
+	pginfo.total_accesses = hotness_factor;
+	pginfo.nr_accesses = hotness_factor;
+	/* fourth~ tail pages */
+	for (i = 0; i < HPAGE_PMD_NR; i++) {
+		idx = 4 + i / 4;
+		offset = i % 4;
+
+		page[idx].compound_pginfo[offset] = pginfo;
+		SetPageHtmm(&page[idx]);
+	}
+
+	if (!memcg)
+		return;
+
+	if (htmm_skip_cooling)
+		page[3].cooling_clock = memcg->cooling_clock + 1;
+	else
+		page[3].cooling_clock = memcg->cooling_clock;
+
+	ClearPageActive(page);
+}
+
+void prep_transhuge_page_for_htmm(struct vm_area_struct *vma, struct page *page)
+{
+	prep_transhuge_page(page);
+
+	if (vma->vm_mm->htmm_enabled)
+		__prep_transhuge_page_for_htmm(vma->vm_mm, page);
+	else
+		return;
+}
+
+void clear_transhuge_pginfo(struct page *page)
+{
+	INIT_LIST_HEAD(&page->lru);
+	set_page_private(page, 0);
+}
+
+void copy_transhuge_pginfo(struct page *page, struct page *newpage)
+{
+	int i, idx, offset;
+	pginfo_t zero_pginfo = { 0 };
+
+	VM_BUG_ON_PAGE(!PageCompound(page), page);
+	VM_BUG_ON_PAGE(!PageCompound(newpage), newpage);
+
+	page = compound_head(page);
+	newpage = compound_head(newpage);
+
+	if (!PageHtmm(&page[3]))
+		return;
+
+	newpage[3].hot_utils = page[3].hot_utils;
+	newpage[3].total_accesses = page[3].total_accesses;
+	newpage[3].skewness_idx = page[3].skewness_idx;
+	newpage[3].cooling_clock = page[3].cooling_clock;
+	newpage[3].idx = page[3].idx;
+
+	SetPageHtmm(&newpage[3]);
+
+	for (i = 0; i < HPAGE_PMD_NR; i++) {
+		idx = 4 + i / 4;
+		offset = i % 4;
+
+		newpage[idx].compound_pginfo[offset].nr_accesses = page[idx].compound_pginfo[offset].nr_accesses;
+		newpage[idx].compound_pginfo[offset].total_accesses = page[idx].compound_pginfo[offset].total_accesses;
+
+		page[idx].compound_pginfo[offset] = zero_pginfo;
+		page[idx].mapping = TAIL_MAPPING;
+		SetPageHtmm(&newpage[idx]);
+	}
+}
+
+pginfo_t *get_compound_pginfo(struct page *page, unsigned long address)
+{
+	int idx, offset;
+	VM_BUG_ON_PAGE(!PageCompound(page), page);
+
+	idx = 4 + ((address & ~HPAGE_PMD_MASK) >> PAGE_SHIFT) / 4;
+	offset = ((address & ~HPAGE_PMD_MASK) >> PAGE_SHIFT) % 4;
+
+	return &(page[idx].compound_pginfo[offset]);
+}
+
+void check_transhuge_cooling(void *arg, struct page *page, bool locked)
+{
+	struct mem_cgroup *memcg = arg ? (struct mem_cgroup *)arg : page_memcg(page);
+	struct page *meta_page;
+	pginfo_t *pginfo;
+	int i, idx, offset;
+	unsigned int memcg_cclock;
+
+	if (!memcg || !memcg->htmm_enabled)
+		return;
+
+	meta_page = get_meta_page(page);
+
+	spin_lock(&memcg->access_lock);
+	/* check cooling */
+	memcg_cclock = READ_ONCE(memcg->cooling_clock);
+	if (memcg_cclock > meta_page->cooling_clock) {
+		unsigned int diff = memcg_cclock - meta_page->cooling_clock;
+		unsigned long prev_idx, cur_idx, skewness = 0;
+		unsigned int refs = 0;
+		unsigned int bp_hot_thres = min(memcg->active_threshold, memcg->bp_active_threshold);
+
+		/* perform cooling */
+		meta_page->hot_utils = 0;
+		for (i = 0; i < HPAGE_PMD_NR; i++) { // subpages
+			int j;
+
+			idx = 4 + i / 4;
+			offset = i % 4;
+			pginfo = &(page[idx].compound_pginfo[offset]);
+			prev_idx = get_idx(pginfo->total_accesses);
+			if (prev_idx >= bp_hot_thres) {
+				meta_page->hot_utils++;
+				refs += pginfo->total_accesses;
+			}
+
+			/* get the sum of the square of H_ij*/
+			skewness += (pginfo->total_accesses * pginfo->total_accesses);
+			if (prev_idx >= (memcg->bp_active_threshold))
+				pginfo->may_hot = true;
+			else
+				pginfo->may_hot = false;
+
+			/* halves access counts of subpages */
+			for (j = 0; j < diff; j++)
+				pginfo->total_accesses >>= 1;
+
+			/* updates estimated base page histogram */
+			cur_idx = get_idx(pginfo->total_accesses);
+			memcg->ebp_hotness_hg[cur_idx]++;
+		}
+
+		/* halves access count for a huge page */
+		for (i = 0; i < diff; i++)
+			meta_page->total_accesses >>= 1;
+
+		cur_idx = meta_page->total_accesses;
+		cur_idx = get_idx(cur_idx);
+		memcg->hotness_hg[cur_idx] += HPAGE_PMD_NR;
+		meta_page->idx = cur_idx;
+
+		/* updates skewness */
+		if (meta_page->hot_utils == 0)
+			skewness = 0;
+		else if (meta_page->idx >= 13) // very hot pages
+			skewness = 0;
+		else {
+			skewness /= 11; /* scale down */
+			skewness = skewness / (meta_page->hot_utils);
+			skewness = skewness / (meta_page->hot_utils);
+			skewness = get_skew_idx(skewness);
+		}
+		meta_page->skewness_idx = skewness;
+		memcg->access_map[skewness] += 1;
+
+		if (meta_page->hot_utils) {
+			refs /= HPAGE_PMD_NR; /* actual access counts */
+			memcg->sum_util += refs; /* total accesses to huge pages */
+			memcg->num_util += 1; /* the number of huge pages */
+		}
+
+		meta_page->cooling_clock = memcg_cclock;
+	} else
+		meta_page->cooling_clock = memcg_cclock;
+
+	spin_unlock(&memcg->access_lock);
+}
+
+void check_base_cooling(pginfo_t *pginfo, struct page *page, bool locked)
+{
+	struct mem_cgroup *memcg = page_memcg(page);
+	unsigned long prev_accessed, cur_idx;
+	unsigned int memcg_cclock;
+
+	if (!memcg || !memcg->htmm_enabled)
+		return;
+
+	spin_lock(&memcg->access_lock);
+	memcg_cclock = READ_ONCE(memcg->cooling_clock);
+	if (memcg_cclock > pginfo->cooling_clock) {
+		unsigned int diff = memcg_cclock - pginfo->cooling_clock;
+		int j;
+
+		prev_accessed = pginfo->total_accesses;
+		cur_idx = get_idx(prev_accessed);
+		if (cur_idx >= (memcg->bp_active_threshold))
+			pginfo->may_hot = true;
+		else
+			pginfo->may_hot = false;
+
+		/* halves access count */
+		for (j = 0; j < diff; j++)
+			pginfo->total_accesses >>= 1;
+		//if (pginfo->total_accesses == 0)
+		//  pginfo->total_accesses = 1;
+
+		cur_idx = get_idx(pginfo->total_accesses);
+		memcg->hotness_hg[cur_idx]++;
+		memcg->ebp_hotness_hg[cur_idx]++;
+
+		pginfo->cooling_clock = memcg_cclock;
+	} else
+		pginfo->cooling_clock = memcg_cclock;
+	spin_unlock(&memcg->access_lock);
+}
+
+int set_page_coolstatus(struct page *page, pte_t *pte, struct mm_struct *mm)
+{
+	struct mem_cgroup *memcg = get_mem_cgroup_from_mm(mm);
+	struct page *pte_page;
+	pginfo_t *pginfo;
+	int hotness_factor;
+
+	if (!memcg || !memcg->htmm_enabled)
+		return 0;
+
+	pte_page = virt_to_page((unsigned long)pte);
+	if (!PageHtmm(pte_page))
+		return 0;
+
+	pginfo = get_pginfo_from_pte(pte);
+	if (!pginfo)
+		return 0;
+
+	hotness_factor = get_accesses_from_idx(memcg->active_threshold + 1);
+
+	pginfo->total_accesses = hotness_factor;
+	pginfo->nr_accesses = hotness_factor;
+	if (htmm_skip_cooling)
+		pginfo->cooling_clock = READ_ONCE(memcg->cooling_clock) + 1;
+	else
+		pginfo->cooling_clock = READ_ONCE(memcg->cooling_clock);
+	pginfo->may_hot = false;
+
+	return 0;
+}
+
+struct deferred_split *get_deferred_split_queue_for_htmm(struct page *page)
+{
+	struct mem_cgroup *memcg = page_memcg(compound_head(page));
+	struct mem_cgroup_per_node *pn = memcg->nodeinfo[page_to_nid(page)];
+
+	if (!memcg || !memcg->htmm_enabled)
+		return NULL;
+	else
+		return &pn->deferred_split_queue;
+}
+
+struct list_head *get_deferred_list(struct page *page)
+{
+	struct mem_cgroup *memcg = page_memcg(compound_head(page));
+	struct mem_cgroup_per_node *pn = memcg->nodeinfo[page_to_nid(page)];
+
+	if (!memcg || !memcg->htmm_enabled)
+		return NULL;
+	else
+		return &pn->deferred_list;
+}
+
+bool deferred_split_huge_page_for_htmm(struct page *page)
+{
+	struct deferred_split *ds_queue = get_deferred_split_queue(page);
+	unsigned long flags;
+
+	VM_BUG_ON_PAGE(!PageTransHuge(page), page);
+
+	if (PageSwapCache(page))
+		return false;
+
+	if (!ds_queue)
+		return false;
+
+	spin_lock_irqsave(&ds_queue->split_queue_lock, flags);
+	if (list_empty(page_deferred_list(page))) {
+		count_vm_event(THP_DEFERRED_SPLIT_PAGE);
+		list_add_tail(page_deferred_list(page), &ds_queue->split_queue);
+		ds_queue->split_queue_len++;
+
+		if (node_is_toptier(page_to_nid(page)))
+			count_vm_event(HTMM_MISSED_DRAMREAD);
+		else
+			count_vm_event(HTMM_MISSED_NVMREAD);
+	}
+	spin_unlock_irqrestore(&ds_queue->split_queue_lock, flags);
+	return true;
+}
+
+void check_failed_list(struct mem_cgroup_per_node *pn, struct list_head *tmp, struct list_head *failed_list)
+{
+	struct mem_cgroup *memcg = pn->memcg;
+
+	while (!list_empty(tmp)) {
+		struct page *page = lru_to_page(tmp);
+		struct page *meta;
+		unsigned int idx;
+
+		list_move(&page->lru, failed_list);
+
+		if (!PageTransHuge(page))
+			VM_WARN_ON(1);
+
+		if (PageLRU(page)) {
+			if (!TestClearPageLRU(page)) {
+				VM_WARN_ON(1);
+			}
+		}
+
+		meta = get_meta_page(page);
+		idx = meta->idx;
+
+		spin_lock(&memcg->access_lock);
+		memcg->hotness_hg[idx] += HPAGE_PMD_NR;
+		spin_unlock(&memcg->access_lock);
+	}
+}
+
+unsigned long deferred_split_scan_for_htmm(struct mem_cgroup_per_node *pn, struct list_head *split_list)
+{
+	struct deferred_split *ds_queue = &pn->deferred_split_queue;
+	//struct list_head *deferred_list = &pn->deferred_list;
+	unsigned long flags;
+	LIST_HEAD(list), *pos, *next;
+	LIST_HEAD(failed_list);
+	struct page *page;
+	unsigned int nr_max = 50; // max: 100MB
+	int split = 0;
+
+	spin_lock_irqsave(&ds_queue->split_queue_lock, flags);
+	list_for_each_safe (pos, next, &ds_queue->split_queue) {
+		page = list_entry((void *)pos, struct page, deferred_list);
+		page = compound_head(page);
+
+		if (page_count(page) < 1) {
+			list_del_init(page_deferred_list(page));
+			ds_queue->split_queue_len--;
+		} else {
+			list_move(page_deferred_list(page), &list);
+		}
+	}
+	spin_unlock_irqrestore(&ds_queue->split_queue_lock, flags);
+
+	list_for_each_safe (pos, next, &list) {
+		LIST_HEAD(tmp);
+		struct lruvec *lruvec = mem_cgroup_page_lruvec(page);
+		bool skip_iso = false;
+
+		if (split >= nr_max)
+			break;
+
+		page = list_entry((void *)pos, struct page, deferred_list);
+		page = compound_head(page);
+
+		if (!PageLRU(page)) {
+			skip_iso = true;
+			goto skip_isolation;
+		}
+
+		if (lruvec != &pn->lruvec) {
+			continue;
+		}
+
+		spin_lock_irq(&lruvec->lru_lock);
+		if (!__isolate_lru_page_prepare(page, 0)) {
+			spin_unlock_irq(&lruvec->lru_lock);
+			continue;
+		}
+
+		if (unlikely(!get_page_unless_zero(page))) {
+			spin_unlock_irq(&lruvec->lru_lock);
+			continue;
+		}
+
+		if (!TestClearPageLRU(page)) {
+			put_page(page);
+			spin_unlock_irq(&lruvec->lru_lock);
+			continue;
+		}
+
+		list_move(&page->lru, &tmp);
+		update_lru_size(lruvec, page_lru(page), page_zonenum(page), -thp_nr_pages(page));
+		spin_unlock_irq(&lruvec->lru_lock);
+	skip_isolation:
+		if (skip_iso) {
+			if (page->lru.next != LIST_POISON1 || page->lru.prev != LIST_POISON2)
+				continue;
+			list_add(&page->lru, &tmp);
+		}
+
+		if (!trylock_page(page)) {
+			list_splice_tail(&tmp, split_list);
+			continue;
+		}
+
+		if (!split_huge_page_to_list(page, &tmp)) {
+			split++;
+			list_splice(&tmp, split_list);
+		} else {
+			check_failed_list(pn, &tmp, &failed_list);
+		}
+
+		unlock_page(page);
+	}
+	putback_movable_pages(&failed_list);
+
+	/* handle list and failed_list */
+	spin_lock_irqsave(&ds_queue->split_queue_lock, flags);
+	list_splice_tail(&list, &ds_queue->split_queue);
+	spin_unlock_irqrestore(&ds_queue->split_queue_lock, flags);
+
+	putback_movable_pages(&failed_list);
+	if (split)
+		pn->memcg->split_happen = true;
+	return split;
+}
+
+void putback_split_pages(struct list_head *split_list, struct lruvec *lruvec)
+{
+	LIST_HEAD(l_active);
+	LIST_HEAD(l_inactive);
+
+	while (!list_empty(split_list)) {
+		struct page *page;
+
+		page = lru_to_page(split_list);
+		list_del(&page->lru);
+
+		if (unlikely(!page_evictable(page))) {
+			putback_lru_page(page);
+			continue;
+		}
+
+		VM_WARN_ON(PageLRU(page));
+
+		if (PageActive(page))
+			list_add(&page->lru, &l_active);
+		else
+			list_add(&page->lru, &l_inactive);
+	}
+
+	spin_lock_irq(&lruvec->lru_lock);
+	move_pages_to_lru(lruvec, &l_active);
+	move_pages_to_lru(lruvec, &l_inactive);
+	list_splice(&l_inactive, &l_active);
+	spin_unlock_irq(&lruvec->lru_lock);
+
+	mem_cgroup_uncharge_list(&l_active);
+	free_unref_page_list(&l_active);
+}
+
+struct page *get_meta_page(struct page *page)
+{
+	page = compound_head(page);
+	return &page[3];
+}
+
+unsigned int get_accesses_from_idx(unsigned int idx)
+{
+	unsigned int accesses = 1;
+
+	if (idx == 0)
+		return 0;
+
+	while (idx--) {
+		accesses <<= 1;
+	}
+
+	return accesses;
+}
+
+unsigned int get_idx(unsigned long num)
+{
+	unsigned int cnt = 0;
+
+	num++;
+	while (1) {
+		num = num >> 1;
+		if (num)
+			cnt++;
+		else
+			return cnt;
+
+		if (cnt == 15)
+			break;
+	}
+
+	return cnt;
+}
+
+int get_skew_idx(unsigned long num)
+{
+	int cnt = 0;
+	unsigned long tmp;
+
+	/* 0, 1-3, 4-15, 16-63, 64-255, 256-1023, 1024-2047, 2048-3071, ... */
+	tmp = num;
+	if (tmp >= 1024) {
+		while (tmp > 1024 && cnt < 9) { // <16
+			tmp -= 1024;
+			cnt++;
+		}
+		cnt += 11;
+	} else {
+		while (tmp) {
+			tmp >>= 1; // >>2
+			cnt++;
+		}
+	}
+
+	return cnt;
+}
+
+/* linux/mm.h */
+void free_pginfo_pte(struct page *pte)
+{
+	if (!PageHtmm(pte))
+		return;
+
+	BUG_ON(pte->pginfo == NULL);
+	kmem_cache_free(pginfo_cache, pte->pginfo);
+	pte->pginfo = NULL;
+	ClearPageHtmm(pte);
+}
+
+void uncharge_htmm_pte(pte_t *pte, struct mem_cgroup *memcg)
+{
+	struct page *pte_page;
+	unsigned int idx;
+	pginfo_t *pginfo;
+
+	if (!memcg || !memcg->htmm_enabled)
+		return;
+
+	pte_page = virt_to_page((unsigned long)pte);
+	if (!PageHtmm(pte_page))
+		return;
+
+	pginfo = get_pginfo_from_pte(pte);
+	if (!pginfo)
+		return;
+
+	idx = get_idx(pginfo->total_accesses);
+	spin_lock(&memcg->access_lock);
+	if (memcg->hotness_hg[idx] > 0)
+		memcg->hotness_hg[idx]--;
+	if (memcg->ebp_hotness_hg[idx] > 0)
+		memcg->ebp_hotness_hg[idx]--;
+	spin_unlock(&memcg->access_lock);
+}
+
+void uncharge_htmm_page(struct page *page, struct mem_cgroup *memcg)
+{
+	unsigned int nr_pages = thp_nr_pages(page);
+	unsigned int idx;
+	int i;
+
+	if (!memcg || !memcg->htmm_enabled)
+		return;
+
+	page = compound_head(page);
+	if (nr_pages != 1) { // hugepage
+		struct page *meta = get_meta_page(page);
+
+		idx = meta->idx;
+
+		spin_lock(&memcg->access_lock);
+		if (memcg->hotness_hg[idx] >= nr_pages)
+			memcg->hotness_hg[idx] -= nr_pages;
+		else
+			memcg->hotness_hg[idx] = 0;
+
+		for (i = 0; i < HPAGE_PMD_NR; i++) {
+			int base_idx = 4 + i / 4;
+			int offset = i % 4;
+			pginfo_t *pginfo;
+
+			pginfo = &(page[base_idx].compound_pginfo[offset]);
+			idx = get_idx(pginfo->total_accesses);
+			if (memcg->ebp_hotness_hg[idx] > 0)
+				memcg->ebp_hotness_hg[idx]--;
+		}
+		spin_unlock(&memcg->access_lock);
+	}
+}
+
+static bool need_cooling(struct mem_cgroup *memcg)
+{
+	struct mem_cgroup_per_node *pn;
+	int nid;
+
+	for_each_node_state (nid, N_MEMORY) {
+		pn = memcg->nodeinfo[nid];
+		if (!pn)
+			continue;
+
+		if (READ_ONCE(pn->need_cooling))
+			return true;
+	}
+	return false;
+}
+
+static void set_lru_cooling(struct mm_struct *mm)
+{
+	struct mem_cgroup *memcg = get_mem_cgroup_from_mm(mm);
+	struct mem_cgroup_per_node *pn;
+	int nid;
+
+	if (!memcg || !memcg->htmm_enabled)
+		return;
+
+	for_each_node_state (nid, N_MEMORY) {
+		pn = memcg->nodeinfo[nid];
+		if (!pn)
+			continue;
+
+		WRITE_ONCE(pn->need_cooling, true);
+	}
+}
+
+void set_lru_adjusting(struct mem_cgroup *memcg, bool inc_thres)
+{
+	struct mem_cgroup_per_node *pn;
+	int nid;
+
+	for_each_node_state (nid, N_MEMORY) {
+		pn = memcg->nodeinfo[nid];
+		if (!pn)
+			continue;
+
+		WRITE_ONCE(pn->need_adjusting, true);
+		if (inc_thres)
+			WRITE_ONCE(pn->need_adjusting_all, true);
+	}
+}
+
+bool check_split_huge_page(struct mem_cgroup *memcg, struct page *meta, bool hot)
+{
+	unsigned long split_thres = memcg->split_threshold;
+	unsigned long split_thres_tail = split_thres - 1;
+	bool tail_idx = false;
+
+	/* check split enable/disable status */
+	if (htmm_thres_split == 0)
+		return false;
+
+	/* no need to split */
+	if (split_thres == 0)
+		return false;
+
+	/* already in the split queue */
+	if (!list_empty(page_deferred_list(compound_head(meta)))) {
+		return false;
+	}
+
+	/* check split thres */
+	if (meta->skewness_idx < split_thres_tail)
+		return false;
+	else if (meta->skewness_idx == split_thres_tail)
+		tail_idx = true;
+	if (memcg->nr_split == 0)
+		tail_idx = true;
+
+	if (tail_idx && memcg->nr_split_tail_idx == 0)
+		return false;
+
+	spin_lock(&memcg->access_lock);
+	if (tail_idx) {
+		if (memcg->nr_split_tail_idx >= HPAGE_PMD_NR)
+			memcg->nr_split_tail_idx -= HPAGE_PMD_NR;
+		else
+			memcg->nr_split_tail_idx = 0;
+	} else {
+		if (memcg->nr_split >= HPAGE_PMD_NR)
+			memcg->nr_split -= HPAGE_PMD_NR;
+		else
+			memcg->nr_split = 0;
+	}
+	if (memcg->access_map[meta->skewness_idx] != 0)
+		memcg->access_map[meta->skewness_idx]--;
+	spin_unlock(&memcg->access_lock);
+	return true;
+}
+
+bool move_page_to_deferred_split_queue(struct mem_cgroup *memcg, struct page *page)
+{
+	struct lruvec *lruvec;
+	bool ret = false;
+
+	page = compound_head(page);
+
+	lruvec = mem_cgroup_page_lruvec(page);
+	spin_lock_irq(&lruvec->lru_lock);
+
+	if (!PageLRU(page))
+		goto lru_unlock;
+
+	if (deferred_split_huge_page_for_htmm(compound_head(page))) {
+		ret = true;
+		goto lru_unlock;
+	}
+
+lru_unlock:
+	spin_unlock_irq(&lruvec->lru_lock);
+
+	return ret;
+}
+
+void move_page_to_active_lru(struct page *page)
+{
+	struct lruvec *lruvec;
+	LIST_HEAD(l_active);
+
+	lruvec = mem_cgroup_page_lruvec(page);
+
+	spin_lock_irq(&lruvec->lru_lock);
+	if (PageActive(page))
+		goto lru_unlock;
+
+	if (!__isolate_lru_page_prepare(page, 0))
+		goto lru_unlock;
+
+	if (unlikely(!get_page_unless_zero(page)))
+		goto lru_unlock;
+
+	if (!TestClearPageLRU(page)) {
+		put_page(page);
+		goto lru_unlock;
+	}
+
+	list_move(&page->lru, &l_active);
+	update_lru_size(lruvec, page_lru(page), page_zonenum(page), -thp_nr_pages(page));
+	SetPageActive(page);
+
+	if (!list_empty(&l_active))
+		move_pages_to_lru(lruvec, &l_active);
+lru_unlock:
+	spin_unlock_irq(&lruvec->lru_lock);
+
+	if (!list_empty(&l_active))
+		BUG();
+}
+
+void move_page_to_inactive_lru(struct page *page)
+{
+	struct lruvec *lruvec;
+	LIST_HEAD(l_inactive);
+
+	lruvec = mem_cgroup_page_lruvec(page);
+
+	spin_lock_irq(&lruvec->lru_lock);
+	if (!PageActive(page))
+		goto lru_unlock;
+
+	if (!__isolate_lru_page_prepare(page, 0))
+		goto lru_unlock;
+
+	if (unlikely(!get_page_unless_zero(page)))
+		goto lru_unlock;
+
+	if (!TestClearPageLRU(page)) {
+		put_page(page);
+		goto lru_unlock;
+	}
+
+	list_move(&page->lru, &l_inactive);
+	update_lru_size(lruvec, page_lru(page), page_zonenum(page), -thp_nr_pages(page));
+	ClearPageActive(page);
+
+	if (!list_empty(&l_inactive))
+		move_pages_to_lru(lruvec, &l_inactive);
+lru_unlock:
+	spin_unlock_irq(&lruvec->lru_lock);
+
+	if (!list_empty(&l_inactive))
+		BUG();
+}
+
+static void update_base_page(struct vm_area_struct *vma, struct page *page, pginfo_t *pginfo)
+{
+	struct mem_cgroup *memcg = get_mem_cgroup_from_mm(vma->vm_mm);
+	unsigned long prev_accessed, prev_idx, cur_idx;
+	bool hot;
+
+	/* check cooling status and perform cooling if the page needs to be cooled */
+	check_base_cooling(pginfo, page, false);
+
+	prev_accessed = pginfo->total_accesses;
+	pginfo->nr_accesses++;
+	pginfo->total_accesses += HPAGE_PMD_NR;
+
+	prev_idx = get_idx(prev_accessed);
+	cur_idx = get_idx(pginfo->total_accesses);
+
+	spin_lock(&memcg->access_lock);
+
+	if (prev_idx != cur_idx) {
+		if (memcg->hotness_hg[prev_idx] > 0)
+			memcg->hotness_hg[prev_idx]--;
+		memcg->hotness_hg[cur_idx]++;
+
+		if (memcg->ebp_hotness_hg[prev_idx] > 0)
+			memcg->ebp_hotness_hg[prev_idx]--;
+		memcg->ebp_hotness_hg[cur_idx]++;
+	}
+
+	if (pginfo->may_hot == true)
+		memcg->max_dram_sampled++;
+	if (cur_idx >= (memcg->bp_active_threshold))
+		pginfo->may_hot = true;
+	else
+		pginfo->may_hot = false;
+
+	spin_unlock(&memcg->access_lock);
+
+	hot = cur_idx >= memcg->active_threshold;
+
+	if (PageActive(page) && !hot)
+		move_page_to_inactive_lru(page);
+	else if (!PageActive(page) && hot)
+		move_page_to_active_lru(page);
+
+	if (hot)
+		move_page_to_active_lru(page);
+	else if (PageActive(page))
+		move_page_to_inactive_lru(page);
+}
+
+static void update_huge_page(struct vm_area_struct *vma, pmd_t *pmd, struct page *page, unsigned long address)
+{
+	struct mem_cgroup *memcg = get_mem_cgroup_from_mm(vma->vm_mm);
+	struct page *meta_page;
+	pginfo_t *pginfo;
+	unsigned long prev_idx, cur_idx;
+	bool hot, pg_split = false;
+	unsigned long pginfo_prev;
+
+	meta_page = get_meta_page(page);
+	pginfo = get_compound_pginfo(page, address);
+
+	/* check cooling status */
+	check_transhuge_cooling((void *)memcg, page, false);
+
+	pginfo_prev = pginfo->total_accesses;
+	pginfo->nr_accesses++;
+	pginfo->total_accesses += HPAGE_PMD_NR;
+
+	meta_page->total_accesses++;
+
+#ifndef DEFERRED_SPLIT_ISOLATED
+	if (check_split_huge_page(memcg, meta_page, false)) {
+		pg_split = move_page_to_deferred_split_queue(memcg, page);
+	}
+#endif
+
+	/*subpage */
+	prev_idx = get_idx(pginfo_prev);
+	cur_idx = get_idx(pginfo->total_accesses);
+	spin_lock(&memcg->access_lock);
+	if (prev_idx != cur_idx) {
+		if (memcg->ebp_hotness_hg[prev_idx] > 0)
+			memcg->ebp_hotness_hg[prev_idx]--;
+		memcg->ebp_hotness_hg[cur_idx]++;
+	}
+	if (pginfo->may_hot == true)
+		memcg->max_dram_sampled++;
+	if (cur_idx >= (memcg->bp_active_threshold))
+		pginfo->may_hot = true;
+	else
+		pginfo->may_hot = false;
+	spin_unlock(&memcg->access_lock);
+
+	/* hugepage */
+	prev_idx = meta_page->idx;
+	cur_idx = meta_page->total_accesses;
+	cur_idx = get_idx(cur_idx);
+	if (prev_idx != cur_idx) {
+		spin_lock(&memcg->access_lock);
+		if (memcg->hotness_hg[prev_idx] >= HPAGE_PMD_NR)
+			memcg->hotness_hg[prev_idx] -= HPAGE_PMD_NR;
+		else
+			memcg->hotness_hg[prev_idx] = 0;
+
+		memcg->hotness_hg[cur_idx] += HPAGE_PMD_NR;
+		spin_unlock(&memcg->access_lock);
+	}
+	meta_page->idx = cur_idx;
+
+	if (pg_split)
+		return;
+
+	hot = cur_idx >= memcg->active_threshold;
+	if (PageActive(page) && !hot) {
+		move_page_to_inactive_lru(page);
+	} else if (!PageActive(page) && hot) {
+		move_page_to_active_lru(page);
+	}
+
+	if (hot)
+		move_page_to_active_lru(page);
+	else if (PageActive(page))
+		move_page_to_inactive_lru(page);
+}
+
+static int __update_pte_pginfo(struct vm_area_struct *vma, pmd_t *pmd, unsigned long address)
+{
+	pte_t *pte, ptent;
+	spinlock_t *ptl;
+	pginfo_t *pginfo;
+	struct page *page, *pte_page;
+	int ret = 0;
+
+	pte = pte_offset_map_lock(vma->vm_mm, pmd, address, &ptl);
+	ptent = *pte;
+	if (!pte_present(ptent))
+		goto pte_unlock;
+
+	page = vm_normal_page(vma, address, ptent);
+	if (!page || PageKsm(page))
+		goto pte_unlock;
+
+	if (page != compound_head(page))
+		goto pte_unlock;
+
+	pte_page = virt_to_page((unsigned long)pte);
+	if (!PageHtmm(pte_page))
+		goto pte_unlock;
+
+	pginfo = get_pginfo_from_pte(pte);
+	if (!pginfo)
+		goto pte_unlock;
+
+	update_base_page(vma, page, pginfo);
+	pte_unmap_unlock(pte, ptl);
+	if (htmm_cxl_mode) {
+		if (node_is_toptier(page_to_nid(page)))
+			return 1;
+		else
+			return 2;
+	} else {
+		if (node_is_toptier(page_to_nid(page)))
+			return 1;
+		else
+			return 2;
+	}
+
+pte_unlock:
+	pte_unmap_unlock(pte, ptl);
+	return ret;
+}
+
+static int __update_pmd_pginfo(struct vm_area_struct *vma, pud_t *pud, unsigned long address)
+{
+	pmd_t *pmd, pmdval;
+	bool ret = 0;
+
+	pmd = pmd_offset(pud, address);
+	if (!pmd || pmd_none(*pmd))
+		return ret;
+
+	if (is_swap_pmd(*pmd))
+		return ret;
+
+	if (!pmd_trans_huge(*pmd) && !pmd_devmap(*pmd) && unlikely(pmd_bad(*pmd))) {
+		pmd_clear_bad(pmd);
+		return ret;
+	}
+
+	pmdval = *pmd;
+	if (pmd_trans_huge(pmdval) || pmd_devmap(pmdval)) {
+		struct page *page;
+
+		if (is_huge_zero_pmd(pmdval))
+			return ret;
+
+		page = pmd_page(pmdval);
+		if (!page)
+			goto pmd_unlock;
+
+		if (!PageCompound(page)) {
+			goto pmd_unlock;
+		}
+
+		update_huge_page(vma, pmd, page, address);
+		if (htmm_cxl_mode) {
+			if (node_is_toptier(page_to_nid(page)))
+				return 1;
+			else
+				return 2;
+		} else {
+			if (node_is_toptier(page_to_nid(page)))
+				return 1;
+			else
+				return 2;
+		}
+	pmd_unlock:
+		return 0;
+	}
+
+	/* base page */
+	return __update_pte_pginfo(vma, pmd, address);
+}
+
+int __update_pginfo(struct vm_area_struct *vma, unsigned long address)
+{
+	pgd_t *pgd;
+	p4d_t *p4d;
+	pud_t *pud;
+
+	pgd = pgd_offset(vma->vm_mm, address);
+	if (pgd_none_or_clear_bad(pgd))
+		return 0;
+
+	p4d = p4d_offset(pgd, address);
+	if (p4d_none_or_clear_bad(p4d))
+		return 0;
+
+	pud = pud_offset(p4d, address);
+	if (pud_none_or_clear_bad(pud))
+		return 0;
+
+	return __update_pmd_pginfo(vma, pud, address);
+}
+EXPORT_SYMBOL(__update_pginfo);
+
+static void set_memcg_split_thres(struct mem_cgroup *memcg)
+{
+	long nr_split = memcg->nr_split;
+	int i;
+
+	if (memcg->nr_split == 0) { // no split
+		memcg->split_threshold = 21;
+		return;
+	}
+
+	spin_lock(&memcg->access_lock);
+	for (i = 20; i > 0; i--) {
+		long nr_pages = memcg->access_map[i] * HPAGE_PMD_NR;
+
+		if (nr_split < nr_pages) {
+			memcg->nr_split_tail_idx = nr_split;
+			memcg->nr_split -= nr_split;
+			break;
+		} else
+			nr_split -= nr_pages;
+	}
+
+	if (i != 20)
+		memcg->split_threshold = i + 1;
+	spin_unlock(&memcg->access_lock);
+}
+
+static void set_memcg_nr_split(struct mem_cgroup *memcg)
+{
+	unsigned long ehr, rhr;
+	unsigned long captier_lat = htmm_cxl_mode ? CXL_ACCESS_LATENCY : NVM_ACCESS_LATENCY;
+	unsigned long nr_records;
+	unsigned int avg_accesses_hp;
+
+	memcg->nr_split = 0;
+	memcg->nr_split_tail_idx = 0;
+
+	ehr = memcg->prev_max_dram_sampled * 95 / 100;
+	rhr = memcg->prev_dram_sampled;
+	if (ehr <= rhr)
+		return;
+	if (memcg->num_util == 0)
+		return;
+
+	/* cooling halves the access counts so that
+	 * NR_SAMPLE(n) = cooling_period + NR_SAMPLE(n-1) / 2
+	 * --> n = htmm_cooling_period * 2 - (htmm_cooling_period >> cooling_counts)
+	 */
+	avg_accesses_hp = memcg->sum_util / memcg->num_util;
+	if (avg_accesses_hp == 0)
+		return;
+
+	nr_records = (htmm_cooling_period << 1) - (htmm_cooling_period >> (memcg->cooling_clock - 1));
+
+	/* N = (eHR - rHR) * (nr_samples / avg_accesses_hp) * (delta lat / fast lat);
+	 * >> (eHR - rHR) == ('ehr' - 'rhr') / 'nr_records';
+	 * >> 'ehr' - 'rhr' == (eHR - rHR) * nr_records
+	 * To reflect actual accesses to huge pages, calibrates nr_records to
+	 * memcg->sum_util;
+	 */
+	memcg->nr_split = (ehr - rhr) * memcg->sum_util / nr_records;
+	memcg->nr_split /= avg_accesses_hp;
+	/* reflects latency gap */
+	memcg->nr_split *= (captier_lat - DRAM_ACCESS_LATENCY);
+	memcg->nr_split /= DRAM_ACCESS_LATENCY;
+	/* multiply hugepage size (counting granularity) */
+	memcg->nr_split *= HPAGE_PMD_NR;
+	/* scale down */
+	memcg->nr_split *= htmm_gamma;
+	memcg->nr_split /= 10;
+}
+
+/* protected by memcg->access_lock */
+static void reset_memcg_stat(struct mem_cgroup *memcg)
+{
+	int i;
+
+	for (i = 0; i < 16; i++) {
+		memcg->hotness_hg[i] = 0;
+		memcg->ebp_hotness_hg[i] = 0;
+	}
+
+	for (i = 0; i < 21; i++)
+		memcg->access_map[i] = 0;
+
+	memcg->sum_util = 0;
+	memcg->num_util = 0;
+}
+
+static bool __cooling(struct mm_struct *mm, struct mem_cgroup *memcg)
+{
+	int nid;
+
+	/* check whether the previous cooling is done or not. */
+	for_each_node_state (nid, N_MEMORY) {
+		struct mem_cgroup_per_node *pn = memcg->nodeinfo[nid];
+		if (pn && READ_ONCE(pn->need_cooling)) {
+			spin_lock(&memcg->access_lock);
+			memcg->cooling_clock++;
+			spin_unlock(&memcg->access_lock);
+			return false;
+		}
+	}
+
+	spin_lock(&memcg->access_lock);
+
+	reset_memcg_stat(memcg);
+	memcg->cooling_clock++;
+	memcg->bp_active_threshold--;
+	memcg->cooled = true;
+	smp_mb();
+	spin_unlock(&memcg->access_lock);
+	set_lru_cooling(mm);
+	return true;
+}
+
+static void __adjust_active_threshold(struct mm_struct *mm, struct mem_cgroup *memcg)
+{
+	unsigned long nr_active = 0;
+	unsigned long max_nr_pages = memcg->max_nr_dram_pages - get_memcg_promotion_watermark(memcg->max_nr_dram_pages);
+	bool need_warm = false;
+	int idx_hot, idx_bp;
+
+	//if (need_cooling(memcg))
+	//	return;
+
+	spin_lock(&memcg->access_lock);
+
+	for (idx_hot = 15; idx_hot >= 0; idx_hot--) {
+		unsigned long nr_pages = memcg->hotness_hg[idx_hot];
+		if (nr_active + nr_pages > max_nr_pages)
+			break;
+		nr_active += nr_pages;
+	}
+	if (idx_hot != 15)
+		idx_hot++;
+
+	if (nr_active < (max_nr_pages * 75 / 100))
+		need_warm = true;
+
+	/* for the estimated base page histogram */
+	nr_active = 0;
+	for (idx_bp = 15; idx_bp >= 0; idx_bp--) {
+		unsigned long nr_pages = memcg->ebp_hotness_hg[idx_bp];
+		if (nr_active + nr_pages > max_nr_pages)
+			break;
+		nr_active += nr_pages;
+	}
+	if (idx_bp != 15)
+		idx_bp++;
+
+	spin_unlock(&memcg->access_lock);
+
+	// minimum hot threshold
+	if (idx_hot < htmm_thres_hot)
+		idx_hot = htmm_thres_hot;
+	if (idx_bp < htmm_thres_hot)
+		idx_bp = htmm_thres_hot;
+
+	/* some pages may not be reflected in the histogram when cooling happens */
+	if (memcg->cooled) {
+		/* when cooling happens, thres will be current - 1 */
+		if (idx_hot < memcg->active_threshold)
+			if (memcg->active_threshold > 1)
+				memcg->active_threshold--;
+		if (idx_bp < memcg->bp_active_threshold)
+			memcg->bp_active_threshold = idx_bp;
+
+		memcg->cooled = false;
+		set_lru_adjusting(memcg, true);
+
+		if (memcg->need_split) {
+			/* set the target number of pages to be split */
+			set_memcg_nr_split(memcg);
+			/* set the split factor thres */
+			set_memcg_split_thres(memcg);
+			/* reset stat for split */
+			memcg->nr_sampled_for_split = 0;
+			memcg->need_split = false;
+			//trace_printk("memcg->nr_split: %lu, memcg->split_thres: %lu\n", memcg->nr_split, memcg->split_threshold);
+		}
+	} else { /* normal case */
+		if (idx_hot > memcg->active_threshold) {
+			//printk("thres: %d -> %d\n", memcg->active_threshold, idx_hot);
+			memcg->active_threshold = idx_hot;
+			set_lru_adjusting(memcg, true);
+		} else if (memcg->split_happen && htmm_thres_split && idx_hot < memcg->active_threshold) {
+			/* if split happens, histogram may be changed.
+			 * Thus, hot-thres could be decreased */
+			memcg->active_threshold = idx_hot;
+			set_lru_adjusting(memcg, true);
+			//memcg->split_happen = false;
+		}
+		/* estimated base page histogram */
+		memcg->bp_active_threshold = idx_bp;
+	}
+
+	/* set warm threshold */
+	if (!htmm_nowarm) { // warm enabled
+		if (need_warm)
+			memcg->warm_threshold = memcg->active_threshold - 1;
+		else
+			memcg->warm_threshold = memcg->active_threshold;
+	} else { // disable warm
+		memcg->warm_threshold = memcg->active_threshold;
+	}
+}
+
+static bool need_memcg_cooling(struct mem_cgroup *memcg)
+{
+	unsigned long usage = page_counter_read(&memcg->memory);
+	if (memcg->nr_alloc + htmm_thres_cooling_alloc <= usage) {
+		memcg->nr_alloc = usage;
+		return true;
+	}
+	return false;
+}
+
+void update_pginfo(pid_t pid, unsigned long address, enum events e)
+{
+	guard(vmstat_stopwatch)(PTEXT_NS);
+	struct task_struct *p = find_get_task_by_vpid(pid);
+	struct mm_struct *mm = p ? p->mm : NULL;
+	struct vm_area_struct *vma;
+	struct mem_cgroup *memcg;
+	int ret;
+	static unsigned long last_thres_adaptation;
+	last_thres_adaptation = jiffies;
+
+	if (htmm_mode == HTMM_NO_MIG)
+		goto put_task;
+
+	if (!mm) {
+		goto put_task;
+	}
+
+	if (!mmap_read_trylock(mm))
+		goto put_task;
+
+	vma = find_vma(mm, address);
+	if (unlikely(!vma))
+		goto mmap_unlock;
+
+	if (!vma->vm_mm || !vma_migratable(vma) ||
+	    (vma->vm_file && (vma->vm_flags & (VM_READ | VM_WRITE)) == (VM_READ)))
+		goto mmap_unlock;
+
+	memcg = get_mem_cgroup_from_mm(mm);
+	if (!memcg || !memcg->htmm_enabled)
+		goto mmap_unlock;
+
+	/* increase sample counts only for valid records */
+	ret = __update_pginfo(vma, address);
+	if (ret == 1) { /* memory accesses to DRAM */
+		count_vm_event(PEBS_NR_SAMPLED_FMEM);
+		memcg->nr_sampled++;
+		memcg->nr_sampled_for_split++;
+		memcg->nr_dram_sampled++;
+		memcg->nr_max_sampled++;
+	} else if (ret == 2) {
+		count_vm_event(PEBS_NR_SAMPLED_SMEM);
+		memcg->nr_sampled++;
+		memcg->nr_sampled_for_split++;
+		memcg->nr_max_sampled++;
+	} else
+		goto mmap_unlock;
+
+	/* cooling and split decision */
+	if (memcg->nr_sampled % htmm_cooling_period == 0 || need_memcg_cooling(memcg)) {
+		/* cooling -- updates thresholds and sets need_cooling flags */
+		if (__cooling(mm, memcg)) {
+			unsigned long temp_rhr = memcg->prev_dram_sampled;
+			/* updates actual access stat */
+			memcg->prev_dram_sampled >>= 1;
+			memcg->prev_dram_sampled += memcg->nr_dram_sampled;
+			memcg->nr_dram_sampled = 0;
+			/* updates estimated access stat */
+			memcg->prev_max_dram_sampled >>= 1;
+			memcg->prev_max_dram_sampled += memcg->max_dram_sampled;
+			memcg->max_dram_sampled = 0;
+
+			/* split decision period */
+			/* split should be performed after cooling due to skewness factor */
+			if (!memcg->need_split && htmm_thres_split) {
+				unsigned long usage = page_counter_read(&memcg->memory);
+				/* htmm_split_period: 2 by default
+				 * This means that the number of sampled records should
+				 * exceed a quarter of the WSS
+				 */
+				usage >>= htmm_split_period;
+				// the num. of samples must be larger than the fast tier size.
+				usage = max(usage, memcg->max_nr_dram_pages);
+
+				if (memcg->nr_sampled_for_split > usage) {
+					/* if split is already performed in the previous
+					 * and rhr is not improved, stop split huge pages */
+					if (memcg->split_happen) {
+						if (memcg->prev_dram_sampled < (temp_rhr * 103 / 100)) { // 3%
+							htmm_thres_split = 0;
+							goto mmap_unlock;
+						}
+					}
+					memcg->split_happen = false;
+					memcg->need_split = true;
+				} else {
+					/* re-calculate split threshold due to cooling */
+					memcg->nr_split = memcg->nr_split + memcg->nr_split_tail_idx;
+					memcg->nr_split_tail_idx = 0;
+					set_memcg_split_thres(memcg);
+				}
+			}
+			printk("total_accesses: %lu max_dram_hits: %lu cur_hits: %lu \n", memcg->nr_max_sampled,
+			       memcg->prev_max_dram_sampled, memcg->prev_dram_sampled);
+			memcg->nr_max_sampled >>= 1;
+		}
+	}
+	/* threshold adaptation */
+	else if (memcg->nr_sampled % htmm_adaptation_period == 0) {
+		__adjust_active_threshold(mm, memcg);
+	}
+
+mmap_unlock:
+	mmap_read_unlock(mm);
+put_task:
+	if (p)
+		put_task_struct(p);
+}
diff --git a/mm/memtis/memcontrol.c b/mm/memtis/memcontrol.c
new file mode 100644
index 000000000000..98aa83974a6c
--- /dev/null
+++ b/mm/memtis/memcontrol.c
@@ -0,0 +1,242 @@
+#include <linux/memcontrol.h>
+#include <linux/node.h>
+#include <linux/seq_buf.h>
+#include <linux/htmm.h>
+
+#include "../internal.h"
+
+/* memcg interfaces for htmm */
+static int memcg_htmm_show(struct seq_file *m, void *v)
+{
+	struct mem_cgroup *memcg = mem_cgroup_from_css(seq_css(m));
+
+	if (memcg->htmm_enabled)
+		seq_printf(m, "[enabled] disabled\n");
+	else
+		seq_printf(m, "enabled [disabled]\n");
+
+	return 0;
+}
+
+static ssize_t memcg_htmm_write(struct kernfs_open_file *of, char *buf,
+				size_t nbytes, loff_t off)
+{
+	struct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));
+	int nid;
+
+	if (sysfs_streq(buf, "enabled"))
+		memcg->htmm_enabled = true;
+	else if (sysfs_streq(buf, "disabled"))
+		memcg->htmm_enabled = false;
+	else
+		return -EINVAL;
+
+	if (memcg->htmm_enabled) {
+		kmigraterd_init();
+	} else {
+		kmigraterd_stop();
+	}
+	for_each_node_state (nid, N_MEMORY) {
+		struct pglist_data *pgdat = NODE_DATA(nid);
+
+		if (memcg->htmm_enabled) {
+			WRITE_ONCE(pgdat->kswapd_failures, MAX_RECLAIM_RETRIES);
+			add_memcg_to_kmigraterd(memcg, nid);
+		} else {
+			WRITE_ONCE(pgdat->kswapd_failures, 0);
+			del_memcg_from_kmigraterd(memcg, nid);
+		}
+	}
+
+	return nbytes;
+}
+
+static struct cftype memcg_htmm_file[] = {
+	{
+		.name = "htmm_enabled",
+		.flags = CFTYPE_NOT_ON_ROOT,
+		.seq_show = memcg_htmm_show,
+		.write = memcg_htmm_write,
+	},
+	{}, /* terminate */
+};
+
+static int __init mem_cgroup_htmm_init(void)
+{
+	WARN_ON(cgroup_add_dfl_cftypes(&memory_cgrp_subsys, memcg_htmm_file));
+	return 0;
+}
+subsys_initcall(mem_cgroup_htmm_init);
+
+static int memcg_access_map_show(struct seq_file *m, void *v)
+{
+	struct mem_cgroup *memcg = mem_cgroup_from_css(seq_css(m));
+	struct seq_buf s;
+	int i;
+
+	seq_buf_init(&s, kmalloc(PAGE_SIZE, GFP_KERNEL), PAGE_SIZE);
+	if (!s.buffer)
+		return 0;
+	for (i = 20; i > 15; i--) {
+		seq_buf_printf(&s, "skewness_idx_map[%2d]: %10lu\n", i,
+			       memcg->access_map[i]);
+	}
+
+	for (i = 15; i >= 0; i--) {
+		seq_buf_printf(
+			&s,
+			"skewness_idx_map[%2d]: %10lu  hotness_hg[%2d]: %10lu  ebp_hotness_hg[%2d]: %10lu\n",
+			i, memcg->access_map[i], i, memcg->hotness_hg[i], i,
+			memcg->ebp_hotness_hg[i]);
+	}
+
+	seq_puts(m, s.buffer);
+	kfree(s.buffer);
+
+	return 0;
+}
+
+static struct cftype memcg_access_map_file[] = {
+	{
+		.name = "access_map",
+		.flags = CFTYPE_NOT_ON_ROOT,
+		.seq_show = memcg_access_map_show,
+	},
+	{},
+};
+
+static int __init mem_cgroup_access_map_init(void)
+{
+	WARN_ON(cgroup_add_dfl_cftypes(&memory_cgrp_subsys,
+				       memcg_access_map_file));
+	return 0;
+}
+subsys_initcall(mem_cgroup_access_map_init);
+
+static int memcg_hotness_stat_show(struct seq_file *m, void *v)
+{
+	struct mem_cgroup *memcg = mem_cgroup_from_css(seq_css(m));
+	struct seq_buf s;
+	unsigned long hot = 0, warm = 0, cold = 0;
+	int i;
+
+	seq_buf_init(&s, kmalloc(PAGE_SIZE, GFP_KERNEL), PAGE_SIZE);
+	if (!s.buffer)
+		return 0;
+
+	for (i = 15; i >= 0; i--) {
+		if (i >= memcg->active_threshold)
+			hot += memcg->hotness_hg[i];
+		else if (i >= memcg->warm_threshold)
+			warm += memcg->hotness_hg[i];
+		else
+			cold += memcg->hotness_hg[i];
+	}
+
+	seq_buf_printf(&s, "hot %lu warm %lu cold %lu\n", hot, warm, cold);
+
+	seq_puts(m, s.buffer);
+	kfree(s.buffer);
+
+	return 0;
+}
+
+static struct cftype memcg_hotness_stat_file[] = {
+	{
+		.name = "hotness_stat",
+		.flags = CFTYPE_NOT_ON_ROOT,
+		.seq_show = memcg_hotness_stat_show,
+	},
+	{},
+};
+
+static int __init mem_cgroup_hotness_stat_init(void)
+{
+	WARN_ON(cgroup_add_dfl_cftypes(&memory_cgrp_subsys,
+				       memcg_hotness_stat_file));
+	return 0;
+}
+subsys_initcall(mem_cgroup_hotness_stat_init);
+
+static int memcg_per_node_max_show(struct seq_file *m, void *v)
+{
+	struct mem_cgroup *memcg = mem_cgroup_from_css(seq_css(m));
+	struct cftype *cur_file = seq_cft(m);
+	int nid = cur_file->numa_node_id;
+	unsigned long max = READ_ONCE(memcg->nodeinfo[nid]->max_nr_base_pages);
+
+	if (max == ULONG_MAX)
+		seq_puts(m, "max\n");
+	else
+		seq_printf(m, "%llu\n", (u64)max * PAGE_SIZE);
+
+	return 0;
+}
+
+static ssize_t memcg_per_node_max_write(struct kernfs_open_file *of, char *buf,
+					size_t nbytes, loff_t off)
+{
+	struct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));
+	struct cftype *cur_file = of_cft(of);
+	int nid = cur_file->numa_node_id;
+	unsigned long max, nr_dram_pages = 0;
+	int err, n;
+
+	buf = strstrip(buf);
+	err = page_counter_memparse(buf, "max", &max);
+	if (err)
+		return err;
+
+	xchg(&memcg->nodeinfo[nid]->max_nr_base_pages, max);
+
+	for_each_node_state (n, N_MEMORY) {
+		if (node_is_toptier(n)) {
+			if (memcg->nodeinfo[n]->max_nr_base_pages != ULONG_MAX)
+				nr_dram_pages +=
+					memcg->nodeinfo[n]->max_nr_base_pages;
+		}
+	}
+	if (nr_dram_pages)
+		memcg->max_nr_dram_pages = nr_dram_pages;
+
+	return nbytes;
+}
+
+static int pgdat_memcg_htmm_init(struct pglist_data *pgdat)
+{
+	pgdat->memcg_htmm_file = kzalloc(sizeof(struct cftype) * 2, GFP_KERNEL);
+	if (!pgdat->memcg_htmm_file) {
+		printk("error: fails to allocate pgdat->memcg_htmm_file\n");
+		return -ENOMEM;
+	}
+#ifdef CONFIG_LOCKDEP
+	lockdep_register_key(&(pgdat->memcg_htmm_file->lockdep_key));
+#endif
+	return 0;
+}
+
+int mem_cgroup_per_node_htmm_init(void)
+{
+	int nid;
+
+	for_each_node_state (nid, N_MEMORY) {
+		struct pglist_data *pgdat = NODE_DATA(nid);
+
+		if (!pgdat || pgdat->memcg_htmm_file)
+			continue;
+		if (pgdat_memcg_htmm_init(pgdat))
+			continue;
+
+		snprintf(pgdat->memcg_htmm_file[0].name, MAX_CFTYPE_NAME,
+			 "max_at_node%d", nid);
+		pgdat->memcg_htmm_file[0].flags = CFTYPE_NOT_ON_ROOT;
+		pgdat->memcg_htmm_file[0].seq_show = memcg_per_node_max_show;
+		pgdat->memcg_htmm_file[0].write = memcg_per_node_max_write;
+		pgdat->memcg_htmm_file[0].numa_node_id = nid;
+
+		WARN_ON(cgroup_add_dfl_cftypes(&memory_cgrp_subsys,
+					       pgdat->memcg_htmm_file));
+	}
+	return 0;
+}
+subsys_initcall(mem_cgroup_per_node_htmm_init);
diff --git a/mm/memtis/migrate.c b/mm/memtis/migrate.c
new file mode 100644
index 000000000000..54c080761a8b
--- /dev/null
+++ b/mm/memtis/migrate.c
@@ -0,0 +1,118 @@
+#include <linux/rmap.h>
+#include <linux/migrate.h>
+#include <linux/pagemap.h>
+#include <linux/htmm.h>
+
+// This function is deleted since 89f6c88 ("mm: __isolate_lru_page_prepare() in isolate_migratepages_block()")
+/*
+ * Attempt to remove the specified page from its LRU.  Only take this page
+ * if it is of the appropriate PageActive status.  Pages which are being
+ * freed elsewhere are also ignored.
+ *
+ * page:	page to consider
+ * mode:	one of the LRU isolation modes defined above
+ *
+ * returns true on success, false on failure.
+ */
+bool __isolate_lru_page_prepare(struct page *page, isolate_mode_t mode)
+{
+	/* Only take pages on the LRU. */
+	if (!PageLRU(page))
+		return false;
+
+	/* Compaction should not handle unevictable pages but CMA can do so */
+	if (PageUnevictable(page) && !(mode & ISOLATE_UNEVICTABLE))
+		return false;
+
+	/*
+	 * To minimise LRU disruption, the caller can indicate that it only
+	 * wants to isolate pages it will be able to operate on without
+	 * blocking - clean pages for the most part.
+	 *
+	 * ISOLATE_ASYNC_MIGRATE is used to indicate that it only wants to pages
+	 * that it is possible to migrate without blocking
+	 */
+	if (mode & ISOLATE_ASYNC_MIGRATE) {
+		/* All the caller can do on PageWriteback is block */
+		if (PageWriteback(page))
+			return false;
+
+		if (PageDirty(page)) {
+			struct address_space *mapping;
+			bool migrate_dirty;
+
+			/*
+			 * Only pages without mappings or that have a
+			 * ->migratepage callback are possible to migrate
+			 * without blocking. However, we can be racing with
+			 * truncation so it's necessary to lock the page
+			 * to stabilise the mapping as truncation holds
+			 * the page lock until after the page is removed
+			 * from the page cache.
+			 */
+			if (!trylock_page(page))
+				return false;
+
+			mapping = page_mapping(page);
+			migrate_dirty = !mapping || mapping->a_ops->migratepage;
+			unlock_page(page);
+			if (!migrate_dirty)
+				return false;
+		}
+	}
+
+	if ((mode & ISOLATE_UNMAPPED) && page_mapped(page))
+		return false;
+
+	return true;
+}
+bool try_to_unmap_clean(struct page_vma_mapped_walk *pvmw, struct page *page)
+{
+	void *addr;
+	bool dirty;
+	pte_t newpte;
+	pginfo_t *pginfo;
+
+	VM_BUG_ON_PAGE(PageCompound(page), page);
+	VM_BUG_ON_PAGE(!PageAnon(page), page);
+	VM_BUG_ON_PAGE(!PageLocked(page), page);
+	VM_BUG_ON_PAGE(pte_present(*pvmw->pte), page);
+
+	if (PageMlocked(page) || (pvmw->vma->vm_flags & VM_LOCKED))
+		return false;
+
+	/* accessed ptes --> no zeroed pages */
+	pginfo = get_pginfo_from_pte(pvmw->pte);
+	if (!pginfo)
+		return false;
+	if (pginfo->nr_accesses > 0)
+		return false;
+
+	/*
+	* The pmd entry mapping the old thp was flushed and the pte mapping
+	* this subpage has been non present. Therefore, this subpage is
+	* inaccessible. We don't need to remap it if it contains only zeros.
+	*/
+	addr = kmap_local_page(page);
+	dirty = memchr_inv(addr, 0, PAGE_SIZE);
+	kunmap_local(addr);
+
+	if (dirty)
+		return false;
+
+	pte_clear_not_present_full(pvmw->vma->vm_mm, pvmw->address, pvmw->pte,
+				   false);
+
+	if (userfaultfd_armed(pvmw->vma)) {
+		newpte = pte_mkspecial(
+			pfn_pte(page_to_pfn(ZERO_PAGE(pvmw->address)),
+				pvmw->vma->vm_page_prot));
+		ptep_clear_flush(pvmw->vma, pvmw->address, pvmw->pte);
+		set_pte_at(pvmw->vma->vm_mm, pvmw->address, pvmw->pte, newpte);
+		dec_mm_counter(pvmw->vma->vm_mm, MM_ANONPAGES);
+		return true;
+	}
+
+	dec_mm_counter(pvmw->vma->vm_mm, mm_counter(page));
+	return true;
+}
diff --git a/mm/memtis/migrater.c b/mm/memtis/migrater.c
new file mode 100644
index 000000000000..ec38fbe8d800
--- /dev/null
+++ b/mm/memtis/migrater.c
@@ -0,0 +1,1103 @@
+/*
+ * Taehyung Lee (SKKU, taehyunggg@skku.edu, taehyung.tlee@gmail.com)
+ * -- kmigrated
+ */
+#include <linux/kthread.h>
+#include <linux/list.h>
+#include <linux/memcontrol.h>
+#include <linux/mempolicy.h>
+#include <linux/mmzone.h>
+#include <linux/mm_inline.h>
+#include <linux/migrate.h>
+#include <linux/swap.h>
+#include <linux/rmap.h>
+#include <linux/delay.h>
+#include <linux/node.h>
+#include <linux/htmm.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+
+#include "../internal.h"
+
+#define MIN_WATERMARK_LOWER_LIMIT 128 * 100 // 50MB
+#define MIN_WATERMARK_UPPER_LIMIT 2560 * 100 // 1000MB
+#define MAX_WATERMARK_LOWER_LIMIT 256 * 100 // 100MB
+#define MAX_WATERMARK_UPPER_LIMIT 3840 * 100 // 1500MB
+
+#ifdef ARCH_HAS_PREFETCHW
+#define prefetchw_prev_lru_page(_page, _base, _field)                                                                  \
+	do {                                                                                                           \
+		if ((_page)->lru.prev != _base) {                                                                      \
+			struct page *prev;                                                                             \
+			prev = lru_to_page(&(_page->lru));                                                             \
+			prefetchw(&prev->_field);                                                                      \
+		}                                                                                                      \
+	} while (0)
+#else
+#define prefetchw_prev_lru_page(_page, _base, _field)                                                                  \
+	do {                                                                                                           \
+	} while (0)
+#endif
+
+void add_memcg_to_kmigraterd(struct mem_cgroup *memcg, int nid)
+{
+	struct mem_cgroup_per_node *mz, *pn = memcg->nodeinfo[nid];
+	pg_data_t *pgdat = NODE_DATA(nid);
+
+	if (!pgdat)
+		return;
+
+	if (pn->memcg != memcg)
+		printk("memcg mismatch!\n");
+
+	spin_lock(&pgdat->kmigraterd_lock);
+	list_for_each_entry (mz, &pgdat->kmigraterd_head, kmigraterd_list) {
+		if (mz == pn)
+			goto add_unlock;
+	}
+	list_add_tail(&pn->kmigraterd_list, &pgdat->kmigraterd_head);
+add_unlock:
+	spin_unlock(&pgdat->kmigraterd_lock);
+}
+
+void del_memcg_from_kmigraterd(struct mem_cgroup *memcg, int nid)
+{
+	struct mem_cgroup_per_node *mz, *pn = memcg->nodeinfo[nid];
+	pg_data_t *pgdat = NODE_DATA(nid);
+
+	if (!pgdat)
+		return;
+
+	spin_lock(&pgdat->kmigraterd_lock);
+	list_for_each_entry (mz, &pgdat->kmigraterd_head, kmigraterd_list) {
+		if (mz == pn) {
+			list_del(&pn->kmigraterd_list);
+			break;
+		}
+	}
+	spin_unlock(&pgdat->kmigraterd_lock);
+}
+
+unsigned long get_memcg_demotion_watermark(unsigned long max_nr_pages)
+{
+	max_nr_pages = max_nr_pages * 2 / 100; // 2%
+	if (max_nr_pages < MIN_WATERMARK_LOWER_LIMIT)
+		return MIN_WATERMARK_LOWER_LIMIT;
+	else if (max_nr_pages > MIN_WATERMARK_UPPER_LIMIT)
+		return MIN_WATERMARK_UPPER_LIMIT;
+	else
+		return max_nr_pages;
+}
+
+unsigned long get_memcg_promotion_watermark(unsigned long max_nr_pages)
+{
+	max_nr_pages = max_nr_pages * 3 / 100; // 3%
+	if (max_nr_pages < MAX_WATERMARK_LOWER_LIMIT)
+		return MIN_WATERMARK_LOWER_LIMIT;
+	else if (max_nr_pages > MAX_WATERMARK_UPPER_LIMIT)
+		return MIN_WATERMARK_UPPER_LIMIT;
+	else
+		return max_nr_pages;
+}
+
+unsigned long get_nr_lru_pages_node(struct mem_cgroup *memcg, pg_data_t *pgdat)
+{
+	struct lruvec *lruvec;
+	unsigned long nr_pages = 0;
+	enum lru_list lru;
+
+	lruvec = mem_cgroup_lruvec(memcg, pgdat);
+
+	for_each_lru (lru)
+		nr_pages += lruvec_lru_size(lruvec, lru, MAX_NR_ZONES);
+
+	return nr_pages;
+}
+
+static unsigned long need_lowertier_promotion(pg_data_t *pgdat, struct mem_cgroup *memcg)
+{
+	struct lruvec *lruvec;
+	unsigned long lruvec_size;
+
+	lruvec = mem_cgroup_lruvec(memcg, pgdat);
+	lruvec_size = lruvec_lru_size(lruvec, LRU_ACTIVE_ANON, MAX_NR_ZONES);
+
+	if (htmm_mode == HTMM_NO_MIG)
+		return 0;
+
+	return lruvec_size;
+}
+
+static bool need_direct_demotion(pg_data_t *pgdat, struct mem_cgroup *memcg)
+{
+	return READ_ONCE(memcg->nodeinfo[pgdat->node_id]->need_demotion);
+}
+
+static bool need_toptier_demotion(pg_data_t *pgdat, struct mem_cgroup *memcg, unsigned long *nr_exceeded)
+{
+	unsigned long nr_lru_pages, max_nr_pages;
+	unsigned long nr_need_promoted;
+	unsigned long fasttier_max_watermark, fasttier_min_watermark;
+	int target_nid = htmm_cxl_mode ? 1 : next_demotion_node(pgdat->node_id);
+	pg_data_t *target_pgdat;
+
+	if (target_nid == NUMA_NO_NODE)
+		return false;
+
+	target_pgdat = NODE_DATA(target_nid);
+
+	max_nr_pages = memcg->nodeinfo[pgdat->node_id]->max_nr_base_pages;
+	nr_lru_pages = get_nr_lru_pages_node(memcg, pgdat);
+
+	fasttier_max_watermark = get_memcg_promotion_watermark(max_nr_pages);
+	fasttier_min_watermark = get_memcg_demotion_watermark(max_nr_pages);
+
+	if (need_direct_demotion(pgdat, memcg)) {
+		if (nr_lru_pages + fasttier_max_watermark <= max_nr_pages)
+			goto check_nr_need_promoted;
+		else if (nr_lru_pages < max_nr_pages)
+			*nr_exceeded = fasttier_max_watermark - (max_nr_pages - nr_lru_pages);
+		else
+			*nr_exceeded = nr_lru_pages + fasttier_max_watermark - max_nr_pages;
+		*nr_exceeded += 1U * 128 * 100; // 100 MB
+		return true;
+	}
+
+check_nr_need_promoted:
+	nr_need_promoted = need_lowertier_promotion(target_pgdat, memcg);
+	if (nr_need_promoted) {
+		if (nr_lru_pages + nr_need_promoted + fasttier_max_watermark <= max_nr_pages)
+			return false;
+	} else {
+		if (nr_lru_pages + fasttier_min_watermark <= max_nr_pages)
+			return false;
+	}
+
+	*nr_exceeded = nr_lru_pages + nr_need_promoted + fasttier_max_watermark - max_nr_pages;
+	return true;
+}
+
+static unsigned long node_free_pages(pg_data_t *pgdat)
+{
+	int z;
+	long free_pages;
+	long total = 0;
+
+	for (z = pgdat->nr_zones - 1; z >= 0; z--) {
+		struct zone *zone = pgdat->node_zones + z;
+		long nr_high_wmark_pages;
+
+		if (!populated_zone(zone))
+			continue;
+
+		free_pages = zone_page_state(zone, NR_FREE_PAGES);
+		free_pages -= zone->nr_reserved_highatomic;
+		free_pages -= zone->lowmem_reserve[ZONE_MOVABLE];
+
+		nr_high_wmark_pages = high_wmark_pages(zone);
+		if (free_pages >= nr_high_wmark_pages)
+			total += (free_pages - nr_high_wmark_pages);
+	}
+	return (unsigned long)total;
+}
+
+static bool promotion_available(int target_nid, struct mem_cgroup *memcg, unsigned long *nr_to_promote)
+{
+	pg_data_t *pgdat;
+	unsigned long max_nr_pages, cur_nr_pages;
+	unsigned long nr_isolated;
+	unsigned long fasttier_max_watermark;
+
+	if (target_nid == NUMA_NO_NODE)
+		return false;
+
+	pgdat = NODE_DATA(target_nid);
+
+	cur_nr_pages = get_nr_lru_pages_node(memcg, pgdat);
+	max_nr_pages = memcg->nodeinfo[target_nid]->max_nr_base_pages;
+	nr_isolated = node_page_state(pgdat, NR_ISOLATED_ANON) + node_page_state(pgdat, NR_ISOLATED_FILE);
+
+	fasttier_max_watermark = get_memcg_promotion_watermark(max_nr_pages);
+
+	if (max_nr_pages == ULONG_MAX) {
+		*nr_to_promote = node_free_pages(pgdat);
+		return true;
+	} else if (cur_nr_pages + nr_isolated < max_nr_pages - fasttier_max_watermark) {
+		*nr_to_promote = max_nr_pages - fasttier_max_watermark - cur_nr_pages - nr_isolated;
+		return true;
+	}
+	return false;
+}
+
+static bool need_lru_cooling(struct mem_cgroup_per_node *pn)
+{
+	return READ_ONCE(pn->need_cooling);
+}
+
+static bool need_lru_adjusting(struct mem_cgroup_per_node *pn)
+{
+	return READ_ONCE(pn->need_adjusting);
+}
+
+static __always_inline void update_lru_sizes(struct lruvec *lruvec, enum lru_list lru, unsigned long *nr_zone_taken)
+{
+	int zid;
+
+	for (zid = 0; zid < MAX_NR_ZONES; zid++) {
+		if (!nr_zone_taken[zid])
+			continue;
+
+		update_lru_size(lruvec, lru, zid, -nr_zone_taken[zid]);
+	}
+}
+
+static unsigned long isolate_lru_pages(unsigned long nr_to_scan, struct lruvec *lruvec, enum lru_list lru,
+				       struct list_head *dst, isolate_mode_t mode)
+{
+	struct list_head *src = &lruvec->lists[lru];
+	unsigned long nr_zone_taken[MAX_NR_ZONES] = { 0 };
+	unsigned long scan = 0, nr_taken = 0;
+	LIST_HEAD(busy_list);
+
+	while (scan < nr_to_scan && !list_empty(src)) {
+		struct page *page;
+		unsigned long nr_pages;
+
+		page = lru_to_page(src);
+		prefetchw_prev_lru_page(page, src, flags);
+		VM_WARN_ON(!PageLRU(page));
+
+		nr_pages = compound_nr(page);
+		scan += nr_pages;
+
+		if (!__isolate_lru_page_prepare(page, 0)) {
+			list_move(&page->lru, src);
+			continue;
+		}
+		if (unlikely(!get_page_unless_zero(page))) {
+			list_move(&page->lru, src);
+			continue;
+		}
+		if (!TestClearPageLRU(page)) {
+			put_page(page);
+			list_move(&page->lru, src);
+			continue;
+		}
+
+		nr_taken += nr_pages;
+		nr_zone_taken[page_zonenum(page)] += nr_pages;
+		list_move(&page->lru, dst);
+	}
+
+	update_lru_sizes(lruvec, lru, nr_zone_taken);
+	return nr_taken;
+}
+
+static struct page *alloc_migrate_page(struct page *page, unsigned long node)
+{
+	int nid = (int)node;
+	int zidx;
+	struct page *newpage = NULL;
+	gfp_t mask = (GFP_HIGHUSER_MOVABLE | __GFP_THISNODE | __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN) &
+		     ~__GFP_RECLAIM;
+
+	if (PageHuge(page))
+		return NULL;
+
+	zidx = zone_idx(page_zone(page));
+	if (is_highmem_idx(zidx) || zidx == ZONE_MOVABLE)
+		mask |= __GFP_HIGHMEM;
+
+	if (thp_migration_supported() && PageTransHuge(page)) {
+		mask |= GFP_TRANSHUGE_LIGHT;
+		newpage = __alloc_pages_node(nid, mask, HPAGE_PMD_ORDER);
+
+		if (!newpage)
+			return NULL;
+
+		prep_transhuge_page(newpage);
+		__prep_transhuge_page_for_htmm(NULL, newpage);
+	} else
+		newpage = __alloc_pages_node(nid, mask, 0);
+
+	return newpage;
+}
+
+static unsigned long migrate_page_list(struct list_head *migrate_list, pg_data_t *pgdat, bool promotion)
+{
+	int target_nid;
+	unsigned int nr_succeeded = 0;
+
+	if (promotion)
+		target_nid = htmm_cxl_mode ? 0 : next_promotion_node(pgdat->node_id);
+	else
+		target_nid = htmm_cxl_mode ? 1 : next_demotion_node(pgdat->node_id);
+
+	if (list_empty(migrate_list))
+		return 0;
+
+	if (target_nid == NUMA_NO_NODE)
+		return 0;
+
+	migrate_pages(migrate_list, alloc_migrate_page, NULL, target_nid, MIGRATE_ASYNC, MR_NUMA_MISPLACED,
+		      &nr_succeeded);
+
+	if (promotion)
+		count_vm_events(HTMM_NR_PROMOTED, nr_succeeded);
+	else
+		count_vm_events(HTMM_NR_DEMOTED, nr_succeeded);
+
+	return nr_succeeded;
+}
+
+static unsigned long shrink_page_list(struct list_head *page_list, pg_data_t *pgdat, struct mem_cgroup *memcg,
+				      bool shrink_active, unsigned long nr_to_reclaim)
+{
+	LIST_HEAD(demote_pages);
+	LIST_HEAD(ret_pages);
+	unsigned long nr_reclaimed = 0;
+	unsigned long nr_demotion_cand = 0;
+
+	cond_resched();
+
+	guard(vmstat_stopwatch)(DEMOTE_NS);
+	while (!list_empty(page_list)) {
+		struct page *page;
+
+		page = lru_to_page(page_list);
+		list_del(&page->lru);
+
+		if (!trylock_page(page))
+			goto keep;
+		if (!shrink_active && PageAnon(page) && PageActive(page))
+			goto keep_locked;
+		if (unlikely(!page_evictable(page)))
+			goto keep_locked;
+		if (PageWriteback(page))
+			goto keep_locked;
+		if (PageTransHuge(page) && !thp_migration_supported())
+			goto keep_locked;
+		if (!PageAnon(page) && nr_demotion_cand > nr_to_reclaim + HTMM_MIN_FREE_PAGES)
+			goto keep_locked;
+
+		if (htmm_nowarm == 0 && PageAnon(page)) {
+			guard(vmstat_stopwatch)(PTEXT_NS);
+			if (PageTransHuge(page)) {
+				struct page *meta = get_meta_page(page);
+
+				if (meta->idx >= memcg->warm_threshold)
+					goto keep_locked;
+			} else {
+				unsigned int idx = get_pginfo_idx(page);
+
+				if (idx >= memcg->warm_threshold)
+					goto keep_locked;
+			}
+		}
+
+		unlock_page(page);
+		list_add(&page->lru, &demote_pages);
+		nr_demotion_cand += compound_nr(page);
+		continue;
+
+	keep_locked:
+		unlock_page(page);
+	keep:
+		list_add(&page->lru, &ret_pages);
+	}
+
+	nr_reclaimed = migrate_page_list(&demote_pages, pgdat, false);
+	if (!list_empty(&demote_pages))
+		list_splice(&demote_pages, page_list);
+
+	list_splice(&ret_pages, page_list);
+	return nr_reclaimed;
+}
+
+static unsigned long promote_page_list(struct list_head *page_list, pg_data_t *pgdat)
+{
+	LIST_HEAD(promote_pages);
+	LIST_HEAD(ret_pages);
+	unsigned long nr_promoted = 0;
+
+	cond_resched();
+
+	guard(vmstat_stopwatch)(PROMOTE_NS);
+	while (!list_empty(page_list)) {
+		struct page *page;
+
+		page = lru_to_page(page_list);
+		list_del(&page->lru);
+
+		if (!trylock_page(page))
+			goto __keep;
+		if (!PageActive(page) && htmm_mode != HTMM_NO_MIG)
+			goto __keep_locked;
+		if (unlikely(!page_evictable(page)))
+			goto __keep_locked;
+		if (PageWriteback(page))
+			goto __keep_locked;
+		if (PageTransHuge(page) && !thp_migration_supported())
+			goto __keep_locked;
+
+		list_add(&page->lru, &promote_pages);
+		unlock_page(page);
+		continue;
+	__keep_locked:
+		unlock_page(page);
+	__keep:
+		list_add(&page->lru, &ret_pages);
+	}
+
+	nr_promoted = migrate_page_list(&promote_pages, pgdat, true);
+	if (!list_empty(&promote_pages))
+		list_splice(&promote_pages, page_list);
+
+	list_splice(&ret_pages, page_list);
+	return nr_promoted;
+}
+
+static unsigned long demote_inactive_list(unsigned long nr_to_scan, unsigned long nr_to_reclaim, struct lruvec *lruvec,
+					  enum lru_list lru, bool shrink_active)
+{
+	LIST_HEAD(page_list);
+	pg_data_t *pgdat = lruvec_pgdat(lruvec);
+	unsigned long nr_reclaimed = 0, nr_taken;
+	int file = is_file_lru(lru);
+
+	lru_add_drain();
+
+	spin_lock_irq(&lruvec->lru_lock);
+	nr_taken = isolate_lru_pages(nr_to_scan, lruvec, lru, &page_list, 0);
+	__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, nr_taken);
+	spin_unlock_irq(&lruvec->lru_lock);
+
+	if (nr_taken == 0) {
+		return 0;
+	}
+
+	nr_reclaimed = shrink_page_list(&page_list, pgdat, lruvec_memcg(lruvec), shrink_active, nr_to_reclaim);
+
+	spin_lock_irq(&lruvec->lru_lock);
+	move_pages_to_lru(lruvec, &page_list);
+	__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, -nr_taken);
+	spin_unlock_irq(&lruvec->lru_lock);
+
+	mem_cgroup_uncharge_list(&page_list);
+	free_unref_page_list(&page_list);
+
+	return nr_reclaimed;
+}
+
+static unsigned long promote_active_list(unsigned long nr_to_scan, struct lruvec *lruvec, enum lru_list lru)
+{
+	LIST_HEAD(page_list);
+	pg_data_t *pgdat = lruvec_pgdat(lruvec);
+	unsigned long nr_taken, nr_promoted;
+
+	lru_add_drain();
+
+	spin_lock_irq(&lruvec->lru_lock);
+	nr_taken = isolate_lru_pages(nr_to_scan, lruvec, lru, &page_list, 0);
+	__mod_node_page_state(pgdat, NR_ISOLATED_ANON, nr_taken);
+	spin_unlock_irq(&lruvec->lru_lock);
+
+	if (nr_taken == 0)
+		return 0;
+
+	nr_promoted = promote_page_list(&page_list, pgdat);
+
+	spin_lock_irq(&lruvec->lru_lock);
+	move_pages_to_lru(lruvec, &page_list);
+	__mod_node_page_state(pgdat, NR_ISOLATED_ANON, -nr_taken);
+	spin_unlock_irq(&lruvec->lru_lock);
+
+	mem_cgroup_uncharge_list(&page_list);
+	free_unref_page_list(&page_list);
+
+	return nr_promoted;
+}
+
+static unsigned long demote_lruvec(unsigned long nr_to_reclaim, short priority, pg_data_t *pgdat, struct lruvec *lruvec,
+				   bool shrink_active)
+{
+	enum lru_list lru, tmp;
+	unsigned long nr_reclaimed = 0;
+	long nr_to_scan;
+
+	/* we need to scan file lrus first */
+	for_each_evictable_lru (tmp) {
+		lru = (tmp + 2) % 4;
+
+		if (!shrink_active && !is_file_lru(lru) && is_active_lru(lru))
+			continue;
+
+		if (is_file_lru(lru)) {
+			nr_to_scan = lruvec_lru_size(lruvec, lru, MAX_NR_ZONES);
+		} else {
+			nr_to_scan = lruvec_lru_size(lruvec, lru, MAX_NR_ZONES) >> priority;
+
+			if (nr_to_scan < nr_to_reclaim)
+				nr_to_scan = nr_to_reclaim * 11 / 10; // because warm pages are not demoted
+		}
+
+		if (!nr_to_scan)
+			continue;
+
+		while (nr_to_scan > 0) {
+			unsigned long scan = min(nr_to_scan, SWAP_CLUSTER_MAX);
+			nr_reclaimed += demote_inactive_list(scan, scan, lruvec, lru, shrink_active);
+			nr_to_scan -= (long)scan;
+			if (nr_reclaimed >= nr_to_reclaim)
+				break;
+		}
+
+		if (nr_reclaimed >= nr_to_reclaim)
+			break;
+	}
+
+	return nr_reclaimed;
+}
+
+static unsigned long promote_lruvec(unsigned long nr_to_promote, short priority, pg_data_t *pgdat,
+				    struct lruvec *lruvec, enum lru_list lru)
+{
+	unsigned long nr_promoted = 0, nr;
+
+	nr = nr_to_promote >> priority;
+	if (nr)
+		nr_promoted += promote_active_list(nr, lruvec, lru);
+
+	return nr_promoted;
+}
+
+static unsigned long demote_node(pg_data_t *pgdat, struct mem_cgroup *memcg, unsigned long nr_exceeded)
+{
+	struct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);
+	short priority = DEF_PRIORITY;
+	unsigned long nr_to_reclaim = 0, nr_evictable_pages = 0, nr_reclaimed = 0;
+	enum lru_list lru;
+	bool shrink_active = false;
+
+	for_each_evictable_lru (lru) {
+		if (!is_file_lru(lru) && is_active_lru(lru))
+			continue;
+
+		nr_evictable_pages += lruvec_lru_size(lruvec, lru, MAX_NR_ZONES);
+	}
+
+	nr_to_reclaim = nr_exceeded;
+
+	if (nr_exceeded > nr_evictable_pages && need_direct_demotion(pgdat, memcg))
+		shrink_active = true;
+
+	do {
+		nr_reclaimed += demote_lruvec(nr_to_reclaim - nr_reclaimed, priority, pgdat, lruvec, shrink_active);
+		if (nr_reclaimed >= nr_to_reclaim)
+			break;
+		priority--;
+	} while (priority);
+
+	if (htmm_nowarm == 0) {
+		int target_nid = htmm_cxl_mode ? 1 : next_demotion_node(pgdat->node_id);
+		unsigned long nr_lowertier_active =
+			target_nid == NUMA_NO_NODE ? 0 : need_lowertier_promotion(NODE_DATA(target_nid), memcg);
+
+		nr_lowertier_active = nr_lowertier_active < nr_to_reclaim ? nr_lowertier_active : nr_to_reclaim;
+		if (nr_lowertier_active && nr_reclaimed < nr_lowertier_active)
+			memcg->warm_threshold = memcg->active_threshold;
+	}
+
+	/* check the condition */
+	do {
+		unsigned long max = memcg->nodeinfo[pgdat->node_id]->max_nr_base_pages;
+		if (get_nr_lru_pages_node(memcg, pgdat) + get_memcg_demotion_watermark(max) < max)
+			WRITE_ONCE(memcg->nodeinfo[pgdat->node_id]->need_demotion, false);
+	} while (0);
+	return nr_reclaimed;
+}
+
+static unsigned long promote_node(pg_data_t *pgdat, struct mem_cgroup *memcg)
+{
+	struct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);
+	unsigned long nr_to_promote, nr_promoted = 0, tmp;
+	enum lru_list lru = LRU_ACTIVE_ANON;
+	short priority = DEF_PRIORITY;
+	int target_nid = htmm_cxl_mode ? 0 : next_promotion_node(pgdat->node_id);
+
+	if (!promotion_available(target_nid, memcg, &nr_to_promote))
+		return 0;
+
+	nr_to_promote = min(nr_to_promote, lruvec_lru_size(lruvec, lru, MAX_NR_ZONES));
+
+	if (nr_to_promote == 0 && htmm_mode == HTMM_NO_MIG) {
+		lru = LRU_INACTIVE_ANON;
+		nr_to_promote = min(tmp, lruvec_lru_size(lruvec, lru, MAX_NR_ZONES));
+	}
+	do {
+		nr_promoted += promote_lruvec(nr_to_promote, priority, pgdat, lruvec, lru);
+		if (nr_promoted >= nr_to_promote)
+			break;
+		priority--;
+	} while (priority);
+
+	return nr_promoted;
+}
+
+static unsigned long cooling_active_list(unsigned long nr_to_scan, struct lruvec *lruvec, enum lru_list lru)
+{
+	unsigned long nr_taken;
+	struct mem_cgroup *memcg = lruvec_memcg(lruvec);
+	pg_data_t *pgdat = lruvec_pgdat(lruvec);
+	LIST_HEAD(l_hold);
+	LIST_HEAD(l_active);
+	LIST_HEAD(l_inactive);
+	int file = is_file_lru(lru);
+
+	lru_add_drain();
+
+	spin_lock_irq(&lruvec->lru_lock);
+	nr_taken = isolate_lru_pages(nr_to_scan, lruvec, lru, &l_hold, 0);
+	__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, nr_taken);
+	spin_unlock_irq(&lruvec->lru_lock);
+
+	cond_resched();
+	while (!list_empty(&l_hold)) {
+		struct page *page;
+
+		page = lru_to_page(&l_hold);
+		list_del(&page->lru);
+
+		if (unlikely(!page_evictable(page))) {
+			putback_lru_page(page);
+			continue;
+		}
+
+		if (!file) {
+			int still_hot;
+
+			if (PageTransHuge(compound_head(page))) {
+				struct page *meta = get_meta_page(page);
+
+#ifdef DEFERRED_SPLIT_ISOLATED
+				if (check_split_huge_page(memcg, get_meta_page(page), false)) {
+					spin_lock_irq(&lruvec->lru_lock);
+					if (deferred_split_huge_page_for_htmm(compound_head(page))) {
+						spin_unlock_irq(&lruvec->lru_lock);
+						check_transhuge_cooling((void *)memcg, page, false);
+						continue;
+					}
+					spin_unlock_irq(&lruvec->lru_lock);
+				}
+#endif
+				check_transhuge_cooling((void *)memcg, page, false);
+
+				if (meta->idx >= memcg->active_threshold)
+					still_hot = 2;
+				else
+					still_hot = 1;
+			} else {
+				still_hot = cooling_page(page, lruvec_memcg(lruvec));
+			}
+
+			if (still_hot == 2) {
+				/* page is still hot after cooling */
+				if (!PageActive(page))
+					SetPageActive(page);
+				list_add(&page->lru, &l_active);
+				continue;
+			} else if (still_hot == 0) {
+				/* not cooled page */
+				if (PageActive(page))
+					list_add(&page->lru, &l_active);
+				else
+					list_add(&page->lru, &l_inactive);
+				continue;
+			}
+			// still_hot == 1
+		}
+		/* cold or file page */
+		ClearPageActive(page);
+		SetPageWorkingset(page);
+		list_add(&page->lru, &l_inactive);
+	}
+
+	spin_lock_irq(&lruvec->lru_lock);
+	move_pages_to_lru(lruvec, &l_active);
+	move_pages_to_lru(lruvec, &l_inactive);
+	list_splice(&l_inactive, &l_active);
+
+	__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, -nr_taken);
+	spin_unlock_irq(&lruvec->lru_lock);
+
+	mem_cgroup_uncharge_list(&l_active);
+	free_unref_page_list(&l_active);
+
+	return nr_taken;
+}
+
+static void cooling_node(pg_data_t *pgdat, struct mem_cgroup *memcg)
+{
+	unsigned long nr_to_scan, nr_scanned = 0, nr_max_scan = 12;
+	struct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);
+	struct mem_cgroup_per_node *pn = memcg->nodeinfo[pgdat->node_id];
+	enum lru_list lru = LRU_ACTIVE_ANON;
+
+re_cooling:
+	nr_to_scan = lruvec_lru_size(lruvec, lru, MAX_NR_ZONES);
+	do {
+		unsigned long scan = nr_to_scan >> 3; /* 12.5% */
+
+		if (!scan)
+			scan = nr_to_scan;
+		/* limits the num. of scanned pages to reduce the lock holding time */
+		nr_scanned += cooling_active_list(scan, lruvec, lru);
+		nr_max_scan--;
+	} while (nr_scanned < nr_to_scan && nr_max_scan);
+
+	if (is_active_lru(lru)) {
+		lru = LRU_INACTIVE_ANON;
+		nr_max_scan = 12;
+		nr_scanned = 0;
+		goto re_cooling;
+	}
+
+	/* active file list */
+	cooling_active_list(lruvec_lru_size(lruvec, LRU_ACTIVE_FILE, MAX_NR_ZONES), lruvec, LRU_ACTIVE_FILE);
+	//if (nr_scanned >= nr_to_scan)
+	WRITE_ONCE(pn->need_cooling, false);
+}
+
+static unsigned long adjusting_lru_list(unsigned long nr_to_scan, struct lruvec *lruvec, enum lru_list lru,
+					unsigned int *nr_huge, unsigned int *nr_base)
+{
+	unsigned long nr_taken;
+	pg_data_t *pgdat = lruvec_pgdat(lruvec);
+	struct mem_cgroup *memcg = lruvec_memcg(lruvec);
+	LIST_HEAD(l_hold);
+	LIST_HEAD(l_active);
+	LIST_HEAD(l_inactive);
+	int file = is_file_lru(lru);
+	bool active = is_active_lru(lru);
+
+	unsigned int nr_split_cand = 0, nr_split_hot = 0;
+
+	if (file)
+		return 0;
+
+	lru_add_drain();
+
+	spin_lock_irq(&lruvec->lru_lock);
+	nr_taken = isolate_lru_pages(nr_to_scan, lruvec, lru, &l_hold, 0);
+	__mod_node_page_state(pgdat, NR_ISOLATED_ANON, nr_taken);
+	spin_unlock_irq(&lruvec->lru_lock);
+
+	cond_resched();
+	while (!list_empty(&l_hold)) {
+		struct page *page;
+		int status;
+
+		page = lru_to_page(&l_hold);
+		list_del(&page->lru);
+
+		if (unlikely(!page_evictable(page))) {
+			putback_lru_page(page);
+			continue;
+		}
+#ifdef DEFERRED_SPLIT_ISOLATED
+		if (PageCompound(page) && check_split_huge_page(memcg, get_meta_page(page), false)) {
+			spin_lock_irq(&lruvec->lru_lock);
+			if (!deferred_split_huge_page_for_htmm(compound_head(page))) {
+				if (PageActive(page))
+					list_add(&page->lru, &l_active);
+				else
+					list_add(&page->lru, &l_inactive);
+			}
+			spin_unlock_irq(&lruvec->lru_lock);
+			continue;
+		}
+#endif
+		if (PageTransHuge(compound_head(page))) {
+			struct page *meta = get_meta_page(page);
+
+			if (meta->idx >= memcg->active_threshold)
+				status = 2;
+			else
+				status = 1;
+			nr_split_hot++;
+		} else {
+			status = page_check_hotness(page, memcg);
+			nr_split_cand++;
+		}
+
+		if (status == 2) {
+			if (active) {
+				list_add(&page->lru, &l_active);
+				continue;
+			}
+
+			SetPageActive(page);
+			list_add(&page->lru, &l_active);
+		} else if (status == 0) {
+			if (PageActive(page))
+				list_add(&page->lru, &l_active);
+			else
+				list_add(&page->lru, &l_inactive);
+		} else {
+			if (!active) {
+				list_add(&page->lru, &l_inactive);
+				continue;
+			}
+
+			ClearPageActive(page);
+			SetPageWorkingset(page);
+			list_add(&page->lru, &l_inactive);
+		}
+	}
+
+	spin_lock_irq(&lruvec->lru_lock);
+	move_pages_to_lru(lruvec, &l_active);
+	move_pages_to_lru(lruvec, &l_inactive);
+	list_splice(&l_inactive, &l_active);
+
+	__mod_node_page_state(pgdat, NR_ISOLATED_ANON, -nr_taken);
+	spin_unlock_irq(&lruvec->lru_lock);
+
+	mem_cgroup_uncharge_list(&l_active);
+	free_unref_page_list(&l_active);
+
+	*nr_huge += nr_split_hot;
+	*nr_base += nr_split_cand;
+
+	return nr_taken;
+}
+
+static void adjusting_node(pg_data_t *pgdat, struct mem_cgroup *memcg, bool active)
+{
+	struct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);
+	struct mem_cgroup_per_node *pn = memcg->nodeinfo[pgdat->node_id];
+	enum lru_list lru = active ? LRU_ACTIVE_ANON : LRU_INACTIVE_ANON;
+	unsigned long nr_to_scan, nr_scanned = 0, nr_max_scan = 12;
+	unsigned int nr_huge = 0, nr_base = 0;
+
+	nr_to_scan = lruvec_lru_size(lruvec, lru, MAX_NR_ZONES);
+	do {
+		unsigned long scan = nr_to_scan >> 3;
+
+		if (!scan)
+			scan = nr_to_scan;
+		nr_scanned += adjusting_lru_list(scan, lruvec, lru, &nr_huge, &nr_base);
+		nr_max_scan--;
+	} while (nr_scanned < nr_to_scan && nr_max_scan);
+
+	if (nr_scanned >= nr_to_scan)
+		WRITE_ONCE(pn->need_adjusting, false);
+	if (nr_scanned >= nr_to_scan && !active)
+		WRITE_ONCE(pn->need_adjusting_all, false);
+}
+
+static struct mem_cgroup_per_node *next_memcg_cand(pg_data_t *pgdat)
+{
+	struct mem_cgroup_per_node *pn;
+
+	spin_lock(&pgdat->kmigraterd_lock);
+	if (!list_empty(&pgdat->kmigraterd_head)) {
+		pn = list_first_entry(&pgdat->kmigraterd_head, typeof(*pn), kmigraterd_list);
+		list_move_tail(&pn->kmigraterd_list, &pgdat->kmigraterd_head);
+	} else
+		pn = NULL;
+	spin_unlock(&pgdat->kmigraterd_lock);
+
+	return pn;
+}
+
+static int kmigraterd_demotion(pg_data_t *pgdat)
+{
+	const struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);
+
+	if (!cpumask_empty(cpumask))
+		set_cpus_allowed_ptr(pgdat->kmigraterd, cpumask);
+
+	for (;;) {
+		guard(vmstat_stopwatch)(LRU_ROTATE_NS);
+		struct mem_cgroup_per_node *pn;
+		struct mem_cgroup *memcg;
+		unsigned long nr_exceeded = 0;
+		LIST_HEAD(split_list);
+
+		if (kthread_should_stop())
+			break;
+
+		pn = next_memcg_cand(pgdat);
+		if (!pn) {
+			msleep_interruptible(1000);
+			continue;
+		}
+
+		memcg = pn->memcg;
+		if (!memcg || !memcg->htmm_enabled) {
+			spin_lock(&pgdat->kmigraterd_lock);
+			if (!list_empty_careful(&pn->kmigraterd_list))
+				list_del(&pn->kmigraterd_list);
+			spin_unlock(&pgdat->kmigraterd_lock);
+			continue;
+		}
+
+		/* performs split */
+		if (htmm_thres_split != 0 && !list_empty(&(&pn->deferred_split_queue)->split_queue)) {
+			unsigned long nr_split;
+			nr_split = deferred_split_scan_for_htmm(pn, &split_list);
+			if (!list_empty(&split_list)) {
+				putback_split_pages(&split_list, mem_cgroup_lruvec(memcg, pgdat));
+			}
+		}
+		/* performs cooling */
+		if (need_lru_cooling(pn))
+			cooling_node(pgdat, memcg);
+		else if (need_lru_adjusting(pn)) {
+			adjusting_node(pgdat, memcg, true);
+			if (pn->need_adjusting_all == true)
+				// adjusting the inactive list
+				adjusting_node(pgdat, memcg, false);
+		}
+
+		/* demotes inactive lru pages */
+		if (need_toptier_demotion(pgdat, memcg, &nr_exceeded)) {
+			demote_node(pgdat, memcg, nr_exceeded);
+		}
+		//if (need_direct_demotion(pgdat, memcg))
+		//  goto demotion;
+
+		/* default: wait 50 ms */
+		wait_event_interruptible_timeout(pgdat->kmigraterd_wait, need_direct_demotion(pgdat, memcg),
+						 msecs_to_jiffies(htmm_demotion_period_in_ms));
+	}
+	return 0;
+}
+
+static int kmigraterd_promotion(pg_data_t *pgdat)
+{
+	const struct cpumask *cpumask;
+
+	if (htmm_cxl_mode)
+		cpumask = cpumask_of_node(pgdat->node_id);
+	else
+		cpumask = cpumask_of_node(pgdat->node_id - 2);
+
+	if (!cpumask_empty(cpumask))
+		set_cpus_allowed_ptr(pgdat->kmigraterd, cpumask);
+
+	for (;;) {
+		guard(vmstat_stopwatch)(LRU_ROTATE_NS);
+		struct mem_cgroup_per_node *pn;
+		struct mem_cgroup *memcg;
+		LIST_HEAD(split_list);
+
+		if (kthread_should_stop())
+			break;
+
+		pn = next_memcg_cand(pgdat);
+		if (!pn) {
+			msleep_interruptible(1000);
+			continue;
+		}
+
+		memcg = pn->memcg;
+		if (!memcg || !memcg->htmm_enabled) {
+			spin_lock(&pgdat->kmigraterd_lock);
+			if (!list_empty_careful(&pn->kmigraterd_list))
+				list_del(&pn->kmigraterd_list);
+			spin_unlock(&pgdat->kmigraterd_lock);
+			continue;
+		}
+
+		/* performs split */
+		if (htmm_thres_split != 0 && !list_empty(&(&pn->deferred_split_queue)->split_queue)) {
+			unsigned long nr_split;
+			nr_split = deferred_split_scan_for_htmm(pn, &split_list);
+			if (!list_empty(&split_list)) {
+				putback_split_pages(&split_list, mem_cgroup_lruvec(memcg, pgdat));
+			}
+		}
+
+		if (need_lru_cooling(pn))
+			cooling_node(pgdat, memcg);
+		else if (need_lru_adjusting(pn)) {
+			adjusting_node(pgdat, memcg, true);
+			if (pn->need_adjusting_all == true)
+				// adjusting the inactive list
+				adjusting_node(pgdat, memcg, false);
+		}
+
+		/* promotes hot pages to fast memory node */
+		if (need_lowertier_promotion(pgdat, memcg)) {
+			promote_node(pgdat, memcg);
+		}
+
+		msleep_interruptible(htmm_promotion_period_in_ms);
+	}
+
+	return 0;
+}
+
+static int kmigraterd(void *p)
+{
+	pg_data_t *pgdat = (pg_data_t *)p;
+	int nid = pgdat->node_id;
+
+	if (htmm_cxl_mode) {
+		if (nid == 0)
+			return kmigraterd_demotion(pgdat);
+		else
+			return kmigraterd_promotion(pgdat);
+	}
+
+	if (node_is_toptier(nid))
+		return kmigraterd_demotion(pgdat);
+	else
+		return kmigraterd_promotion(pgdat);
+}
+
+void kmigraterd_wakeup(int nid)
+{
+	pg_data_t *pgdat = NODE_DATA(nid);
+	wake_up_interruptible(&pgdat->kmigraterd_wait);
+}
+
+static void kmigraterd_run(int nid)
+{
+	pg_data_t *pgdat = NODE_DATA(nid);
+	if (!pgdat || pgdat->kmigraterd)
+		return;
+
+	init_waitqueue_head(&pgdat->kmigraterd_wait);
+
+	pgdat->kmigraterd = kthread_run(kmigraterd, pgdat, "kmigraterd%d", nid);
+	if (IS_ERR(pgdat->kmigraterd)) {
+		pr_err("Fails to start kmigraterd on node %d\n", nid);
+		pgdat->kmigraterd = NULL;
+	}
+}
+
+void kmigraterd_stop(void)
+{
+	int nid;
+
+	for_each_node_state (nid, N_MEMORY) {
+		struct task_struct *km = NODE_DATA(nid)->kmigraterd;
+
+		if (km) {
+			kthread_stop(km);
+			NODE_DATA(nid)->kmigraterd = NULL;
+		}
+	}
+}
+
+int kmigraterd_init(void)
+{
+	int nid;
+
+	for_each_node_state (nid, N_MEMORY)
+		kmigraterd_run(nid);
+	return 0;
+}
diff --git a/mm/memtis/rmap.c b/mm/memtis/rmap.c
new file mode 100644
index 000000000000..1c7deaae9ee0
--- /dev/null
+++ b/mm/memtis/rmap.c
@@ -0,0 +1,231 @@
+#include <linux/rmap.h>
+#include <linux/random.h>
+#include <linux/htmm.h>
+
+struct htmm_cooling_arg {
+	/*
+	 * page_is_hot: 0 --> already cooled.
+	 * page_is_hot: 1 --> cold after cooling
+	 * page_is_hot: 2 --> hot after cooling
+	 */
+	int page_is_hot;
+	struct mem_cgroup *memcg;
+};
+
+static bool cooling_page_one(struct page *page, struct vm_area_struct *vma,
+			     unsigned long address, void *arg)
+{
+	struct htmm_cooling_arg *hca = arg;
+	struct page_vma_mapped_walk pvmw = {
+		.page = page,
+		.vma = vma,
+		.address = address,
+	};
+	pginfo_t *pginfo;
+
+	while (page_vma_mapped_walk(&pvmw)) {
+		address = pvmw.address;
+		page = pvmw.page;
+
+		if (pvmw.pte) {
+			struct page *pte_page;
+			unsigned long prev_accessed, cur_idx;
+			unsigned int memcg_cclock;
+			pte_t *pte = pvmw.pte;
+
+			pte_page = virt_to_page((unsigned long)pte);
+			if (!PageHtmm(pte_page))
+				continue;
+
+			pginfo = get_pginfo_from_pte(pte);
+			if (!pginfo)
+				continue;
+
+			spin_lock(&hca->memcg->access_lock);
+			memcg_cclock = READ_ONCE(hca->memcg->cooling_clock);
+			if (memcg_cclock > pginfo->cooling_clock) {
+				unsigned int diff =
+					memcg_cclock - pginfo->cooling_clock;
+				int j;
+
+				prev_accessed = pginfo->total_accesses;
+				pginfo->nr_accesses = 0;
+				for (j = 0; j < diff; j++)
+					pginfo->total_accesses >>= 1;
+
+				cur_idx = get_idx(pginfo->total_accesses);
+				hca->memcg->hotness_hg[cur_idx]++;
+				hca->memcg->ebp_hotness_hg[cur_idx]++;
+
+				if (cur_idx >=
+				    (hca->memcg->active_threshold - 1))
+					hca->page_is_hot = 2;
+				else
+					hca->page_is_hot = 1;
+				if (get_idx(prev_accessed) >=
+				    (hca->memcg->bp_active_threshold))
+					pginfo->may_hot = true;
+				else
+					pginfo->may_hot = false;
+				pginfo->cooling_clock = memcg_cclock;
+			}
+			spin_unlock(&hca->memcg->access_lock);
+		} else if (pvmw.pmd) {
+			/* do nothing */
+			continue;
+		}
+	}
+
+	return true;
+}
+
+/**
+ * cooling_page - cooling page and return true if the page is still hot page
+ */
+int cooling_page(struct page *page, struct mem_cgroup *memcg)
+{
+	struct htmm_cooling_arg hca = {
+		.page_is_hot = 0,
+		.memcg = memcg,
+	};
+	struct rmap_walk_control rwc = {
+		.rmap_one = cooling_page_one,
+		.arg = (void *)&hca,
+	};
+
+	if (!memcg || !memcg->htmm_enabled)
+		return false;
+
+	if (!PageAnon(page) || PageKsm(page))
+		return false;
+
+	if (!page_mapped(page))
+		return false;
+
+	rmap_walk(page, &rwc);
+	return hca.page_is_hot;
+}
+
+static bool page_check_hotness_one(struct page *page,
+				   struct vm_area_struct *vma,
+				   unsigned long address, void *arg)
+{
+	struct htmm_cooling_arg *hca = arg;
+	struct page_vma_mapped_walk pvmw = {
+		.page = page,
+		.vma = vma,
+		.address = address,
+	};
+	pginfo_t *pginfo;
+
+	while (page_vma_mapped_walk(&pvmw)) {
+		address = pvmw.address;
+		page = pvmw.page;
+
+		if (pvmw.pte) {
+			struct page *pte_page;
+			unsigned long cur_idx;
+			pte_t *pte = pvmw.pte;
+
+			pte_page = virt_to_page((unsigned long)pte);
+			if (!PageHtmm(pte_page))
+				continue;
+
+			pginfo = get_pginfo_from_pte(pte);
+			if (!pginfo)
+				continue;
+
+			cur_idx = pginfo->total_accesses;
+			cur_idx = get_idx(cur_idx);
+			if (cur_idx >= hca->memcg->active_threshold)
+				hca->page_is_hot = 2;
+			else
+				hca->page_is_hot = 1;
+		} else if (pvmw.pmd) {
+			/* do nothing */
+			continue;
+		}
+	}
+
+	return true;
+}
+
+int page_check_hotness(struct page *page, struct mem_cgroup *memcg)
+{
+	struct htmm_cooling_arg hca = {
+		.page_is_hot = 0,
+		.memcg = memcg,
+	};
+	struct rmap_walk_control rwc = {
+		.rmap_one = page_check_hotness_one,
+		.arg = (void *)&hca,
+	};
+
+	if (!PageAnon(page) || PageKsm(page))
+		return -1;
+
+	if (!page_mapped(page))
+		return -1;
+
+	rmap_walk(page, &rwc);
+	return hca.page_is_hot;
+}
+
+static bool get_pginfo_idx_one(struct page *page, struct vm_area_struct *vma,
+			       unsigned long address, void *arg)
+{
+	struct htmm_cooling_arg *hca = arg;
+	struct page_vma_mapped_walk pvmw = {
+		.page = page,
+		.vma = vma,
+		.address = address,
+	};
+	pginfo_t *pginfo;
+
+	while (page_vma_mapped_walk(&pvmw)) {
+		address = pvmw.address;
+		page = pvmw.page;
+
+		if (pvmw.pte) {
+			struct page *pte_page;
+			unsigned long cur_idx;
+			pte_t *pte = pvmw.pte;
+
+			pte_page = virt_to_page((unsigned long)pte);
+			if (!PageHtmm(pte_page))
+				continue;
+
+			pginfo = get_pginfo_from_pte(pte);
+			if (!pginfo)
+				continue;
+
+			cur_idx = pginfo->total_accesses;
+			cur_idx = get_idx(cur_idx);
+			hca->page_is_hot = cur_idx;
+		} else if (pvmw.pmd) {
+			hca->page_is_hot = -1;
+		}
+	}
+
+	return true;
+}
+
+int get_pginfo_idx(struct page *page)
+{
+	struct htmm_cooling_arg hca = {
+		.page_is_hot = -1,
+	};
+	struct rmap_walk_control rwc = {
+		.rmap_one = get_pginfo_idx_one,
+		.arg = (void *)&hca,
+	};
+
+	if (!PageAnon(page) || PageKsm(page))
+		return -1;
+
+	if (!page_mapped(page))
+		return -1;
+
+	rmap_walk(page, &rwc);
+	return hca.page_is_hot;
+}
diff --git a/mm/memtis/sampler.c b/mm/memtis/sampler.c
new file mode 100644
index 000000000000..40b9b13811b4
--- /dev/null
+++ b/mm/memtis/sampler.c
@@ -0,0 +1,419 @@
+/*
+ * memory access sampling for hugepage-aware tiered memory management.
+ */
+#include <linux/kthread.h>
+#include <linux/memcontrol.h>
+#include <linux/mempolicy.h>
+#include <linux/sched.h>
+#include <linux/perf_event.h>
+#include <linux/delay.h>
+#include <linux/sched/cputime.h>
+#include <linux/htmm.h>
+
+#include "../../kernel/events/internal.h"
+#include "../profile.h"
+
+struct task_struct *access_sampling = NULL;
+struct perf_event ***mem_event;
+
+static bool valid_va(unsigned long addr)
+{
+	if (!(addr >> (PGDIR_SHIFT + 9)) && addr != 0)
+		return true;
+	else
+		return false;
+}
+
+static __u64 get_pebs_event(enum events e)
+{
+	switch (e) {
+	case DRAMREAD:
+		return DRAM_LLC_LOAD_MISS;
+	case NVMREAD:
+		if (!htmm_cxl_mode)
+			return NVM_LLC_LOAD_MISS;
+		else
+			return N_HTMMEVENTS;
+	case MEMWRITE:
+		return ALL_STORES;
+	case CXLREAD:
+		if (htmm_cxl_mode)
+			return REMOTE_DRAM_LLC_LOAD_MISS;
+		else
+			return N_HTMMEVENTS;
+	default:
+		return N_HTMMEVENTS;
+	}
+}
+
+static int pebs_event_open(__u64 config, __u64 config1, __u64 cpu, __u64 type, __u32 pid)
+{
+	struct perf_event_attr attr = {
+		.type = PERF_TYPE_RAW,
+		.size = sizeof(struct perf_event_attr),
+		.config = config,
+		.config1 = config1,
+		.sample_period = config == ALL_STORES ? htmm_inst_sample_period : get_sample_period(0),
+		.sample_type = PERF_SAMPLE_IP | PERF_SAMPLE_TID | PERF_SAMPLE_ADDR,
+		.disabled = 0,
+		.exclude_kernel = 1,
+		.exclude_hv = 1,
+		.exclude_callchain_kernel = 1,
+		.exclude_callchain_user = 1,
+		// .precise_ip = 1,
+		.precise_ip = 3,
+		// .enable_on_exec = 1,
+		.inherit = 1,
+	};
+	int event_fd = htmm__perf_event_open(&attr, pid ?: -1, cpu, -1, 0);
+	//event_fd = htmm__perf_event_open(&attr, -1, cpu, -1, 0);
+	if (event_fd <= 0) {
+		pr_err("%s: htmm__perf_event_open(config=0x%lx, config1=0x%lx, sample_period=%lu, type=0x%lx) = %d (%s)\n",
+			__func__, config, config1, attr.sample_period, attr.sample_period, event_fd, errname(event_fd)
+		);
+		return -1;
+	}
+
+	if (!fget(event_fd)) {
+		pr_err("invalid file\n");
+		return -1;
+	}
+	mem_event[cpu][type] = fget(event_fd)->private_data;
+	return 0;
+}
+
+static int pebs_init(pid_t pid, int node)
+{
+	int cpu, event;
+
+	mem_event = kzalloc(sizeof(struct perf_event **) * HTMM_CPUS, GFP_KERNEL);
+	for (cpu = 0; cpu < HTMM_CPUS; cpu++) {
+		mem_event[cpu] = kzalloc(sizeof(struct perf_event *) * N_HTMMEVENTS, GFP_KERNEL);
+	}
+
+	pr_info("pebs_init\n");
+	for (cpu = 0; cpu < HTMM_CPUS; cpu++) {
+		for (event = 0; event < N_HTMMEVENTS; event++) {
+			if (get_pebs_event(event) == N_HTMMEVENTS) {
+				mem_event[cpu][event] = NULL;
+				continue;
+			}
+
+			if (pebs_event_open(get_pebs_event(event), 0, cpu, event, pid))
+				return -1;
+			if (htmm__perf_event_init(mem_event[cpu][event], BUFFER_SIZE))
+				return -1;
+		}
+	}
+
+	return 0;
+}
+
+static void pebs_disable(void)
+{
+	int cpu, event;
+
+	pr_info("pebs disable\n");
+	for (cpu = 0; cpu < HTMM_CPUS; cpu++) {
+		for (event = 0; event < N_HTMMEVENTS; event++) {
+			if (mem_event[cpu][event])
+				perf_event_disable(mem_event[cpu][event]);
+		}
+	}
+}
+
+static void pebs_enable(void)
+{
+	int cpu, event;
+
+	pr_info("pebs enable\n");
+	for (cpu = 0; cpu < HTMM_CPUS; cpu++) {
+		for (event = 0; event < N_HTMMEVENTS; event++) {
+			if (mem_event[cpu][event])
+				perf_event_enable(mem_event[cpu][event]);
+		}
+	}
+}
+
+static void pebs_update_period(uint64_t value, uint64_t inst_value)
+{
+	int cpu, event;
+
+	for (cpu = 0; cpu < HTMM_CPUS; cpu++) {
+		for (event = 0; event < N_HTMMEVENTS; event++) {
+			int ret;
+			if (!mem_event[cpu][event])
+				continue;
+
+			switch (event) {
+			case DRAMREAD:
+			case NVMREAD:
+			case CXLREAD:
+				ret = perf_event_period(mem_event[cpu][event], value);
+				break;
+			case MEMWRITE:
+				ret = perf_event_period(mem_event[cpu][event], inst_value);
+				break;
+			default:
+				ret = 0;
+				break;
+			}
+
+			if (ret == -EINVAL)
+				pr_err("failed to update sample period");
+		}
+	}
+}
+
+static int ksamplingd(void *data)
+{
+	unsigned long long nr_sampled = 0, nr_dram = 0, nr_nvm = 0, nr_write = 0;
+	unsigned long long nr_throttled = 0, nr_lost = 0, nr_unknown = 0;
+	unsigned long long nr_skip = 0;
+
+	/* used for calculating average cpu usage of ksampled */
+	struct task_struct *t = current;
+	/* a unit of cputime: permil (1/1000) */
+	u64 total_runtime, exec_runtime, cputime = 0;
+	unsigned long total_cputime, elapsed_cputime, cur;
+	/* used for periodic checks*/
+	unsigned long cpucap_period = msecs_to_jiffies(15000); // 15s
+	unsigned long sample_period = 0;
+	unsigned long sample_inst_period = 0;
+	/* report cpu/period stat */
+	unsigned long trace_cputime,
+		trace_period = msecs_to_jiffies(1500); // 3s
+	unsigned long trace_runtime;
+	/* for timeout */
+	unsigned long sleep_timeout;
+
+	/* for analytic purpose */
+	unsigned long hr_dram = 0, hr_nvm = 0;
+
+	/* orig impl: see read_sum_exec_runtime() */
+	trace_runtime = total_runtime = exec_runtime = t->se.sum_exec_runtime;
+
+	trace_cputime = total_cputime = elapsed_cputime = jiffies;
+	sleep_timeout = usecs_to_jiffies(2000);
+
+	while (!kthread_should_stop()) {
+		guard(vmstat_stopwatch)(SAMPLING_NS);
+		int cpu, event, cond = false;
+
+		if (htmm_mode == HTMM_NO_MIG) {
+			msleep_interruptible(10000);
+			continue;
+		}
+
+		for (cpu = 0; cpu < HTMM_CPUS; cpu++) {
+			for (event = 0; event < N_HTMMEVENTS; event++) {
+				do {
+					struct perf_buffer *rb;
+					struct perf_event_mmap_page *up;
+					struct perf_event_header *ph;
+					struct htmm_event *he;
+					unsigned long pg_index, offset;
+					int page_shift;
+					__u64 head;
+
+					if (!mem_event[cpu][event]) {
+						//continue;
+						break;
+					}
+
+					__sync_synchronize();
+
+					rb = mem_event[cpu][event]->rb;
+					if (!rb) {
+						pr_err("event->rb is NULL\n");
+						return -1;
+					}
+					/* perf_buffer is ring buffer */
+					up = READ_ONCE(rb->user_page);
+					head = READ_ONCE(up->data_head);
+					if (head == up->data_tail) {
+						if (cpu < 16)
+							nr_skip++;
+						//continue;
+						break;
+					}
+
+					head -= up->data_tail;
+					if (head > (BUFFER_SIZE * ksampled_max_sample_ratio / 100)) {
+						cond = true;
+					} else if (head < (BUFFER_SIZE * ksampled_min_sample_ratio / 100)) {
+						cond = false;
+					}
+
+					/* read barrier */
+					smp_rmb();
+
+					page_shift = PAGE_SHIFT + page_order(rb);
+					/* get address of a tail sample */
+					offset = READ_ONCE(up->data_tail);
+					pg_index = (offset >> page_shift) & (rb->nr_pages - 1);
+					offset &= (1 << page_shift) - 1;
+
+					ph = (void *)(rb->data_pages[pg_index] + offset);
+					switch (ph->type) {
+					case PERF_RECORD_SAMPLE:
+						he = (struct htmm_event *)ph;
+						if (!valid_va(he->addr)) {
+							break;
+						}
+
+						update_pginfo(he->pid, he->addr, event);
+						count_vm_event(PEBS_NR_SAMPLED);
+						nr_sampled++;
+
+						if (event == DRAMREAD) {
+							nr_dram++;
+							hr_dram++;
+						} else if (event == CXLREAD || event == NVMREAD) {
+							nr_nvm++;
+							hr_nvm++;
+						} else
+							nr_write++;
+						break;
+					case PERF_RECORD_THROTTLE:
+					case PERF_RECORD_UNTHROTTLE:
+						nr_throttled++;
+						break;
+					case PERF_RECORD_LOST_SAMPLES:
+						nr_lost++;
+						break;
+					default:
+						nr_unknown++;
+						break;
+					}
+					if (nr_sampled % 500000 == 0) {
+						trace_printk(
+							"nr_sampled: %llu, nr_dram: %llu, nr_nvm: %llu, nr_write: %llu, nr_throttled: %llu \n",
+							nr_sampled, nr_dram, nr_nvm, nr_write, nr_throttled);
+						nr_dram = 0;
+						nr_nvm = 0;
+						nr_write = 0;
+					}
+					/* read, write barrier */
+					smp_mb();
+					WRITE_ONCE(up->data_tail, up->data_tail + ph->size);
+				} while (cond);
+			}
+		}
+		/* if ksampled_soft_cpu_quota is zero, disable dynamic pebs feature */
+		if (!ksampled_soft_cpu_quota)
+			continue;
+
+		/* sleep */
+		schedule_timeout_interruptible(sleep_timeout);
+
+		/* check elasped time */
+		cur = jiffies;
+		if ((cur - elapsed_cputime) >= cpucap_period) {
+			u64 cur_runtime = t->se.sum_exec_runtime;
+			exec_runtime = cur_runtime - exec_runtime; //ns
+			elapsed_cputime = jiffies_to_usecs(cur - elapsed_cputime); //us
+			if (!cputime) {
+				u64 cur_cputime = div64_u64(exec_runtime, elapsed_cputime);
+				// EMA with the scale factor (0.2)
+				cputime = ((cur_cputime << 3) + (cputime << 1)) / 10;
+			} else
+				cputime = div64_u64(exec_runtime, elapsed_cputime);
+
+			/* to prevent frequent updates, allow for a slight variation of +/- 0.5% */
+			if (cputime > (ksampled_soft_cpu_quota + 5) && sample_period != ARRAY_SIZE(pebs_period_list)) {
+				/* need to increase the sample period */
+				/* only increase by 1 */
+				unsigned long tmp1 = sample_period, tmp2 = sample_inst_period;
+				increase_sample_period(&sample_period, &sample_inst_period);
+				if (tmp1 != sample_period || tmp2 != sample_inst_period)
+					pebs_update_period(get_sample_period(sample_period),
+							   get_sample_inst_period(sample_inst_period));
+			} else if (cputime < (ksampled_soft_cpu_quota - 5) && sample_period) {
+				unsigned long tmp1 = sample_period, tmp2 = sample_inst_period;
+				decrease_sample_period(&sample_period, &sample_inst_period);
+				if (tmp1 != sample_period || tmp2 != sample_inst_period)
+					pebs_update_period(get_sample_period(sample_period),
+							   get_sample_inst_period(sample_inst_period));
+			}
+			/* does it need to prevent ping-pong behavior? */
+
+			elapsed_cputime = cur;
+			exec_runtime = cur_runtime;
+		}
+
+		/* This is used for reporting the sample period and cputime */
+		if (cur - trace_cputime >= trace_period) {
+			unsigned long hr = 0;
+			u64 cur_runtime = t->se.sum_exec_runtime;
+			trace_runtime = cur_runtime - trace_runtime;
+			trace_cputime = jiffies_to_usecs(cur - trace_cputime);
+			trace_cputime = div64_u64(trace_runtime, trace_cputime);
+
+			if (hr_dram + hr_nvm == 0)
+				hr = 0;
+			else
+				hr = hr_dram * 10000 / (hr_dram + hr_nvm);
+			trace_printk("sample_period: %lu || cputime: %lu  || hit ratio: %lu\n",
+				     get_sample_period(sample_period), trace_cputime, hr);
+
+			hr_dram = hr_nvm = 0;
+			trace_cputime = cur;
+			trace_runtime = cur_runtime;
+		}
+	}
+
+	total_runtime = (t->se.sum_exec_runtime) - total_runtime; // ns
+	total_cputime = jiffies_to_usecs(jiffies - total_cputime); // us
+
+	pr_info("nr_sampled: %llu, nr_throttled: %llu, nr_lost: %llu\n", nr_sampled, nr_throttled, nr_lost);
+	pr_info("total runtime: %llu ns, total cputime: %lu us, cpu usage: %llu\n", total_runtime, total_cputime,
+	       total_runtime / (total_cputime + 1));
+
+	return 0;
+}
+
+static int ksamplingd_run(void)
+{
+	int err = 0;
+
+	if (!access_sampling) {
+		access_sampling = kthread_create(ksamplingd, NULL, "ksamplingd");
+		if (IS_ERR(access_sampling)) {
+			err = PTR_ERR(access_sampling);
+			access_sampling = NULL;
+		}
+		/* TODO implements per-CPU node ksamplingd by using pg_data_t */
+		/* Currently uses a single CPU node(0) */
+		const struct cpumask *cpumask = cpumask_of_node(CPU_NODE);
+		if (!cpumask_empty(cpumask))
+			do_set_cpus_allowed(access_sampling, cpumask);
+		wake_up_process(access_sampling);
+	}
+	return err;
+}
+
+int ksamplingd_init(pid_t pid, int node)
+{
+	int ret;
+
+	if (access_sampling)
+		return 0;
+
+	ret = pebs_init(pid, node);
+	if (ret) {
+		pr_err("htmm__perf_event_init failure... ERROR:%d\n", ret);
+		return 0;
+	}
+
+	return ksamplingd_run();
+}
+
+void ksamplingd_exit(void)
+{
+	pebs_disable();
+	if (access_sampling) {
+		kthread_stop(access_sampling);
+		access_sampling = NULL;
+	}
+}
diff --git a/mm/memtis/sysfs.c b/mm/memtis/sysfs.c
new file mode 100644
index 000000000000..f1c065a5c361
--- /dev/null
+++ b/mm/memtis/sysfs.c
@@ -0,0 +1,576 @@
+#include <linux/kobject.h>
+#include <linux/sysfs.h>
+#include <linux/htmm.h>
+
+/* sysfs htmm */
+unsigned int htmm_sample_period = 199;
+unsigned int htmm_inst_sample_period = 100007;
+unsigned int htmm_thres_hot = 1;
+unsigned int htmm_cooling_period = 2000000;
+unsigned int htmm_adaptation_period = 100000;
+unsigned int htmm_split_period = 2; /* used to shift the wss of memcg */
+unsigned int ksampled_min_sample_ratio = 50; // 50%
+unsigned int ksampled_max_sample_ratio = 10; // 10%
+unsigned int htmm_demotion_period_in_ms = 500;
+unsigned int htmm_promotion_period_in_ms = 500;
+unsigned int htmm_thres_split = 2;
+unsigned int htmm_nowarm = 0; // enabled: 0, disabled: 1
+unsigned int htmm_util_weight = 10; // no impact (unused)
+unsigned int htmm_mode = 1;
+unsigned int htmm_gamma = 4; /* 0.4; divide this by 10 */
+bool htmm_cxl_mode = false;
+bool htmm_skip_cooling = true;
+// unit: 4KiB, default: 10GB
+unsigned int htmm_thres_cooling_alloc = 256 * 1024 * 10;
+unsigned int ksampled_soft_cpu_quota = 30; // 3 %
+
+static ssize_t htmm_sample_period_show(struct kobject *kobj,
+				       struct kobj_attribute *attr, char *buf)
+{
+	return sysfs_emit(buf, "%u\n", htmm_sample_period);
+}
+
+static ssize_t htmm_sample_period_store(struct kobject *kobj,
+					struct kobj_attribute *attr,
+					const char *buf, size_t count)
+{
+	int err;
+	unsigned int period;
+
+	err = kstrtouint(buf, 10, &period);
+	if (err)
+		return err;
+
+	WRITE_ONCE(htmm_sample_period, period);
+	return count;
+}
+
+static struct kobj_attribute htmm_sample_period_attr =
+	__ATTR(htmm_sample_period, 0644, htmm_sample_period_show,
+	       htmm_sample_period_store);
+
+static ssize_t htmm_inst_sample_period_show(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    char *buf)
+{
+	return sysfs_emit(buf, "%u\n", htmm_inst_sample_period);
+}
+
+static ssize_t htmm_inst_sample_period_store(struct kobject *kobj,
+					     struct kobj_attribute *attr,
+					     const char *buf, size_t count)
+{
+	int err;
+	unsigned int period;
+
+	err = kstrtouint(buf, 10, &period);
+	if (err)
+		return err;
+
+	WRITE_ONCE(htmm_inst_sample_period, period);
+	return count;
+}
+
+static struct kobj_attribute htmm_inst_sample_period_attr =
+	__ATTR(htmm_inst_sample_period, 0644, htmm_inst_sample_period_show,
+	       htmm_inst_sample_period_store);
+
+static ssize_t htmm_split_period_show(struct kobject *kobj,
+				      struct kobj_attribute *attr, char *buf)
+{
+	return sysfs_emit(buf, "%u\n", htmm_split_period);
+}
+
+static ssize_t htmm_split_period_store(struct kobject *kobj,
+				       struct kobj_attribute *attr,
+				       const char *buf, size_t count)
+{
+	int err;
+	unsigned int thres;
+
+	err = kstrtouint(buf, 10, &thres);
+	if (err)
+		return err;
+
+	WRITE_ONCE(htmm_split_period, thres);
+	return count;
+}
+
+static struct kobj_attribute htmm_split_period_attr =
+	__ATTR(htmm_split_period, 0644, htmm_split_period_show,
+	       htmm_split_period_store);
+
+static ssize_t htmm_thres_hot_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buf)
+{
+	return sysfs_emit(buf, "%u\n", htmm_thres_hot);
+}
+
+static ssize_t htmm_thres_hot_store(struct kobject *kobj,
+				    struct kobj_attribute *attr,
+				    const char *buf, size_t count)
+{
+	int err;
+	unsigned int thres;
+
+	err = kstrtouint(buf, 10, &thres);
+	if (err)
+		return err;
+
+	WRITE_ONCE(htmm_thres_hot, thres);
+	return count;
+}
+
+static struct kobj_attribute htmm_thres_hot_attr =
+	__ATTR(htmm_thres_hot, 0644, htmm_thres_hot_show, htmm_thres_hot_store);
+
+static ssize_t htmm_cooling_period_show(struct kobject *kobj,
+					struct kobj_attribute *attr, char *buf)
+{
+	return sysfs_emit(buf, "%u\n", htmm_cooling_period);
+}
+
+static ssize_t htmm_cooling_period_store(struct kobject *kobj,
+					 struct kobj_attribute *attr,
+					 const char *buf, size_t count)
+{
+	int err;
+	unsigned int period;
+
+	err = kstrtouint(buf, 10, &period);
+	if (err)
+		return err;
+
+	WRITE_ONCE(htmm_cooling_period, period);
+	return count;
+}
+
+static struct kobj_attribute htmm_cooling_period_attr =
+	__ATTR(htmm_cooling_period, 0644, htmm_cooling_period_show,
+	       htmm_cooling_period_store);
+
+static ssize_t ksampled_min_sample_ratio_show(struct kobject *kobj,
+					      struct kobj_attribute *attr,
+					      char *buf)
+{
+	return sysfs_emit(buf, "%u\n", ksampled_min_sample_ratio);
+}
+
+static ssize_t ksampled_min_sample_ratio_store(struct kobject *kobj,
+					       struct kobj_attribute *attr,
+					       const char *buf, size_t count)
+{
+	int err;
+	unsigned int interval;
+
+	err = kstrtouint(buf, 10, &interval);
+	if (err)
+		return err;
+
+	WRITE_ONCE(ksampled_min_sample_ratio, interval);
+	return count;
+}
+
+static struct kobj_attribute ksampled_min_sample_ratio_attr =
+	__ATTR(ksampled_min_sample_ratio, 0644, ksampled_min_sample_ratio_show,
+	       ksampled_min_sample_ratio_store);
+
+static ssize_t ksampled_max_sample_ratio_show(struct kobject *kobj,
+					      struct kobj_attribute *attr,
+					      char *buf)
+{
+	return sysfs_emit(buf, "%u\n", ksampled_max_sample_ratio);
+}
+
+static ssize_t ksampled_max_sample_ratio_store(struct kobject *kobj,
+					       struct kobj_attribute *attr,
+					       const char *buf, size_t count)
+{
+	int err;
+	unsigned int interval;
+
+	err = kstrtouint(buf, 10, &interval);
+	if (err)
+		return err;
+
+	WRITE_ONCE(ksampled_max_sample_ratio, interval);
+	return count;
+}
+
+static struct kobj_attribute ksampled_max_sample_ratio_attr =
+	__ATTR(ksampled_max_sample_ratio, 0644, ksampled_max_sample_ratio_show,
+	       ksampled_max_sample_ratio_store);
+
+static ssize_t htmm_demotion_period_show(struct kobject *kobj,
+					 struct kobj_attribute *attr, char *buf)
+{
+	return sysfs_emit(buf, "%u\n", htmm_demotion_period_in_ms);
+}
+
+static ssize_t htmm_demotion_period_store(struct kobject *kobj,
+					  struct kobj_attribute *attr,
+					  const char *buf, size_t count)
+{
+	int err;
+	unsigned int thres;
+
+	err = kstrtouint(buf, 10, &thres);
+	if (err)
+		return err;
+
+	WRITE_ONCE(htmm_demotion_period_in_ms, thres);
+	return count;
+}
+
+static struct kobj_attribute htmm_demotion_period_attr =
+	__ATTR(htmm_demotion_period_in_ms, 0644, htmm_demotion_period_show,
+	       htmm_demotion_period_store);
+
+static ssize_t htmm_promotion_period_show(struct kobject *kobj,
+					  struct kobj_attribute *attr,
+					  char *buf)
+{
+	return sysfs_emit(buf, "%u\n", htmm_promotion_period_in_ms);
+}
+
+static ssize_t htmm_promotion_period_store(struct kobject *kobj,
+					   struct kobj_attribute *attr,
+					   const char *buf, size_t count)
+{
+	int err;
+	unsigned int thres;
+
+	err = kstrtouint(buf, 10, &thres);
+	if (err)
+		return err;
+
+	WRITE_ONCE(htmm_promotion_period_in_ms, thres);
+	return count;
+}
+
+static struct kobj_attribute htmm_promotion_period_attr =
+	__ATTR(htmm_promotion_period_in_ms, 0644, htmm_promotion_period_show,
+	       htmm_promotion_period_store);
+
+static ssize_t ksampled_soft_cpu_quota_show(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    char *buf)
+{
+	return sysfs_emit(buf, "%u\n", ksampled_soft_cpu_quota);
+}
+
+static ssize_t ksampled_soft_cpu_quota_store(struct kobject *kobj,
+					     struct kobj_attribute *attr,
+					     const char *buf, size_t count)
+{
+	int err;
+	unsigned int sp_count;
+
+	err = kstrtouint(buf, 10, &sp_count);
+	if (err)
+		return err;
+
+	WRITE_ONCE(ksampled_soft_cpu_quota, sp_count);
+	return count;
+}
+
+static struct kobj_attribute ksampled_soft_cpu_quota_attr =
+	__ATTR(ksampled_soft_cpu_quota, 0644, ksampled_soft_cpu_quota_show,
+	       ksampled_soft_cpu_quota_store);
+
+static ssize_t htmm_thres_split_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buf)
+{
+	return sysfs_emit(buf, "%u\n", htmm_thres_split);
+}
+
+static ssize_t htmm_thres_split_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count)
+{
+	int err;
+	unsigned int thres;
+
+	err = kstrtouint(buf, 10, &thres);
+	if (err)
+		return err;
+
+	WRITE_ONCE(htmm_thres_split, thres);
+	return count;
+}
+
+static struct kobj_attribute htmm_thres_split_attr = __ATTR(
+	htmm_thres_split, 0644, htmm_thres_split_show, htmm_thres_split_store);
+
+static ssize_t htmm_nowarm_show(struct kobject *kobj,
+				struct kobj_attribute *attr, char *buf)
+{
+	return sysfs_emit(buf, "%u\n", htmm_nowarm);
+}
+
+static ssize_t htmm_nowarm_store(struct kobject *kobj,
+				 struct kobj_attribute *attr, const char *buf,
+				 size_t count)
+{
+	int err;
+	unsigned int thres;
+
+	err = kstrtouint(buf, 10, &thres);
+	if (err)
+		return err;
+
+	WRITE_ONCE(htmm_nowarm, thres);
+	return count;
+}
+
+static struct kobj_attribute htmm_nowarm_attr =
+	__ATTR(htmm_nowarm, 0644, htmm_nowarm_show, htmm_nowarm_store);
+
+static ssize_t htmm_adaptation_period_show(struct kobject *kobj,
+					   struct kobj_attribute *attr,
+					   char *buf)
+{
+	return sysfs_emit(buf, "%u\n", htmm_adaptation_period);
+}
+
+static ssize_t htmm_adaptation_period_store(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    const char *buf, size_t count)
+{
+	int err;
+	unsigned int period;
+
+	err = kstrtouint(buf, 10, &period);
+	if (err)
+		return err;
+
+	WRITE_ONCE(htmm_adaptation_period, period);
+	return count;
+}
+
+static struct kobj_attribute htmm_adaptation_period_attr =
+	__ATTR(htmm_adaptation_period, 0644, htmm_adaptation_period_show,
+	       htmm_adaptation_period_store);
+
+static ssize_t htmm_util_weight_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buf)
+{
+	return sysfs_emit(buf, "%u\n", htmm_util_weight);
+}
+
+static ssize_t htmm_util_weight_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count)
+{
+	int err;
+	unsigned int util_w;
+
+	err = kstrtouint(buf, 10, &util_w);
+	if (err)
+		return err;
+
+	WRITE_ONCE(htmm_util_weight, util_w);
+	return count;
+}
+
+static struct kobj_attribute htmm_util_weight_attr = __ATTR(
+	htmm_util_weight, 0644, htmm_util_weight_show, htmm_util_weight_store);
+
+static ssize_t htmm_gamma_show(struct kobject *kobj,
+			       struct kobj_attribute *attr, char *buf)
+{
+	return sysfs_emit(buf, "%u\n", htmm_gamma);
+}
+
+static ssize_t htmm_gamma_store(struct kobject *kobj,
+				struct kobj_attribute *attr, const char *buf,
+				size_t count)
+{
+	int err;
+	unsigned int g;
+
+	err = kstrtouint(buf, 10, &g);
+	if (err)
+		return err;
+
+	WRITE_ONCE(htmm_gamma, g);
+	return count;
+}
+
+static struct kobj_attribute htmm_gamma_attr =
+	__ATTR(htmm_gamma, 0644, htmm_gamma_show, htmm_gamma_store);
+
+static ssize_t htmm_cxl_mode_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf)
+{
+	if (htmm_cxl_mode)
+		return sysfs_emit(buf, "CXL-emulated: %s\n",
+				  "[enabled] disabled");
+	else
+		return sysfs_emit(buf, "CXL-emulated: %s\n",
+				  "enabled [disabled]");
+}
+
+static ssize_t htmm_cxl_mode_store(struct kobject *kobj,
+				   struct kobj_attribute *attr, const char *buf,
+				   size_t count)
+{
+	if (sysfs_streq(buf, "enabled"))
+		htmm_cxl_mode = true;
+	else if (sysfs_streq(buf, "disabled"))
+		htmm_cxl_mode = false;
+	else
+		return -EINVAL;
+
+	return count;
+}
+
+static struct kobj_attribute htmm_cxl_mode_attr =
+	__ATTR(htmm_cxl_mode, 0644, htmm_cxl_mode_show, htmm_cxl_mode_store);
+
+static ssize_t htmm_mode_show(struct kobject *kobj, struct kobj_attribute *attr,
+			      char *buf)
+{
+	char const *msg = NULL;
+	if (htmm_mode == HTMM_NO_MIG)
+		msg = "[NO MIG-0], BASELINE-1, HUGEPAGE_OPT-2, HUGEPAGE_OPT_V2-3";
+	else if (htmm_mode == HTMM_BASELINE)
+		msg = "NO MIG-0, [BASELINE-1], HUGEPAGE_OPT-2, HUGEPAGE_OPT_V2";
+	else if (htmm_mode == HTMM_HUGEPAGE_OPT)
+		msg = "NO MIG-0, BASELINE-1, [HUGEPAGE_OPT-2], HUGEPAGE_OPT_V2-3";
+	else if (htmm_mode == HTMM_HUGEPAGE_OPT_V2)
+		msg = "NO MIG-0, BASELINE-1, HUGEPAGE_OPT-2, [HUGEPAGE_OPT_V2]";
+	else
+		BUG(); // unreachable
+	return sysfs_emit(buf, "%s\n", msg);
+}
+
+static ssize_t htmm_mode_store(struct kobject *kobj,
+			       struct kobj_attribute *attr, const char *buf,
+			       size_t count)
+{
+	int err;
+	unsigned int mode;
+
+	err = kstrtouint(buf, 10, &mode);
+	if (err)
+		return err;
+
+	switch (mode) {
+	case HTMM_NO_MIG:
+	case HTMM_BASELINE:
+	case HTMM_HUGEPAGE_OPT:
+	case HTMM_HUGEPAGE_OPT_V2:
+		WRITE_ONCE(htmm_mode, mode);
+		break;
+	default:
+		return -EINVAL;
+	}
+	return count;
+}
+
+static struct kobj_attribute htmm_mode_attr =
+	__ATTR(htmm_mode, 0644, htmm_mode_show, htmm_mode_store);
+/* sysfs related to newly allocated pages */
+static ssize_t htmm_skip_cooling_show(struct kobject *kobj,
+				      struct kobj_attribute *attr, char *buf)
+{
+	if (htmm_skip_cooling)
+		return sysfs_emit(buf, "[enabled] disabled\n");
+	else
+		return sysfs_emit(buf, "enabled [disabled]\n");
+}
+
+static ssize_t htmm_skip_cooling_store(struct kobject *kobj,
+				       struct kobj_attribute *attr,
+				       const char *buf, size_t count)
+{
+	if (sysfs_streq(buf, "enabled"))
+		htmm_skip_cooling = true;
+	else if (sysfs_streq(buf, "disabled"))
+		htmm_skip_cooling = false;
+	else
+		return -EINVAL;
+
+	return count;
+}
+
+static struct kobj_attribute htmm_skip_cooling_attr =
+	__ATTR(htmm_skip_cooling, 0644, htmm_skip_cooling_show,
+	       htmm_skip_cooling_store);
+
+static ssize_t htmm_thres_cooling_alloc_show(struct kobject *kobj,
+					     struct kobj_attribute *attr,
+					     char *buf)
+{
+	return sysfs_emit(buf, "%u\n", htmm_thres_cooling_alloc);
+}
+
+static ssize_t htmm_thres_cooling_alloc_store(struct kobject *kobj,
+					      struct kobj_attribute *attr,
+					      const char *buf, size_t count)
+{
+	int err;
+	unsigned int thres;
+
+	err = kstrtouint(buf, 10, &thres);
+	if (err)
+		return err;
+
+	WRITE_ONCE(htmm_thres_cooling_alloc, thres);
+	return count;
+}
+
+static struct kobj_attribute htmm_thres_cooling_alloc_attr =
+	__ATTR(htmm_thres_cooling_alloc, 0644, htmm_thres_cooling_alloc_show,
+	       htmm_thres_cooling_alloc_store);
+
+static struct attribute *htmm_attrs[] = {
+	&htmm_sample_period_attr.attr,
+	&htmm_inst_sample_period_attr.attr,
+	&htmm_split_period_attr.attr,
+	&htmm_thres_hot_attr.attr,
+	&htmm_cooling_period_attr.attr,
+	&htmm_adaptation_period_attr.attr,
+	&ksampled_min_sample_ratio_attr.attr,
+	&ksampled_max_sample_ratio_attr.attr,
+	&htmm_demotion_period_attr.attr,
+	&htmm_promotion_period_attr.attr,
+	&ksampled_soft_cpu_quota_attr.attr,
+	&htmm_thres_split_attr.attr,
+	&htmm_nowarm_attr.attr,
+	&htmm_util_weight_attr.attr,
+	&htmm_mode_attr.attr,
+	&htmm_gamma_attr.attr,
+	&htmm_cxl_mode_attr.attr,
+	&htmm_skip_cooling_attr.attr,
+	&htmm_thres_cooling_alloc_attr.attr,
+	NULL,
+};
+
+static const struct attribute_group htmm_attr_group = {
+	.attrs = htmm_attrs,
+};
+
+static int __init htmm_init_sysfs(void)
+{
+	int err;
+	struct kobject *htmm_kobj;
+
+	htmm_kobj = kobject_create_and_add("htmm", mm_kobj);
+	if (!htmm_kobj) {
+		pr_err("failed to create htmm kobject\n");
+		return -ENOMEM;
+	}
+	err = sysfs_create_group(htmm_kobj, &htmm_attr_group);
+	if (err) {
+		pr_err("failed to register numa group\n");
+		goto delete_obj;
+	}
+	return 0;
+
+delete_obj:
+	kobject_put(htmm_kobj);
+	return err;
+}
+subsys_initcall(htmm_init_sysfs);
diff --git a/mm/migrate.c b/mm/migrate.c
index c37af50f312d..f21303d6a688 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -50,6 +50,7 @@
 #include <linux/ptrace.h>
 #include <linux/oom.h>
 #include <linux/memory.h>
+#include <linux/sched/numa_balancing.h>
 
 #include <asm/tlbflush.h>
 
@@ -175,8 +176,15 @@ void putback_movable_pages(struct list_head *l)
 static bool remove_migration_pte(struct page *page, struct vm_area_struct *vma,
 				 unsigned long addr, void *old)
 {
+#ifdef CONFIG_HTMM
+	struct htmm_rmap_walk_arg *rmap_walk_arg = old;
+#endif
 	struct page_vma_mapped_walk pvmw = {
+#ifdef CONFIG_HTMM
+		.page = rmap_walk_arg->arg,
+#else
 		.page = old,
+#endif
 		.vma = vma,
 		.address = addr,
 		.flags = PVMW_SYNC | PVMW_MIGRATION,
@@ -201,7 +209,10 @@ static bool remove_migration_pte(struct page *page, struct vm_area_struct *vma,
 			continue;
 		}
 #endif
-
+#ifdef CONFIG_HTMM
+		if (rmap_walk_arg->unmap_clean && try_to_unmap_clean(&pvmw, new))
+			continue;
+#endif
 		get_page(new);
 		pte = pte_mkold(mk_pte(new, READ_ONCE(vma->vm_page_prot)));
 		if (pte_swp_soft_dirty(*pvmw.pte))
@@ -244,6 +255,15 @@ static bool remove_migration_pte(struct page *page, struct vm_area_struct *vma,
 		} else
 #endif
 		{
+#ifdef CONFIG_NUMA_BALANCING
+			if (page_is_demoted(page) && vma_migratable(vma)) {
+				bool writable = pte_write(pte);
+
+				pte = pte_modify(pte, PAGE_NONE);
+				if (writable)
+					pte = pte_mk_savedwrite(pte);
+			}
+#endif
 			set_pte_at(vma->vm_mm, pvmw.address, pvmw.pte, pte);
 
 			if (PageAnon(new))
@@ -251,6 +271,27 @@ static bool remove_migration_pte(struct page *page, struct vm_area_struct *vma,
 			else
 				page_add_file_rmap(new, false);
 		}
+#ifdef CONFIG_HTMM /* remove_migration_pte() */
+		{
+			struct mem_cgroup *memcg = page_memcg(pvmw.page);
+			struct page *pte_page;
+			pginfo_t *pginfo;
+
+			if (!memcg || !memcg->htmm_enabled)
+				goto out_cooling_check;
+
+			pte_page = virt_to_page((unsigned long)pvmw.pte);
+			if (!PageHtmm(pte_page))
+				goto out_cooling_check;
+
+			pginfo = get_pginfo_from_pte(pvmw.pte);
+			if (!pginfo)
+				goto out_cooling_check;
+
+			check_base_cooling(pginfo, new, true);
+		}
+	out_cooling_check:
+#endif
 		if (vma->vm_flags & VM_LOCKED && !PageTransCompound(new))
 			mlock_vma_page(new);
 
@@ -268,11 +309,22 @@ static bool remove_migration_pte(struct page *page, struct vm_area_struct *vma,
  * Get rid of all migration entries and replace them by
  * references to the indicated page.
  */
-void remove_migration_ptes(struct page *old, struct page *new, bool locked)
+void remove_migration_ptes(struct page *old, struct page *new, bool locked,
+			   bool unmap_clean)
 {
+#ifdef CONFIG_HTMM
+	struct htmm_rmap_walk_arg rmap_walk_arg = {
+		.arg = old,
+		.unmap_clean = unmap_clean,
+	};
+#endif
 	struct rmap_walk_control rwc = {
 		.rmap_one = remove_migration_pte,
+#ifdef CONFIG_HTMM
+		.arg = &rmap_walk_arg,
+#else
 		.arg = old,
+#endif
 	};
 
 	if (locked)
@@ -386,6 +438,9 @@ int migrate_page_move_mapping(struct address_space *mapping,
 	int expected_count = expected_page_refs(mapping, page) + extra_count;
 	int nr = thp_nr_pages(page);
 
+	if (page_count(page) != expected_count)
+		count_vm_events(PGMIGRATE_REFCOUNT_FAIL, thp_nr_pages(page));
+
 	if (!mapping) {
 		/* Anonymous page without mapping */
 		if (page_count(page) != expected_count)
@@ -611,6 +666,10 @@ void migrate_page_states(struct page *newpage, struct page *page)
 		SetPageReadahead(newpage);
 
 	copy_page_owner(page, newpage);
+#ifdef CONFIG_HTMM
+	if (PageTransHuge(page))
+		copy_transhuge_pginfo(page, newpage);
+#endif
 
 	if (!PageHuge(page))
 		mem_cgroup_migrate(page, newpage);
@@ -832,7 +891,7 @@ static int writeout(struct address_space *mapping, struct page *page)
 	 * At this point we know that the migration attempt cannot
 	 * be successful.
 	 */
-	remove_migration_ptes(page, page, false);
+	remove_migration_ptes(page, page, false, false);
 
 	rc = mapping->a_ops->writepage(page, &wbc);
 
@@ -846,8 +905,8 @@ static int writeout(struct address_space *mapping, struct page *page)
 /*
  * Default handling if a filesystem does not provide a migration function.
  */
-static int fallback_migrate_page(struct address_space *mapping,
-	struct page *newpage, struct page *page, enum migrate_mode mode)
+int fallback_migrate_page(struct address_space *mapping, struct page *newpage,
+			  struct page *page, enum migrate_mode mode)
 {
 	if (PageDirty(page)) {
 		/* Only writeback pages in full synchronous migration */
@@ -883,8 +942,8 @@ static int fallback_migrate_page(struct address_space *mapping,
  *   < 0 - error code
  *  MIGRATEPAGE_SUCCESS - success
  */
-static int move_to_new_page(struct page *newpage, struct page *page,
-				enum migrate_mode mode)
+int move_to_new_page(struct page *newpage, struct page *page,
+		     enum migrate_mode mode)
 {
 	struct address_space *mapping;
 	int rc = -EAGAIN;
@@ -1077,8 +1136,9 @@ static int __unmap_and_move(struct page *page, struct page *newpage,
 		rc = move_to_new_page(newpage, page, mode);
 
 	if (page_was_mapped)
-		remove_migration_ptes(page,
-			rc == MIGRATEPAGE_SUCCESS ? newpage : page, false);
+		remove_migration_ptes(
+			page, rc == MIGRATEPAGE_SUCCESS ? newpage : page, false,
+			false);
 
 out_unlock_both:
 	unlock_page(newpage);
@@ -1152,6 +1212,9 @@ static int __unmap_and_move(struct page *page, struct page *newpage,
 static int node_demotion[MAX_NUMNODES] __read_mostly =
 	{[0 ...  MAX_NUMNODES - 1] = NUMA_NO_NODE};
 
+static int node_promotion[MAX_NUMNODES] __read_mostly =
+	{ [0 ... MAX_NUMNODES - 1] = NUMA_NO_NODE };
+
 /**
  * next_demotion_node() - Get the next node in the demotion path
  * @node: The starting node to lookup the next node
@@ -1181,6 +1244,17 @@ int next_demotion_node(int node)
 	return target;
 }
 
+/* can be used when CONFIG_HTMM is set */
+int next_promotion_node(int node)
+{
+	int target;
+
+	rcu_read_lock();
+	target = READ_ONCE(node_promotion[node]);
+	rcu_read_unlock();
+
+	return target;
+}
 /*
  * Obtain the lock on page, remove all ptes and migrate the page
  * to the newly allocated page in newpage.
@@ -1215,6 +1289,10 @@ static int unmap_and_move(new_page_t get_new_page,
 	if (!newpage)
 		return -ENOMEM;
 
+	/* TODO: check whether Ksm pages can be demoted? */
+	if (reason == MR_DEMOTION && !PageKsm(page))
+		set_page_demoted(newpage);
+
 	rc = __unmap_and_move(page, newpage, force, mode);
 	if (rc == MIGRATEPAGE_SUCCESS)
 		set_page_owner_migrate_reason(newpage, reason);
@@ -1280,11 +1358,10 @@ static int unmap_and_move(new_page_t get_new_page,
  * because then pte is replaced with migration swap entry and direct I/O code
  * will wait in the page fault for migration to complete.
  */
-static int unmap_and_move_huge_page(new_page_t get_new_page,
-				free_page_t put_new_page, unsigned long private,
-				struct page *hpage, int force,
-				enum migrate_mode mode, int reason,
-				struct list_head *ret)
+int unmap_and_move_huge_page(new_page_t get_new_page, free_page_t put_new_page,
+			     unsigned long private, struct page *hpage,
+			     int force, enum migrate_mode mode, int reason,
+			     struct list_head *ret)
 {
 	int rc = -EAGAIN;
 	int page_was_mapped = 0;
@@ -1373,8 +1450,9 @@ static int unmap_and_move_huge_page(new_page_t get_new_page,
 		rc = move_to_new_page(new_hpage, hpage, mode);
 
 	if (page_was_mapped)
-		remove_migration_ptes(hpage,
-			rc == MIGRATEPAGE_SUCCESS ? new_hpage : hpage, false);
+		remove_migration_ptes(
+			hpage, rc == MIGRATEPAGE_SUCCESS ? new_hpage : hpage,
+			false, false);
 
 unlock_put_anon:
 	unlock_page(new_hpage);
@@ -1409,8 +1487,8 @@ static int unmap_and_move_huge_page(new_page_t get_new_page,
 	return rc;
 }
 
-static inline int try_split_thp(struct page *page, struct page **page2,
-				struct list_head *from)
+inline int try_split_thp(struct page *page, struct page **page2,
+			 struct list_head *from)
 {
 	int rc = 0;
 
@@ -1549,6 +1627,7 @@ int migrate_pages(struct list_head *from, new_page_t get_new_page,
 					goto out;
 				}
 				nr_failed++;
+				count_vm_events(PGMIGRATE_NOMEM_FAIL, thp_nr_pages(page));
 				goto out;
 			case -EAGAIN:
 				if (is_thp) {
@@ -2136,8 +2215,10 @@ static int numamigrate_isolate_page(pg_data_t *pgdat, struct page *page)
 		return 0;
 
 	/* Avoid migrating to a node that is nearly full */
-	if (!migrate_balanced_pgdat(pgdat, nr_pages))
+	if (!migrate_balanced_pgdat(pgdat, nr_pages)) {
+		count_vm_events(PGMIGRATE_DST_NODE_FULL_FAIL, thp_nr_pages(page));
 		return 0;
+	}
 
 	if (isolate_lru_page(page))
 		return 0;
@@ -2163,9 +2244,11 @@ static int numamigrate_isolate_page(pg_data_t *pgdat, struct page *page)
 int migrate_misplaced_page(struct page *page, struct vm_area_struct *vma,
 			   int node)
 {
+	guard(vmstat_stopwatch)(PROMOTE_NS);
 	pg_data_t *pgdat = NODE_DATA(node);
 	int isolated;
 	int nr_remaining;
+	bool is_file;
 	LIST_HEAD(migratepages);
 	new_page_t *new;
 	bool compound;
@@ -2191,18 +2274,15 @@ int migrate_misplaced_page(struct page *page, struct vm_area_struct *vma,
 	    (vma->vm_flags & VM_EXEC))
 		goto out;
 
-	/*
-	 * Also do not migrate dirty pages as not all filesystems can move
-	 * dirty pages in MIGRATE_ASYNC mode which is a waste of cycles.
-	 */
-	if (page_is_file_lru(page) && PageDirty(page))
-		goto out;
-
 	isolated = numamigrate_isolate_page(pgdat, page);
-	if (!isolated)
+	if (!isolated) {
+		count_vm_events(PGMIGRATE_NUMA_ISOLATE_FAIL, thp_nr_pages(page));
 		goto out;
+	}
 
+	is_file = page_is_file_lru(page);
 	list_add(&page->lru, &migratepages);
+	count_vm_numa_event(PGPROMOTE_TRIED);
 	nr_remaining = migrate_pages(&migratepages, *new, NULL, node,
 				     MIGRATE_ASYNC, MR_NUMA_MISPLACED, NULL);
 	if (nr_remaining) {
@@ -2213,8 +2293,13 @@ int migrate_misplaced_page(struct page *page, struct vm_area_struct *vma,
 			putback_lru_page(page);
 		}
 		isolated = 0;
-	} else
+	} else {
 		count_vm_numa_events(NUMA_PAGE_MIGRATE, nr_pages);
+		if (is_file)
+			count_vm_numa_event(PGPROMOTE_FILE);
+		else
+			count_vm_numa_event(PGPROMOTE_ANON);
+	}
 	BUG_ON(!list_empty(&migratepages));
 	return isolated;
 
@@ -2687,7 +2772,7 @@ static void migrate_vma_unmap(struct migrate_vma *migrate)
 		if (!page || (migrate->src[i] & MIGRATE_PFN_MIGRATE))
 			continue;
 
-		remove_migration_ptes(page, page, false);
+		remove_migration_ptes(page, page, false, false);
 
 		migrate->src[i] = 0;
 		unlock_page(page);
@@ -3065,7 +3150,7 @@ void migrate_vma_finalize(struct migrate_vma *migrate)
 			newpage = page;
 		}
 
-		remove_migration_ptes(page, newpage, false);
+		remove_migration_ptes(page, newpage, false, false);
 		unlock_page(page);
 
 		if (is_zone_device_page(page))
@@ -3091,8 +3176,10 @@ static void __disable_all_migrate_targets(void)
 {
 	int node;
 
-	for_each_online_node(node)
+	for_each_online_node (node) {
 		node_demotion[node] = NUMA_NO_NODE;
+		node_promotion[node] = NUMA_NO_NODE;
+	}
 }
 
 static void disable_all_migrate_targets(void)
@@ -3139,6 +3226,8 @@ static int establish_migrate_target(int node, nodemask_t *used)
 		return NUMA_NO_NODE;
 
 	node_demotion[node] = migration_target;
+	node_promotion[migration_target] = node;
+	pr_info("%s: %d<->%d\n", node, migration_target);
 
 	return migration_target;
 }
diff --git a/mm/mprotect.c b/mm/mprotect.c
index ed18dc49533f..2b1251bb1fce 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -83,6 +83,7 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 			 */
 			if (prot_numa) {
 				struct page *page;
+				int nid;
 
 				/* Avoid TLB flush if possible */
 				if (pte_protnone(oldpte))
@@ -109,7 +110,12 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 				 * Don't mess with PTEs if page is already on the node
 				 * a single-threaded process is running on.
 				 */
-				if (target_node == page_to_nid(page))
+				nid = page_to_nid(page);
+				if (target_node == nid)
+					continue;
+
+				/* skip scanning toptier node */
+				if (numa_promotion_tiered_enabled && node_is_toptier(nid))
 					continue;
 			}
 
diff --git a/mm/nomad/Kconfig b/mm/nomad/Kconfig
new file mode 100644
index 000000000000..ed4abae5f922
--- /dev/null
+++ b/mm/nomad/Kconfig
@@ -0,0 +1,13 @@
+config NOMAD
+	bool "NOMAD from OSDI24"
+	default n
+	depends on NUMA_BALANCING && MIGRATION && X86_64 && !X86_5LEVEL
+	help
+	  Enable NOMAD support.
+
+config NOMAD_MODULE
+        tristate "NOMAD module"
+	default m
+	depends on NOMAD
+	help
+	  Enable NOMAD module.
diff --git a/mm/nomad/Makefile b/mm/nomad/Makefile
new file mode 100644
index 000000000000..536047406bbd
--- /dev/null
+++ b/mm/nomad/Makefile
@@ -0,0 +1,4 @@
+obj-$(CONFIG_NOMAD) += memory.o migrate.o rmap.o
+
+obj-$(CONFIG_NOMAD_MODULE) += async_promote.o
+async_promote-objs := async_promote_main.o buffer_ring.o
diff --git a/mm/nomad/async_promote_main.c b/mm/nomad/async_promote_main.c
new file mode 100644
index 000000000000..73e2b0be162c
--- /dev/null
+++ b/mm/nomad/async_promote_main.c
@@ -0,0 +1,1144 @@
+#include <linux/init.h> /* Needed for the macros */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/page-flags.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/migrate.h>
+#include <linux/workqueue.h>
+#include <linux/nomad.h>
+#include <linux/printk.h> /* Needed for pr_info() */
+#include <linux/types.h>
+#include <linux/fs.h>
+#include <linux/device.h>
+#include <linux/cdev.h>
+#include <linux/rmap.h>
+#include <linux/gfp.h>
+#include <linux/pagemap.h>
+#include <linux/mm_inline.h>
+#include <linux/slab.h>
+#include <linux/pid.h>
+
+#include "../internal.h"
+#include "buffer_ring.h"
+
+#define MAX_TASK 1024
+#define OUTPUT_BUF_LEN 4096
+#define DEVICE_NAME "async_prom"
+#define WORK_DELAY (msecs_to_jiffies(0))
+
+#define PROMOTE_QUEUE_LEN (1ULL << 9)
+
+#define INTERACTION_BUFFER_SIZE (PAGE_SIZE)
+#define BUFFER_ENTRY_NUM (INTERACTION_BUFFER_SIZE / sizeof(uint64_t))
+
+#define PG_FAULT_TASK_CAPACITY 128
+#define PG_FAULT_LOOKBACK_DIST 20
+// __attribute__((optimize("O1")))
+// #define NOMAD_DEBUG
+
+enum module_op {
+	READ_MODULE_INFO,
+	SCAN_VMA,
+	// for debugging purpose
+	SCAN_SHADOWMAPPING,
+	RELEASE_SHADOWPAGE,
+};
+
+struct pg_fault_tasks {
+	struct list_head head;
+	struct page *page;
+	pte_t *ptep;
+	int target_nid;
+};
+
+struct promote_task {
+	struct page *page;
+	unsigned long promote_time;
+	int target_nid;
+};
+
+struct promote_context {
+	unsigned long task_num;
+	// used to protect stop_receiving_requests
+	struct rw_semaphore stop_lock;
+	bool stop_receiving_requests;
+	// this is the queue for the promotion tasks
+	struct ring_queue promotion_queue;
+	// the array that really hold tasks
+	struct promote_task *task_array;
+
+	// variables that will be read from user space, we don't need
+	// it to be accurate, thus no lock needed
+	spinlock_t info_lock;
+	unsigned long success_nr;
+	unsigned long retry_nr;
+	unsigned long transactional_success_nr;
+	unsigned long transactional_fail_nr;
+	unsigned long retreated_page_nr;
+	unsigned long try_to_promote_nr;
+	spinlock_t q_lock;
+};
+// describing print read mod info context
+struct read_mod_info_task {
+	bool printed;
+};
+
+struct read_pagetable_task {
+	pid_t pid;
+	void *progress;
+	struct mm_struct *mm;
+	struct vm_area_struct *vma;
+	uint64_t *u64_buffer;
+};
+
+struct module_io_state {
+	enum module_op op_type;
+	spinlock_t lock;
+
+	struct read_mod_info_task rd_mod_info;
+	struct read_pagetable_task rd_pgtable;
+};
+// used to describe the context of the requests sent to the module
+struct module_request_format {
+	enum module_op op_type;
+	union {
+		struct {
+		} task_read_mod_info;
+		struct {
+			uint64_t pid;
+			void *vaddr;
+		} task_scan_vma;
+		struct {
+		} task_scan_shadowmapping;
+		struct {
+		} task_release_shadowpage;
+	};
+};
+
+struct page_fault_decision_context {
+	struct kmem_cache *fault_task_allocator;
+	// maintain the previously fault pages and we look back in next page fault
+	// to decide whether we should promote the page
+	struct list_head prev_fault_pages;
+	spinlock_t lock;
+};
+
+struct shadow_page_dev {
+	struct xarray page_mapping;
+	spinlock_t lock;
+	uint64_t kv_num;
+	uint64_t batch_free_num;
+	atomic64_t demote_find_counter;
+	atomic64_t demote_breakup_counter;
+	uint64_t link_num;
+	atomic64_t wp_num;
+	atomic64_t cow_num;
+};
+
+static void promote_work_handler(struct work_struct *w);
+/* Prototypes for device functions */
+static int device_open(struct inode *, struct file *);
+static int device_release(struct inode *, struct file *);
+static ssize_t device_read(struct file *, char *, size_t, loff_t *);
+
+static int init_interface(void);
+static void destroy_interface(void);
+
+static enum nomad_stat_async_promotion
+queue_pages_for_promotion(struct page *page, int target_nid);
+
+static struct promote_context context;
+static struct workqueue_struct *wq = NULL;
+static DECLARE_WORK(promote_work, promote_work_handler);
+
+static int major_num;
+static dev_t devno;
+static struct class *cls;
+static struct device *test_device;
+
+static struct page_fault_decision_context fault_decision_context;
+
+static struct shadow_page_dev mapping_dev;
+
+// record the status for how the user program access the module, what we want
+// the module to do and the progress of the access, we assume there is only one
+// process accessing the module.
+
+// make decisions about the page. we look back the past pages. decisions include:
+// 1. should we migrate it?
+// 2. should we set it as accessed?
+// 3. don't do anything
+static int decide_queue_page(struct page *page, pte_t *ptep, int target_nid)
+{
+	struct pg_fault_tasks *new_task, *task, *tmp;
+	int idx = -1, err = 0;
+	if (target_nid < 0) {
+		goto out;
+	}
+	new_task = kmem_cache_alloc(fault_decision_context.fault_task_allocator,
+				    GFP_NOWAIT);
+	if (!new_task) {
+		err = -ENOMEM;
+		goto out;
+	}
+	new_task->page = page;
+	new_task->ptep = ptep;
+	new_task->target_nid = target_nid;
+	spin_lock(&fault_decision_context.lock);
+	list_for_each_entry_safe (
+		task, tmp, &fault_decision_context.prev_fault_pages, head) {
+		pte_t entry;
+		idx++;
+		entry = READ_ONCE(*task->ptep);
+		/**
+		 * 1. if the pte is pointing somewhere else, we give up (delete task)
+		 * 2. if the page is active and recently accessed, we promote (delete task)
+		 * 3. if the page is inactive and recently accessed, we mark it accessed
+		 * 4. if the page is not recently accessed, we do nothing 
+		 * 5. the pages that are beyond look back distance, we free it (delete task)
+		*/
+		if (pte_pfn(entry) != page_to_pfn(task->page)) {
+			list_del(&task->head);
+			kmem_cache_free(
+				fault_decision_context.fault_task_allocator,
+				task);
+			continue;
+		};
+
+		if (pte_young(entry) && PageActive(task->page)) {
+			// queue page for promotion
+			get_page(task->page);
+			queue_pages_for_promotion(task->page, task->target_nid);
+			list_del(&task->head);
+			kmem_cache_free(
+				fault_decision_context.fault_task_allocator,
+				task);
+			continue;
+		}
+
+		if (pte_young(entry) && !PageActive(task->page)) {
+			// remove bit
+			mark_page_accessed(task->page);
+			// TODO(lingfeng): right now we don't change the bit
+			// in case that it's not a page table entry. Maybe we can do that
+			// let's set it as a future work for now
+			// cmpxchg(task->ptep, entry, pte_mkold(entry));
+			continue;
+		}
+		if (idx > PG_FAULT_LOOKBACK_DIST) {
+			list_del(&task->head);
+			kmem_cache_free(
+				fault_decision_context.fault_task_allocator,
+				task);
+		}
+	}
+	list_add(&new_task->head, &fault_decision_context.prev_fault_pages);
+	spin_unlock(&fault_decision_context.lock);
+out:
+	return err;
+}
+
+static struct page *alloc_promote_page(struct page *page, unsigned long node)
+{
+	int nid = (int)node;
+	struct page *newpage;
+
+	newpage = alloc_pages_node(nid,
+				   (GFP_HIGHUSER_MOVABLE | __GFP_THISNODE |
+				    __GFP_NOMEMALLOC | __GFP_NORETRY |
+				    __GFP_NOWARN) &
+					   ~__GFP_RECLAIM,
+				   0);
+
+	return newpage;
+}
+
+static void promote_work_handler(struct work_struct *w)
+{
+	guard(vmstat_stopwatch)(PROMOTE_NS);
+	int i;
+	int promotion_queued_out, err, promote_fail_nr;
+	uint64_t head, next;
+	unsigned int nr_retry_times, nr_succeeded = 0;
+	LIST_HEAD(promote_pages);
+	struct page *prom_node, *tmp;
+
+	struct nomad_context stack_contxt;
+	stack_contxt.transactional_migrate_success_nr = 0;
+	stack_contxt.transactional_migrate_fail_nr = 0;
+
+	promotion_queued_out =
+		ring_queque_consume_begin(&context.promotion_queue,
+					  PROMOTE_QUEUE_LEN, PROMOTE_QUEUE_LEN,
+					  &head, &next, NULL, &context.q_lock);
+
+	if (promotion_queued_out == 0) {
+		goto out;
+	}
+	// isolate pages
+	for (i = 0; i < promotion_queued_out; i++) {
+		struct promote_task *promote_task =
+			context.task_array + (head + i) % PROMOTE_QUEUE_LEN;
+		BUG_ON(!PageAnon(promote_task->page));
+		err = isolate_lru_page(promote_task->page);
+		if (!err) {
+			list_add(&promote_task->page->lru, &promote_pages);
+			// if we successfuly isolated a page, we may safely drop caller's ref count
+			put_page(promote_task->page);
+		}
+	}
+	// anonymous pages for transition should only have two ref counts
+	// TODO(lingfeng): right now we have the corret target nid written in, use that in the future
+	promote_fail_nr =
+		nomad_transit_pages(&promote_pages, alloc_promote_page, NULL, 0,
+				    MIGRATE_ASYNC, MR_NUMA_MISPLACED,
+				    &nr_succeeded, &nr_retry_times,
+				    &stack_contxt);
+
+	// sometimes we still may fail, these could either be failed to get lock
+	// or it's not an anonymous page. The pages that are not moved are also
+	// called successful ones, and are already handled within *_transit_pages()
+	list_for_each_entry_safe (prom_node, tmp, &promote_pages, lru) {
+		BUG_ON(prom_node->lru.next == LIST_POISON1 ||
+		       prom_node->lru.prev == LIST_POISON2);
+		list_del(&prom_node->lru);
+		dec_node_page_state(prom_node,
+				    NR_ISOLATED_ANON +
+					    page_is_file_lru(prom_node));
+		putback_lru_page(prom_node);
+	}
+
+	for (i = 0; i < promotion_queued_out; i++) {
+		struct promote_task *promote_task =
+			context.task_array + (head + i) % PROMOTE_QUEUE_LEN;
+		// Page may be freed after migration, which means all the flags
+		// will be cleared, it's normal that an enqueued page have no promote
+		// flag when queued out
+		TestClearPagePromQueued(promote_task->page);
+	}
+
+	// after copy and remap clear the prom bit
+
+	ring_queque_consume_end(&context.promotion_queue, &head, &next,
+				&context.q_lock);
+	spin_lock(&context.info_lock);
+	context.success_nr += nr_succeeded;
+	context.retry_nr += nr_retry_times;
+	context.transactional_success_nr +=
+		stack_contxt.transactional_migrate_success_nr;
+	context.transactional_fail_nr +=
+		stack_contxt.transactional_migrate_fail_nr;
+	if (promote_fail_nr > 0) {
+		context.retreated_page_nr += promote_fail_nr;
+	}
+	context.try_to_promote_nr += promotion_queued_out;
+	spin_unlock(&context.info_lock);
+out:
+	return;
+}
+
+static enum nomad_stat_async_promotion
+queue_pages_for_promotion(struct page *page, int target_nid)
+{
+	enum nomad_stat_async_promotion ret = NOMAD_ASYNC_QUEUED_FAIL;
+	uint64_t head, next, n;
+	if (!down_read_trylock(&context.stop_lock)) {
+		goto out;
+	}
+	if (context.stop_receiving_requests) {
+		goto context_is_stopped;
+	}
+	if (!trylock_page(page)) {
+		goto context_is_stopped;
+	}
+	if (PagePromQueued(page)) {
+		goto out1;
+	}
+	// TODO(lingfeng): a simple work around
+	if (!PageAnon(page)) {
+		goto out1;
+	}
+
+	// get requests
+	n = ring_queque_produce_begin(&context.promotion_queue,
+				      PROMOTE_QUEUE_LEN, 1, &head, &next, NULL,
+				      &context.q_lock);
+	if (n == 0) {
+		goto out1;
+	}
+	get_page(page);
+	SetPagePromQueued(page);
+	context.task_array[head % PROMOTE_QUEUE_LEN].page = page;
+	context.task_array[head % PROMOTE_QUEUE_LEN].target_nid = target_nid;
+	// page queued success, transfer the ref count to the handler, ref count
+	// must be increased first for thread-safety
+
+	ring_queque_produce_end(&context.promotion_queue, &head, &next,
+				&context.q_lock);
+
+	if (wq && !work_pending(&promote_work)) {
+		queue_work(wq, &promote_work);
+	}
+
+	ret = NOMAD_ASYNC_QUEUED_SUCCESS;
+out1:
+	unlock_page(page);
+
+context_is_stopped:
+	up_read(&context.stop_lock);
+
+out:
+	// caller has already taken a ref count of the page
+	put_page(page);
+	return ret;
+}
+
+// link the old page with the new page
+// called when we promote a page
+// return 0 on success
+static int link_shadow_page(struct page *newpage, struct page *oldpage)
+{
+	int err = 0;
+	spin_lock(&mapping_dev.lock);
+	err = xa_err(xa_store(&mapping_dev.page_mapping, (unsigned long)newpage,
+			      oldpage, GFP_ATOMIC));
+	if (err) {
+		goto out;
+	}
+	mapping_dev.kv_num += 1;
+	BUG_ON(!PageLocked(newpage));
+	// caller has already taken the page lock
+	SetPageShadowed(newpage);
+	get_page(oldpage);
+	mapping_dev.link_num += 1;
+out:
+	spin_unlock(&mapping_dev.lock);
+	return err;
+}
+
+// called when we demote a clean page to its shadow page
+// this function looks for a page's shadow page
+// after usage, the original page and shadow page should
+// breakup with each other
+static struct page *
+demote_shadow_page_find(struct page *page,
+			struct demote_shadow_page_context *contxt)
+{
+	struct page *shadow_page = NULL;
+	contxt->shadow_page = NULL;
+	if (TestClearPageShadowed(page) == 0) {
+		goto out;
+	}
+	if (!contxt) {
+		pr_err("[%s]:[%d], wrong caller!", __FILE__, __LINE__);
+		BUG();
+	}
+	contxt->use_shadow_page = 1;
+	contxt->shadow_page_ref_num += 1;
+	spin_lock(&mapping_dev.lock);
+	shadow_page = xa_erase(&mapping_dev.page_mapping, (unsigned long)page);
+	mapping_dev.kv_num -= 1;
+	BUG_ON(page_mapcount(shadow_page) != 0);
+	BUG_ON(!shadow_page);
+	// A corner case. We may get the shadow page when allocating a new page.
+	// Before we proceed, a new WP fault happens. Fault handler takes the lock and
+	// breaks the shadow relationship. This shadow page may be reclaimed and used for
+	// other purposes. If we use that page in demotion. It's dangerous. Thus, we get
+	// the shadow page when it's allocated. We also get the original page so that
+	// the following write in user space will be guaranteed as a CoW rather than
+	// a page reuse;
+	get_page(shadow_page);
+	get_page(page);
+	contxt->shadow_page = shadow_page;
+	spin_unlock(&mapping_dev.lock);
+
+	atomic64_fetch_inc(&mapping_dev.demote_find_counter);
+out:
+
+	return shadow_page;
+}
+
+// called when we demote a clean page to its shadow page
+// this function break the link between old and new page
+static bool
+demote_shadow_page_breakup(struct page *oldpage,
+			   struct demote_shadow_page_context *contxt)
+{
+	bool ret = false;
+	BUG_ON(!contxt);
+	if (contxt->use_shadow_page == 0) {
+		goto out;
+	}
+	if (PageHuge(oldpage) || PageTransHuge(oldpage)) {
+		pr_err("[%s]:[%d], never expected this to happen", __FILE__,
+		       __LINE__);
+		BUG();
+	}
+
+	if (contxt->made_writable) {
+		// TODO(lingfeng) For now we do a relatively conservative design.
+		// If there are multiple mapping, we still copy the page info.
+		// Maybe it's right to reuse the page content. Check in the future.
+		ret = false;
+	} else {
+		ret = true;
+	}
+	contxt->shadow_page_ref_num -= 1;
+	atomic64_fetch_inc(&mapping_dev.demote_breakup_counter);
+
+	// now the PTEs has already been cleared, there's no way for WP fault happening
+	// at this time, safe to drop page ref count
+	BUG_ON(!contxt->shadow_page);
+	put_page(oldpage);
+	put_page(contxt->shadow_page);
+out:
+	return ret;
+}
+
+// called when writing to a write protected page
+// we release the previous linked page
+static struct page *release_shadow_page(struct page *page, void *private,
+					bool in_fault)
+{
+	struct page *shadow_page = NULL;
+	struct counter_tuple {
+		atomic64_t *wp_counter_ptr;
+		atomic64_t *cow_counter_ptr;
+	};
+	struct counter_tuple *counters = (struct counter_tuple *)private;
+	if (TestClearPageShadowed(page) == 0) {
+		goto out;
+	}
+	if (counters) {
+		counters->wp_counter_ptr = &mapping_dev.wp_num;
+		counters->cow_counter_ptr = &mapping_dev.cow_num;
+	}
+	spin_lock(&mapping_dev.lock);
+	shadow_page = xa_erase(&mapping_dev.page_mapping, (unsigned long)page);
+	mapping_dev.kv_num -= 1;
+	BUG_ON(!shadow_page || page_mapcount(shadow_page) != 0);
+	BUG_ON(PageShadowed(shadow_page));
+	spin_unlock(&mapping_dev.lock);
+
+	if (!in_fault)
+		put_page(shadow_page);
+out:
+	if (!in_fault)
+		return NULL;
+
+	return shadow_page;
+}
+
+// free the shadow pages on node nid and return the number of pages reclaimed
+static unsigned long reclaim_shadow_page(int nid, int nr_to_reclaim)
+{
+	unsigned long pickout_nr = 0;
+	struct page *original_page, *shadow_page, *del, *tmp;
+	struct xarray *xa = &mapping_dev.page_mapping;
+
+	LIST_HEAD(release_list);
+	XA_STATE(xas, xa, 0);
+	if (nid == 0) {
+		// TODO(lingfeng): find a better way to avoid unwanted scanning
+		goto out;
+	}
+	spin_lock(&mapping_dev.lock);
+	rcu_read_lock();
+	xas_for_each (&xas, shadow_page, ULONG_MAX) {
+		original_page = (struct page *)xas.xa_index;
+		if (!trylock_page(original_page)) {
+			goto out0;
+		}
+		if (!trylock_page(shadow_page)) {
+			goto out1;
+		}
+		if (PageLRU(shadow_page) || page_count(shadow_page) != 1 ||
+		    page_mapcount(shadow_page) != 0) {
+			goto out2;
+		};
+		// now we delete index so nobody can use it, we can safely free a page
+		if (TestClearPageShadowed(original_page) != 0) {
+			// only whoever clears the bit has the chance to delete page from XA
+			struct page *tmp_page = xa_erase(xa, xas.xa_index);
+			BUG_ON(tmp_page != shadow_page);
+			mapping_dev.kv_num -= 1;
+			list_add(&shadow_page->lru, &release_list);
+			pickout_nr += 1;
+			mapping_dev.batch_free_num += 1;
+		}
+
+	out2:
+		unlock_page(shadow_page);
+	out1:
+		unlock_page(original_page);
+	out0:
+		if (pickout_nr >= nr_to_reclaim) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	spin_unlock(&mapping_dev.lock);
+
+	list_for_each_entry_safe (del, tmp, &release_list, lru) {
+		list_del(&del->lru);
+		put_page(del);
+	}
+out:
+	return pickout_nr;
+}
+
+static int init_output_context(struct module_io_state *io_state)
+{
+	spin_lock_init(&io_state->lock);
+	io_state->op_type = READ_MODULE_INFO;
+	io_state->rd_mod_info.printed = true;
+	io_state->rd_pgtable.u64_buffer =
+		kvmalloc_array(BUFFER_ENTRY_NUM, sizeof(uint64_t), GFP_KERNEL);
+	if (!io_state->rd_pgtable.u64_buffer) {
+		pr_err("allocate memory for io context");
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+static void destroy_output_context(struct module_io_state *io_state)
+{
+	kfree(io_state->rd_pgtable.u64_buffer);
+}
+
+static ssize_t read_module_status(char *buffer, size_t len, loff_t *offset,
+				  struct module_io_state *io_context)
+{
+	struct {
+		u64 op;
+		u64 success_nr;
+		u64 retry_nr;
+		u64 retreated_page_nr;
+		u64 try_to_promote_nr;
+		u64 task_num;
+		u64 wp_num;
+		u64 link_num;
+		u64 shadow_demote_num;
+		u64 batch_free_num;
+		u64 transactional_migration_success;
+		u64 transactional_migration_fail;
+		u64 xa_mapping_num;
+	} output_buffer;
+	ssize_t ret = 0;
+	if (len < sizeof(output_buffer)) {
+		ret = -EFAULT;
+		goto out;
+	}
+	if (io_context->rd_mod_info.printed == true) {
+		ret = 0;
+		goto out;
+	}
+
+	spin_lock(&context.info_lock);
+	output_buffer.op = READ_MODULE_INFO;
+	output_buffer.success_nr = context.success_nr;
+	output_buffer.retry_nr = context.retry_nr;
+	output_buffer.retreated_page_nr = context.retreated_page_nr;
+	output_buffer.try_to_promote_nr = context.try_to_promote_nr;
+	output_buffer.task_num = context.task_num;
+	output_buffer.wp_num = atomic64_read(&mapping_dev.wp_num);
+	output_buffer.link_num = mapping_dev.link_num;
+	output_buffer.shadow_demote_num =
+		atomic64_read(&mapping_dev.demote_breakup_counter);
+	output_buffer.batch_free_num = mapping_dev.batch_free_num;
+	output_buffer.transactional_migration_success =
+		context.transactional_success_nr;
+	output_buffer.transactional_migration_fail =
+		context.transactional_fail_nr;
+	output_buffer.xa_mapping_num = mapping_dev.kv_num;
+	spin_unlock(&context.info_lock);
+	if (copy_to_user(buffer, &output_buffer, sizeof(output_buffer))) {
+		ret = -EFAULT;
+		goto out;
+	}
+	ret = sizeof(output_buffer);
+	*offset += ret;
+	io_context->rd_mod_info.printed = true;
+out:
+	return ret;
+}
+
+static ssize_t iterate_pagetable(char *buffer, size_t len, loff_t *offset,
+				 struct module_io_state *io_context)
+{
+	struct mm_struct *mm;
+	struct vm_area_struct *vma;
+	uint64_t vaddr, va_begin, va_end;
+	uint64_t *local_buffer = io_context->rd_pgtable.u64_buffer;
+	pgd_t *pgd;
+	p4d_t *p4d;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte;
+	ssize_t ret = -EINVAL;
+	int i = 0;
+
+	mm = io_context->rd_pgtable.mm;
+	vma = io_context->rd_pgtable.vma;
+	if (!vma) {
+		// no suitable VMA found
+		ret = 0;
+		goto out;
+	}
+
+	va_begin = vma->vm_start;
+	va_end = vma->vm_end;
+	if (va_begin > (uint64_t)io_context->rd_pgtable.progress) {
+		vaddr = va_begin;
+	} else {
+		vaddr = (uint64_t)io_context->rd_pgtable.progress;
+	}
+
+	for (; vaddr < va_end && i < BUFFER_ENTRY_NUM;
+	     vaddr += PAGE_SIZE, i += 2) {
+		// page table walk
+		pgd = pgd_offset(mm, vaddr);
+		if (pgd_none(*pgd))
+			continue;
+
+		p4d = p4d_offset(pgd, vaddr);
+		if (p4d_none(*p4d))
+			continue;
+
+		pud = pud_offset(p4d, vaddr);
+		if (pud_none(*pud))
+			continue;
+
+		pmd = pmd_offset(pud, vaddr);
+		if (pmd_none(*pmd))
+			continue;
+
+		pte = pte_offset_kernel(pmd, vaddr);
+		local_buffer[i] = vaddr;
+		local_buffer[i + 1] = (uint64_t)pte->pte;
+	}
+
+	if (copy_to_user(buffer, local_buffer, sizeof(uint64_t) * i)) {
+		ret = -EFAULT;
+	} else {
+		ret = sizeof(uint64_t) * i;
+	}
+	io_context->rd_pgtable.progress = (void *)vaddr;
+	if (vaddr >= va_end) {
+		ret = 0;
+	}
+out:
+	return ret;
+}
+
+static ssize_t check_shadowmapping(char *buffer, size_t len, loff_t *offset,
+				   struct module_io_state *io_context)
+{
+	// trigger a scanning in shadow mapping, don't give anything to
+	// userspace
+	unsigned long index;
+	struct page *key_page, *value_page;
+	xa_for_each (&mapping_dev.page_mapping, index, value_page) {
+		key_page = (struct page *)index;
+		pr_info("k: %px, v: %px", key_page, value_page);
+	}
+	return 0;
+}
+
+static ssize_t device_read(struct file *flip, char *buffer, size_t len,
+			   loff_t *offset)
+{
+	ssize_t ret = -EINVAL;
+	struct module_io_state *io_context =
+		(struct module_io_state *)flip->private_data;
+	spin_lock(&io_context->lock);
+	if (io_context->op_type == READ_MODULE_INFO) {
+		ret = read_module_status(buffer, len, offset, io_context);
+	} else if (io_context->op_type == SCAN_VMA) {
+		ret = iterate_pagetable(buffer, len, offset, io_context);
+	} else if (io_context->op_type == SCAN_SHADOWMAPPING) {
+		ret = check_shadowmapping(buffer, len, offset, io_context);
+	}
+	spin_unlock(&io_context->lock);
+	return ret;
+}
+
+static ssize_t setup_read_mod_task(struct module_request_format *request,
+				   struct module_io_state *io_context)
+{
+	io_context->rd_mod_info.printed = false;
+	return sizeof(*request);
+}
+
+static ssize_t setup_scan_vma_task(struct module_request_format *request,
+				   struct module_io_state *io_context)
+{
+	struct task_struct *task;
+	pid_t pid;
+	ssize_t ret = -EINVAL;
+	struct vm_area_struct *vma;
+	struct read_pagetable_task *pg_tbl_task = &io_context->rd_pgtable;
+	pid = request->task_scan_vma.pid;
+	task = pid_task(find_vpid(pid), PIDTYPE_PID);
+	if (!task) {
+		goto out;
+	}
+	if (!task->mm) {
+		ret = -EINVAL;
+		goto out;
+	}
+	pg_tbl_task->mm = task->mm;
+	if (!task->mm->mmap) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	for (vma = task->mm->mmap; vma; vma = vma->vm_next) {
+		uint64_t va = (uint64_t)request->task_scan_vma.vaddr;
+		if (va < vma->vm_end && va >= vma->vm_start) {
+			pg_tbl_task->progress = (void *)vma->vm_start;
+			pg_tbl_task->vma = vma;
+			break;
+		} else {
+			pg_tbl_task->progress = NULL;
+			pg_tbl_task->vma = NULL;
+		}
+	}
+
+	pg_tbl_task->pid = pid;
+	io_context->op_type = SCAN_VMA;
+	ret = sizeof(*request);
+out:
+	return ret;
+}
+
+static ssize_t
+setup_scan_shadowmapping_task(struct module_request_format *request,
+			      struct module_io_state *io_context)
+{
+	io_context->op_type = SCAN_SHADOWMAPPING;
+	return 0;
+}
+
+static int setup_release_shadowoages(void)
+{
+	reclaim_shadow_page(1, 1000000);
+	return 0;
+}
+
+/* Users use write operation to request data from the module. By default, we read the module info. */
+static ssize_t device_write(struct file *flip, const char *buffer, size_t len,
+			    loff_t *offset)
+{
+	/* This is a read-only device */
+	ssize_t ret = -EINVAL;
+
+	struct module_io_state *io_context =
+		(struct module_io_state *)flip->private_data;
+	struct module_request_format update_module_request;
+
+	spin_lock(&io_context->lock);
+	if (len < sizeof(update_module_request)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (copy_from_user(&update_module_request, buffer,
+			   sizeof(update_module_request))) {
+		ret = -EFAULT;
+		goto out;
+	};
+	if (update_module_request.op_type == READ_MODULE_INFO) {
+		ret = setup_read_mod_task(&update_module_request, io_context);
+	} else if (update_module_request.op_type == SCAN_VMA) {
+		ret = setup_scan_vma_task(&update_module_request, io_context);
+	} else if (update_module_request.op_type == SCAN_SHADOWMAPPING) {
+		ret = setup_scan_shadowmapping_task(&update_module_request,
+						    io_context);
+	} else if (update_module_request.op_type == RELEASE_SHADOWPAGE) {
+		ret = setup_release_shadowoages();
+	}
+
+out:
+	spin_unlock(&io_context->lock);
+	return ret;
+}
+
+/* Called when a process opens our device */
+static int device_open(struct inode *inode, struct file *filep)
+{
+	/* If device is open, return busy */
+	// if (device_open_count) {
+	// 	return -EBUSY;
+	// }
+	// device_open_count++;
+	int ret = 0;
+	struct module_io_state *io_context;
+
+	io_context = kmalloc(sizeof(struct module_io_state), GFP_KERNEL);
+	if (!io_context) {
+		ret = -EINVAL;
+		pr_err("open async promote device");
+		goto out;
+	}
+	if (init_output_context(io_context)) {
+		kfree(io_context);
+		pr_err("initialize a io context");
+		ret = -EINVAL;
+		goto out;
+	};
+	filep->private_data = io_context;
+	try_module_get(THIS_MODULE);
+out:
+	return ret;
+}
+
+// /* Called when a process closes our device */
+static int device_release(struct inode *inode, struct file *filep)
+{
+	/* Decrement the open counter and usage count. Without this, the module would not unload. */
+	// device_open_count--;
+	struct module_io_state *io_context;
+	module_put(THIS_MODULE);
+	if (filep->private_data) {
+		io_context = (struct module_io_state *)filep->private_data;
+		destroy_output_context(io_context);
+	}
+	return 0;
+}
+
+static struct file_operations file_ops = { .owner = THIS_MODULE,
+					   .read = device_read,
+					   .write = device_write,
+					   .open = device_open,
+					   .release = device_release };
+
+static int init_interface(void)
+{
+	major_num = register_chrdev(0, DEVICE_NAME, &file_ops);
+	if (major_num < 0) {
+		pr_err("Could not register device: %d\n", major_num);
+		return -ENODEV;
+	}
+	pr_info("tpp watch module loaded with device major number: %d\n",
+		major_num);
+
+	devno = MKDEV(major_num, 0);
+	cls = class_create(THIS_MODULE, "myclass");
+	if (IS_ERR(cls)) {
+		destroy_interface();
+		return -ENODEV;
+	}
+	test_device = device_create(cls, NULL, devno, NULL, DEVICE_NAME);
+	if (IS_ERR(test_device)) {
+		class_destroy(cls);
+		destroy_interface();
+		return -ENODEV;
+	}
+	return 0;
+}
+
+static void destroy_interface(void)
+{
+	if (test_device) {
+		device_destroy(cls, devno);
+		test_device = NULL;
+	}
+	if (cls) {
+		class_destroy(cls);
+		cls = NULL;
+	}
+	unregister_chrdev(major_num, DEVICE_NAME);
+	major_num = 0;
+}
+
+static void page_fault_task_constructor(void *task)
+{
+	struct pg_fault_tasks *pg_fault_task = (struct pg_fault_tasks *)task;
+	INIT_LIST_HEAD(&pg_fault_task->head);
+	pg_fault_task->page = NULL;
+	pg_fault_task->ptep = NULL;
+	pg_fault_task->target_nid = 0;
+}
+
+static int init_decision_cache(struct page_fault_decision_context *context)
+{
+	int ret = 0;
+	struct kmem_cache *task_cache;
+	INIT_LIST_HEAD(&context->prev_fault_pages);
+	task_cache = kmem_cache_create("page_fault_decision_cache",
+				       PG_FAULT_TASK_CAPACITY,
+				       sizeof(struct pg_fault_tasks), 0,
+				       page_fault_task_constructor);
+	if (!task_cache) {
+		context->fault_task_allocator = NULL;
+		ret = -ENOMEM;
+		pr_err("allocate memory for decision cache");
+		goto out;
+	}
+	context->fault_task_allocator = task_cache;
+	spin_lock_init(&context->lock);
+out:
+	return ret;
+}
+
+static void destroy_decision_cache(struct page_fault_decision_context *context)
+{
+	if (context->fault_task_allocator) {
+		kmem_cache_destroy(context->fault_task_allocator);
+		context->fault_task_allocator = NULL;
+	}
+}
+
+// return 0 on success, negative value when fails
+static int init_promotion_context(struct promote_context *prom_context)
+{
+	int ret = 0;
+	init_rwsem(&prom_context->stop_lock);
+	prom_context->stop_receiving_requests = false;
+	prom_context->task_num = 0;
+	spin_lock_init(&prom_context->info_lock);
+	spin_lock_init(&prom_context->q_lock);
+
+	prom_context->success_nr = 0;
+	prom_context->retreated_page_nr = 0;
+	prom_context->transactional_success_nr = 0;
+	prom_context->transactional_fail_nr = 0;
+	prom_context->retry_nr = 0;
+	prom_context->try_to_promote_nr = 0;
+
+	prom_context->promotion_queue.cons.head = 0;
+	prom_context->promotion_queue.cons.tail = 0;
+	prom_context->promotion_queue.prod.head = 0;
+	prom_context->promotion_queue.prod.tail = 0;
+	prom_context->task_array = kzalloc(
+		sizeof(struct promote_task) * PROMOTE_QUEUE_LEN, GFP_KERNEL);
+	if (!prom_context->task_array) {
+		ret = -ENOMEM;
+		pr_err("allocate memory for promotion context");
+	}
+	pr_info("prom context at 0x%px", prom_context);
+	return ret;
+}
+
+// return 0 on success, negative value when fails
+static void destroy_promotion_context(struct promote_context *prom_context)
+{
+	BUG_ON(prom_context->task_array == NULL);
+	kfree(prom_context->task_array);
+}
+
+static void initialize_shadow_mapping_dev(void)
+{
+	spin_lock_init(&mapping_dev.lock);
+	atomic64_set(&mapping_dev.demote_find_counter, 0);
+	atomic64_set(&mapping_dev.demote_breakup_counter, 0);
+	mapping_dev.link_num = 0;
+	mapping_dev.batch_free_num = 0;
+	mapping_dev.kv_num = 0;
+	atomic64_set(&mapping_dev.wp_num, 0);
+	atomic64_set(&mapping_dev.cow_num, 0);
+}
+
+static int initialize_global_sync_ctrl(void)
+{
+	int ret = 0;
+	initialize_shadow_mapping_dev();
+	// make sure previous initialization work is done.
+	smp_wmb();
+	async_mod_glob_ctrl.queue_page_fault = decide_queue_page;
+	async_mod_glob_ctrl.link_shadow_page = link_shadow_page;
+	async_mod_glob_ctrl.demote_shadow_page_find = demote_shadow_page_find;
+	async_mod_glob_ctrl.demote_shadow_page_breakup =
+		demote_shadow_page_breakup;
+	async_mod_glob_ctrl.release_shadow_page = release_shadow_page;
+	async_mod_glob_ctrl.reclaim_page = reclaim_shadow_page;
+	async_mod_glob_ctrl.initialized = 1;
+	return ret;
+}
+// TODO(lingfeng): How to safely remove function pointers when
+// the module is removed?
+static void destroy_global_sync_ctrl(void)
+{
+	// this may be wrong if we remove the module
+	async_mod_glob_ctrl.queue_page_fault = NULL;
+	async_mod_glob_ctrl.link_shadow_page = NULL;
+	async_mod_glob_ctrl.demote_shadow_page_find = NULL;
+	async_mod_glob_ctrl.demote_shadow_page_breakup = NULL;
+	async_mod_glob_ctrl.release_shadow_page = NULL;
+	async_mod_glob_ctrl.reclaim_page = NULL;
+}
+
+static int __init init_async_promote(void)
+{
+	int ret = 0;
+	ret = init_interface();
+	if (ret) {
+		goto err_init_interface;
+	}
+
+	wq = alloc_workqueue("async_promote", WQ_UNBOUND, 1);
+
+	if (!wq) {
+		pr_err("work queue creation failed");
+		ret = -ENODEV;
+		goto err_init_workqueue;
+	}
+
+	ret = init_decision_cache(&fault_decision_context);
+	if (ret) {
+		goto err_init_decision_cache;
+	}
+
+	ret = init_promotion_context(&context);
+	if (ret) {
+		goto err_init_ptomotion;
+	}
+
+	ret = initialize_global_sync_ctrl();
+	if (ret) {
+		goto err_enable_promotion;
+	}
+	return ret;
+
+err_enable_promotion:
+	destroy_promotion_context(&context);
+err_init_ptomotion:
+	destroy_decision_cache(&fault_decision_context);
+
+err_init_decision_cache:
+	destroy_workqueue(wq);
+	wq = NULL;
+
+err_init_workqueue:
+	destroy_interface();
+
+err_init_interface:
+	return ret;
+}
+
+static void __exit exit_async_promote(void)
+{
+	destroy_global_sync_ctrl();
+
+	destroy_promotion_context(&context);
+
+	destroy_decision_cache(&fault_decision_context);
+
+	if (wq) {
+		// drain the existing tasks
+		promote_work_handler(NULL);
+		destroy_workqueue(wq);
+	}
+	wq = NULL;
+
+	destroy_interface();
+
+	pr_info("aync promote exit\n");
+}
+
+MODULE_AUTHOR("Nobody");
+MODULE_DESCRIPTION("A module that asynchronously promote pages");
+MODULE_LICENSE("GPL");
+module_init(init_async_promote) module_exit(exit_async_promote)
diff --git a/mm/nomad/buffer_ring.c b/mm/nomad/buffer_ring.c
new file mode 100644
index 000000000000..959dc1a1dbc6
--- /dev/null
+++ b/mm/nomad/buffer_ring.c
@@ -0,0 +1,182 @@
+/**
+ * @file buffer_ring.c
+ * @author your name (you@domain.com)
+ * @brief
+ * @version 0.1
+ * @date 2023-02-13
+ * different from normal buffer ring, this one is used in
+ * our file system and we use dax driver to do write operations
+ *  each time, the writable region must not exceed the boundary
+ * of the ring buffer. Therefore a write that crosses the buffer ring
+ * boundary will be split into two.
+ * @copyright Copyright (c) 2023
+ *
+ */
+
+#include "buffer_ring.h"
+
+#include <linux/atomic.h>
+#include <linux/processor.h>
+#include <linux/sched/signal.h>
+#include <linux/sched.h>
+// #include "kernel_func.h"
+
+#define MPLOCKED "lock ; "
+
+#define ROUNDUP(x, y) ((x + ((y)-1)) & ~((y)-1))
+
+static inline int atomic64_cmpset(volatile uint64_t *dst, uint64_t exp,
+				  uint64_t src)
+{
+	uint8_t res;
+
+	asm volatile(MPLOCKED "cmpxchg %[src], %[dst];"
+			      "sete %[res];"
+		     : [res] "=a"(res), /* output */
+		       [dst] "=m"(*dst)
+		     : [src] "r"(src), /* input */
+		       "a"(exp), "m"(*dst)
+		     : "memory"); /* no-clobber list */
+	return res;
+}
+
+static int ring_queue_move_prod_head(struct ring_queue *buffer,
+				     uint32_t capacity, uint32_t n,
+				     uint64_t *old_head, uint64_t *new_head,
+				     uint32_t *free_entries)
+{
+	//   atomic_t *prod_head_ptr = &buffer->prod.head;
+	uint32_t max = n;
+	int success;
+	do {
+		n = max;
+		*old_head = buffer->prod.head;
+		smp_rmb();
+		*free_entries = capacity + buffer->cons.tail - *old_head;
+		if (n > *free_entries)
+			n = *free_entries;
+
+		if (n == 0)
+			return 0;
+		*new_head = *old_head + n;
+		success = atomic64_cmpset(&buffer->prod.head, *old_head,
+					  *new_head);
+		// success = __sync_bool_compare_and_swap()
+	} while (success == 0);
+	return n;
+}
+
+/**
+ * @brief the move cons code not tested
+ *
+ */
+static int ring_queue_move_cons_head(struct ring_queue *buffer,
+				     uint32_t capacity, uint32_t n,
+				     uint64_t *old_head, uint64_t *new_head,
+				     uint32_t *entries)
+{
+	uint32_t max = n;
+	int success;
+	do {
+		n = max;
+		*old_head = buffer->cons.head;
+		smp_rmb();
+
+		*entries = (buffer->prod.tail - *old_head);
+
+		if (n > *entries)
+			n = *entries;
+		if (n == 0)
+			return 0;
+		*new_head = *old_head + n;
+		success = atomic64_cmpset(&buffer->cons.head, *old_head,
+					  *new_head);
+	} while (success == 0);
+	return n;
+}
+
+void wait_until_equal_64(volatile uint64_t *addr, uint64_t expected,
+			 int memorder)
+{
+	spin_until_cond(__atomic_load_n(addr, __ATOMIC_RELAXED) == expected);
+}
+
+void update_tail(struct head_tail *ht, uint64_t old_val, uint64_t new_val,
+		 char enqueue)
+{
+	if (enqueue)
+		smp_wmb();
+	else
+		smp_rmb();
+	wait_until_equal_64(&ht->tail, old_val, __ATOMIC_RELAXED);
+	ht->tail = new_val;
+}
+
+/**
+ * @brief
+ *
+ * @param buffer
+ * @param capacity
+ * @param n
+ * @param head
+ * @param next
+ * @param free_space returns the amount of space after the enqueue operation has
+ * finished
+ * @return uint32_t
+ */
+uint32_t ring_queque_produce_begin(struct ring_queue *buffer, uint32_t capacity,
+				   uint32_t n, uint64_t *head, uint64_t *next,
+				   uint32_t *free_space, spinlock_t *lock)
+{
+	uint32_t free_entries, requested_size;
+	spin_lock(lock);
+	requested_size = ring_queue_move_prod_head(buffer, capacity, n, head,
+						   next, &free_entries);
+	if (free_space) {
+		*free_space = free_entries - requested_size;
+	}
+	spin_unlock(lock);
+	return requested_size;
+};
+
+void ring_queque_produce_end(struct ring_queue *buffer, uint64_t *head,
+			     uint64_t *next, spinlock_t *lock)
+{
+	spin_lock(lock);
+	update_tail(&buffer->prod, *head, *next, 1);
+	spin_unlock(lock);
+};
+
+/**
+ * @brief
+ *
+ * @param buffer
+ * @param capacity
+ * @param n
+ * @param head
+ * @param next
+ * @param available the number of remaining entries after dequeue is finished
+ * @return uint32_t the number of elements requested
+ */
+uint32_t ring_queque_consume_begin(struct ring_queue *buffer, uint32_t capacity,
+				   uint32_t n, uint64_t *head, uint64_t *next,
+				   uint32_t *available, spinlock_t *lock)
+{
+	uint32_t enqueued_entries, requested_size;
+	spin_lock(lock);
+	requested_size = ring_queue_move_cons_head(buffer, capacity, n, head,
+						   next, &enqueued_entries);
+	if (available) {
+		*available = enqueued_entries - requested_size;
+	}
+	spin_unlock(lock);
+	return requested_size;
+};
+
+void ring_queque_consume_end(struct ring_queue *buffer, uint64_t *head,
+			     uint64_t *next, spinlock_t *lock)
+{
+	spin_lock(lock);
+	update_tail(&buffer->cons, *head, *next, 0);
+	spin_unlock(lock);
+};
\ No newline at end of file
diff --git a/mm/nomad/buffer_ring.h b/mm/nomad/buffer_ring.h
new file mode 100644
index 000000000000..8d5a9f73d904
--- /dev/null
+++ b/mm/nomad/buffer_ring.h
@@ -0,0 +1,31 @@
+#ifndef _BUFFER_RING_H
+#define _BUFFER_RING_H
+
+#include <linux/types.h>
+#include <linux/kernel.h>
+struct head_tail {
+	volatile uint64_t head;
+	volatile uint64_t tail;
+};
+
+struct ring_queue {
+	struct head_tail prod;
+	struct head_tail cons;
+};
+
+// Notion: The capacity of the ring queue must be the power of 2!
+uint32_t ring_queque_produce_begin(struct ring_queue *buffer, uint32_t capacity,
+				   uint32_t n, uint64_t *head, uint64_t *next,
+				   uint32_t *free_space, spinlock_t *lock);
+
+void ring_queque_produce_end(struct ring_queue *buffer, uint64_t *head,
+			     uint64_t *next, spinlock_t *lock);
+
+uint32_t ring_queque_consume_begin(struct ring_queue *buffer, uint32_t capacity,
+				   uint32_t n, uint64_t *head, uint64_t *next,
+				   uint32_t *available, spinlock_t *lock);
+
+void ring_queque_consume_end(struct ring_queue *buffer, uint64_t *head,
+			     uint64_t *next, spinlock_t *lock);
+
+#endif
\ No newline at end of file
diff --git a/mm/nomad/memory.c b/mm/nomad/memory.c
new file mode 100644
index 000000000000..8c0898d43998
--- /dev/null
+++ b/mm/nomad/memory.c
@@ -0,0 +1,13 @@
+#include <linux/nomad.h>
+
+struct async_promote_ctrl async_mod_glob_ctrl = {
+	.initialized = 0,
+	.queue_page_fault = NULL,
+
+	.link_shadow_page = NULL,
+	.demote_shadow_page_find = NULL,
+	.demote_shadow_page_breakup = NULL,
+	.release_shadow_page = NULL,
+	.reclaim_page = NULL,
+};
+EXPORT_SYMBOL(async_mod_glob_ctrl);
diff --git a/mm/nomad/migrate.c b/mm/nomad/migrate.c
new file mode 100644
index 000000000000..47f7930fcf1d
--- /dev/null
+++ b/mm/nomad/migrate.c
@@ -0,0 +1,1383 @@
+#include <linux/mm.h>
+#include <linux/mm_inline.h>
+#include <linux/swap.h>
+#include <linux/swapops.h>
+#include <linux/rmap.h>
+#include <linux/hugetlb.h>
+#include <linux/migrate.h>
+#include <linux/buffer_head.h>
+#include <linux/page_owner.h>
+#include <linux/sched/numa_balancing.h>
+#include <linux/nomad.h>
+
+#include <trace/events/migrate.h>
+
+#include "../internal.h"
+
+extern int move_to_new_page(struct page *newpage, struct page *page,
+				enum migrate_mode mode);
+extern int fallback_migrate_page(struct address_space *mapping,
+	struct page *newpage, struct page *page, enum migrate_mode mode);
+extern int unmap_and_move_huge_page(new_page_t get_new_page,
+				free_page_t put_new_page, unsigned long private,
+				struct page *hpage, int force,
+				enum migrate_mode mode, int reason,
+				struct list_head *ret);
+extern inline int try_split_thp(struct page *page, struct page **page2,
+				struct list_head *from);
+
+static bool demotion_remove_migration_pte(struct page *page,
+					  struct vm_area_struct *vma,
+					  unsigned long addr, void *arg)
+{
+	struct demote_shadow_page_context *contxt =
+		(struct demote_shadow_page_context *)arg;
+	struct page_vma_mapped_walk pvmw = {
+		.page = contxt->old_page,
+		.vma = vma,
+		.address = addr,
+		.flags = PVMW_SYNC | PVMW_MIGRATION,
+	};
+	struct page *new;
+	pte_t pte;
+	swp_entry_t entry;
+
+	VM_BUG_ON_PAGE(PageTail(page), page);
+	while (page_vma_mapped_walk(&pvmw)) {
+		if (PageKsm(page))
+			new = page;
+		else
+			new = page - pvmw.page->index +
+				linear_page_index(vma, pvmw.address);
+
+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION
+		/* PMD-mapped THP migration entry */
+		if (!pvmw.pte) {
+			VM_BUG_ON_PAGE(PageHuge(page) || !PageTransCompound(page), page);
+			remove_migration_pmd(&pvmw, new);
+			continue;
+		}
+#endif
+
+		get_page(new);
+		pte = pte_mkold(mk_pte(new, READ_ONCE(vma->vm_page_prot)));
+		if (pte_swp_soft_dirty(*pvmw.pte))
+			pte = pte_mksoft_dirty(pte);
+
+		/*
+		 * Recheck VMA as permissions can change since migration started
+		 */
+		entry = pte_to_swp_entry(*pvmw.pte);
+		if (is_writable_migration_entry(entry))
+			pte = maybe_mkwrite(pte, vma);
+		else if (pte_swp_uffd_wp(*pvmw.pte))
+			pte = pte_mkuffd_wp(pte);
+
+		if (unlikely(is_device_private_page(new))) {
+			entry = make_readable_device_private_entry(swp_offset(entry));
+			pte = swp_entry_to_pte(entry);
+			if (pte_swp_soft_dirty(*pvmw.pte))
+				pte = pte_swp_mksoft_dirty(pte);
+			if (pte_swp_uffd_wp(*pvmw.pte))
+				pte = pte_swp_mkuffd_wp(pte);
+		}
+
+#ifdef CONFIG_HUGETLB_PAGE
+		if (PageHuge(new)) {
+			unsigned int shift = huge_page_shift(hstate_vma(vma));
+			pte = pte_mkhuge(pte);
+			pte = arch_make_huge_pte(pte, shift, vma->vm_flags);
+			set_huge_pte_at(vma->vm_mm, pvmw.address, pvmw.pte, pte);
+			if (PageAnon(new))
+				hugepage_add_anon_rmap(new, vma, pvmw.address);
+			else
+				page_dup_rmap(new, true);
+		} else
+#endif
+		{
+#ifdef CONFIG_NUMA_BALANCING
+			if (page_is_demoted(page) && vma_migratable(vma)) {
+				bool writable = pte_write(pte);
+
+				pte = pte_modify(pte, PAGE_NONE);
+				if (writable ||
+				    contxt->was_writable_before_shadowed)
+					pte = pte_mk_savedwrite(pte);
+			}
+#endif
+			set_pte_at(vma->vm_mm, pvmw.address, pvmw.pte, pte);
+
+			if (PageAnon(new))
+				page_add_anon_rmap(new, vma, pvmw.address, false);
+			else
+				page_add_file_rmap(new, false);
+		}
+		if (vma->vm_flags & VM_LOCKED && !PageTransCompound(new))
+			mlock_vma_page(new);
+
+		if (PageTransHuge(page) && PageMlocked(page))
+			clear_page_mlock(page);
+
+		/* No need to invalidate - it was non-present before */
+		update_mmu_cache(vma, pvmw.address, pvmw.pte);
+	}
+
+	return true;
+}
+
+void demotion_remove_migration_ptes(struct demote_shadow_page_context *contxt, struct page *new, bool locked)
+{
+	struct rmap_walk_control rwc = {
+		.rmap_one = demotion_remove_migration_pte,
+		.arg = contxt,
+	};
+
+	if (locked)
+		rmap_walk_locked(new, &rwc);
+	else
+		rmap_walk(new, &rwc);
+}
+
+
+bool enable_copy_and_remap = true;
+
+static void nomad_copy_page(struct page *page, struct page *newpage)
+{
+	BUG_ON(PageWriteback(page)); /* Writeback must be complete */
+
+	migrate_page_copy(newpage, page);
+}
+
+// first round set the page table entries as clean, after that we move the pages
+// finally check if the page table entries are dirty
+static bool nomad_clean_pte(struct page *page, struct vm_area_struct *vma,
+				 unsigned long addr, void *arg)
+{
+
+	struct page_vma_mapped_walk pvmw = {
+		.page = page,
+		.vma = vma,
+		.address = addr,
+		// we cannot have migrate flag, because rmap walker assert
+		// the pte is a migration entry with migration flag while it's not
+		.flags = PVMW_SYNC,
+	};
+	VM_BUG_ON_PAGE(PageTail(page), page);
+	while (page_vma_mapped_walk(&pvmw)) {
+		pte_t pte_val;
+		// assert page tabe entry has value
+		BUG_ON(!pvmw.pte);
+		pte_val = *pvmw.pte;
+		if (pte_dirty(pte_val)) {
+			SetPageDirty(page);
+			flush_cache_page(vma, addr, pte_pfn(*pvmw.pte));
+			pte_val = ptep_clear_flush(vma, addr, pvmw.pte);
+			// pte_val = pte_wrprotect(pte_val);
+			pte_val = pte_mkclean(pte_val);
+			set_pte_at(vma->vm_mm, addr, pvmw.pte, pte_val);
+		}
+	};
+	return true;
+}
+
+static int page_not_mapped(struct page *page)
+{
+	return !page_mapped(page);
+}
+
+/**
+ * @brief make preparations for the page duplication, we don't do
+ * anything with the page table entry, but we need to lock the page
+ * until all the duplication is over.
+ * 
+ * @param page 
+ * @param newpage 
+ * @param contxt 
+ * @return int return 0 on success, non-zero value otherwise
+ */
+static int nomad_copy_and_remap_page(struct page *page,
+					  struct page *newpage,
+					  struct nomad_context *contxt)
+{
+	int ret = MIGRATEPAGE_SUCCESS;
+	unsigned long old_flags;
+	struct address_space *mapping;
+	struct nomad_remap_status remap_arg = {
+		.flags = TTU_IGNORE_MLOCK,
+		.use_new_page = true,
+		.new_page = newpage,
+	};
+
+	struct rmap_walk_control rwc = {
+		.arg = NULL,
+		.rmap_one = nomad_clean_pte,
+		.done = page_not_mapped,
+		.anon_lock = page_lock_anon_vma_read,
+	};
+
+	mapping = page_mapping(page);
+
+	BUG_ON(mapping != NULL);
+
+	// TODO(lingfeng): Now we only have one reference, we temporarily keep
+	// the reference to the page and later modify that. Solve this for
+	// shared pages.
+	if (page_mapcount(page) != 1) {
+		ret = -EAGAIN;
+		goto out;
+	};
+
+	rmap_walk(page, &rwc);
+	old_flags = READ_ONCE(page->flags);
+	nomad_copy_page(page, newpage);
+	// Since we do a copy and remapping, for an anonymous page, its ref count should be 2.
+	// this page is still mapped at the moment.
+
+	// TODO(lingfeng): metadata may be dirty during remapping, fix this later
+	ret = migrate_page_move_mapping(mapping, newpage, page, 1);
+	if (!ret) {
+		bool unmapped_original_page =
+			nomad_try_to_remap(page, &remap_arg);
+		contxt->transactional_migrate_success_nr += 1;
+		BUG_ON(unmapped_original_page != remap_arg.use_new_page);
+	} else if (ret == -EAGAIN) {
+		contxt->transactional_migrate_fail_nr += 1;
+		WRITE_ONCE(page->flags, old_flags);
+		goto out;
+	} else {
+		BUG();
+	}
+
+	if (remap_arg.use_new_page) {
+		ret = MIGRATEPAGE_SUCCESS;
+	} else {
+		WRITE_ONCE(page->flags, old_flags);
+		ret = -EAGAIN;
+	}
+
+out:
+	return ret;
+}
+
+int nomad_migrate_page(struct address_space *mapping,
+		struct page *newpage, struct page *page,
+		enum migrate_mode mode)
+{
+	int rc;
+
+	BUG_ON(PageWriteback(page));	/* Writeback must be complete */
+
+	if (mode != MIGRATE_SYNC_NO_COPY)
+		migrate_page_copy(newpage, page);
+	else
+		migrate_page_states(newpage, page);
+	rc = migrate_page_move_mapping(mapping, newpage, page, 0);
+	if (rc != MIGRATEPAGE_SUCCESS)
+		return rc;
+
+	return MIGRATEPAGE_SUCCESS;
+}
+
+int nomad_move_to_new_page(struct page *newpage, struct page *page,
+				enum migrate_mode mode)
+{
+	struct address_space *mapping;
+	int rc = -EAGAIN;
+	bool is_lru = !__PageMovable(page);
+
+	VM_BUG_ON_PAGE(!PageLocked(page), page);
+	VM_BUG_ON_PAGE(!PageLocked(newpage), newpage);
+
+	mapping = page_mapping(page);
+
+	if (likely(is_lru)) {
+		if (!mapping)
+			rc = nomad_migrate_page(mapping, newpage, page, mode);
+		else if (mapping->a_ops->migratepage)
+			/*
+			 * Most pages have a mapping and most filesystems
+			 * provide a migratepage callback. Anonymous pages
+			 * are part of swap space which also has its own
+			 * migratepage callback. This is the most common path
+			 * for page migration.
+			 */
+			rc = mapping->a_ops->migratepage(mapping, newpage,
+							page, mode);
+		else
+			rc = fallback_migrate_page(mapping, newpage,
+							page, mode);
+	} else {
+		/*
+		 * In case of non-lru page, it could be released after
+		 * isolation step. In that case, we shouldn't try migration.
+		 */
+		VM_BUG_ON_PAGE(!PageIsolated(page), page);
+		if (!PageMovable(page)) {
+			rc = MIGRATEPAGE_SUCCESS;
+			__ClearPageIsolated(page);
+			goto out;
+		}
+
+		rc = mapping->a_ops->migratepage(mapping, newpage,
+						page, mode);
+		WARN_ON_ONCE(rc == MIGRATEPAGE_SUCCESS &&
+			!PageIsolated(page));
+	}
+
+	/*
+	 * When successful, old pagecache page->mapping must be cleared before
+	 * page is freed; but stats require that PageAnon be left as PageAnon.
+	 */
+	if (rc == MIGRATEPAGE_SUCCESS) {
+		if (__PageMovable(page)) {
+			VM_BUG_ON_PAGE(!PageIsolated(page), page);
+
+			/*
+			 * We clear PG_movable under page_lock so any compactor
+			 * cannot try to migrate this page.
+			 */
+			__ClearPageIsolated(page);
+		}
+
+		/*
+		 * Anonymous and movable page->mapping will be cleared by
+		 * free_pages_prepare so don't reset it here for keeping
+		 * the type to work PageAnon, for example.
+		 */
+		if (!PageMappingFlags(page))
+			page->mapping = NULL;
+
+		if (likely(!is_zone_device_page(newpage)))
+			flush_dcache_page(newpage);
+
+	}
+out:
+	return rc;
+}
+
+static int __nomad_copy_and_remap(struct page *page, struct page *newpage,
+				       int force, enum migrate_mode mode,
+				       bool *go_cpy_remap,
+				       struct nomad_context *contxt)
+{
+	int rc = -EAGAIN;
+	int page_was_mapped = 0;
+	struct anon_vma *anon_vma = NULL;
+	bool is_lru = !__PageMovable(page);
+	bool copy_and_remap = false;
+	*go_cpy_remap = false;
+
+	if (!trylock_page(page)) {
+		if (!force || mode == MIGRATE_ASYNC)
+			goto out;
+		// we never expect this to happen
+		BUG();
+		/*
+		 * It's not safe for direct compaction to call lock_page.
+		 * For example, during page readahead pages are added locked
+		 * to the LRU. Later, when the IO completes the pages are
+		 * marked uptodate and unlocked. However, the queueing
+		 * could be merging multiple pages for one bio (e.g.
+		 * mpage_readahead). If an allocation happens for the
+		 * second or third page, the process can end up locking
+		 * the same page twice and deadlocking. Rather than
+		 * trying to be clever about what pages can be locked,
+		 * avoid the use of lock_page for direct compaction
+		 * altogether.
+		 */
+		if (current->flags & PF_MEMALLOC)
+			goto out;
+
+		lock_page(page);
+	}
+
+	if (PageWriteback(page)) {
+		/*
+		 * Only in the case of a full synchronous migration is it
+		 * necessary to wait for PageWriteback. In the async case,
+		 * the retry loop is too short and in the sync-light case,
+		 * the overhead of stalling is too much
+		 */
+		switch (mode) {
+		case MIGRATE_SYNC:
+		case MIGRATE_SYNC_NO_COPY:
+			break;
+		default:
+			rc = -EBUSY;
+			goto out_unlock;
+		}
+		if (!force)
+			goto out_unlock;
+		wait_on_page_writeback(page);
+	}
+
+	/*
+	 * By try_to_unmap(), page->mapcount goes down to 0 here. In this case,
+	 * we cannot notice that anon_vma is freed while we migrates a page.
+	 * This get_anon_vma() delays freeing anon_vma pointer until the end
+	 * of migration. File cache pages are no problem because of page_lock()
+	 * File Caches may use write_page() or lock_page() in migration, then,
+	 * just care Anon page here.
+	 *
+	 * Only page_get_anon_vma() understands the subtleties of
+	 * getting a hold on an anon_vma from outside one of its mms.
+	 * But if we cannot get anon_vma, then we won't need it anyway,
+	 * because that implies that the anon page is no longer mapped
+	 * (and cannot be remapped so long as we hold the page lock).
+	 */
+	if (PageAnon(page) && !PageKsm(page))
+		anon_vma = page_get_anon_vma(page);
+
+	/*
+	 * Block others from accessing the new page when we get around to
+	 * establishing additional references. We are usually the only one
+	 * holding a reference to newpage at this point. We used to have a BUG
+	 * here if trylock_page(newpage) fails, but would like to allow for
+	 * cases where there might be a race with the previous use of newpage.
+	 * This is much like races on refcount of oldpage: just don't BUG().
+	 */
+	if (unlikely(!trylock_page(newpage))) {
+		// we never expect this to happen
+		BUG();
+		goto out_unlock;
+	}
+
+	if (unlikely(!is_lru)) {
+		// we never expect this to happen
+		BUG();
+		rc = nomad_move_to_new_page(newpage, page, mode);
+		goto out_unlock_both;
+	}
+	// TODO(lingfeng): only work on anonymous page that only has 2 reference
+	// one comes from allocation, another comes from mapping
+	copy_and_remap = (page_mapcount(page) == 1) && PageAnon(page) &&
+			 enable_copy_and_remap;
+
+	/*
+	 * Corner case handling:
+	 * 1. When a new swap-cache page is read into, it is added to the LRU
+	 * and treated as swapcache but it has no rmap yet.
+	 * Calling try_to_unmap() against a page->mapping==NULL page will
+	 * trigger a BUG.  So handle it here.
+	 * 2. An orphaned page (see truncate_cleanup_page) might have
+	 * fs-private metadata. The page can be picked up due to memory
+	 * offlining.  Everywhere else except page reclaim, the page is
+	 * invisible to the vm, so the page can not be migrated.  So try to
+	 * free the metadata, so the page can be freed.
+	 */
+	if (!page->mapping) {
+		pr_info("[%s:%d], this is not expected", __FILE__, __LINE__);
+		VM_BUG_ON_PAGE(PageAnon(page), page);
+		// we never expect this to happen
+		BUG();
+		if (page_has_private(page)) {
+			try_to_free_buffers(page);
+			goto out_unlock_both;
+		}
+	} else if (page_mapped(page)) {
+		/* Establish migration ptes */
+		VM_BUG_ON_PAGE(PageAnon(page) && !PageKsm(page) && !anon_vma,
+			       page);
+		if (copy_and_remap) {
+			rc = nomad_copy_and_remap_page(page, newpage,
+							    contxt);
+		} else {
+			try_to_unmap(page, 0);
+
+			page_was_mapped = 1;
+		}
+	}
+
+	if (copy_and_remap) {
+		int tmp_err = 0;
+		*go_cpy_remap = true;
+
+		if (rc == MIGRATEPAGE_SUCCESS &&
+		    async_mod_glob_ctrl.initialized &&
+		    async_mod_glob_ctrl.link_shadow_page) {
+			tmp_err = async_mod_glob_ctrl.link_shadow_page(newpage,
+								       page);
+		}
+
+		BUG_ON(tmp_err);
+	} else {
+		if (!page_mapped(page))
+			rc = move_to_new_page(newpage, page, mode);
+
+		if (page_was_mapped)
+			remove_migration_ptes(
+				page,
+				rc == MIGRATEPAGE_SUCCESS ? newpage : page,
+				false, false);
+	}
+
+out_unlock_both:
+	unlock_page(newpage);
+out_unlock:
+	/* Drop an anon_vma reference if we took one */
+	if (anon_vma)
+		put_anon_vma(anon_vma);
+	unlock_page(page);
+out:
+	/*
+	 * If migration is successful, decrease refcount of the newpage
+	 * which will not free the page because new page owner increased
+	 * refcounter. As well, if it is LRU page, add the page to LRU
+	 * list in here. Use the old state of the isolated source page to
+	 * determine if we migrated a LRU page. newpage was already unlocked
+	 * and possibly modified by its owner - don't rely on the page
+	 * state.
+	 */
+	if (copy_and_remap) {
+		if (rc == MIGRATEPAGE_SUCCESS) {
+			if (unlikely(!is_lru))
+				put_page(newpage);
+			else
+				putback_lru_page(newpage);
+		} else if (rc == -EAGAIN) {
+		} else {
+			BUG();
+		}
+	} else {
+		// original execution in __unmap_and_move
+		if (rc == MIGRATEPAGE_SUCCESS) {
+			if (unlikely(!is_lru))
+				put_page(newpage);
+			else
+				putback_lru_page(newpage);
+		}
+	}
+
+	return rc;
+}
+
+static int nomad_unmap_and_move(new_page_t get_new_page,
+				     free_page_t put_new_page,
+				     unsigned long private, struct page *page,
+				     int force, enum migrate_mode mode,
+				     enum migrate_reason reason,
+				     struct list_head *ret,
+				     struct nomad_context *contxt)
+{
+	int rc = MIGRATEPAGE_SUCCESS;
+	struct page *newpage = NULL;
+	bool go_copy_and_remap;
+
+	if (!thp_migration_supported() && PageTransHuge(page))
+		return -ENOSYS;
+
+	if (page_count(page) == 1) {
+		/* page was freed from under us. So we are done. */
+		ClearPageActive(page);
+		ClearPageUnevictable(page);
+		if (unlikely(__PageMovable(page))) {
+			lock_page(page);
+			if (!PageMovable(page))
+				__ClearPageIsolated(page);
+			unlock_page(page);
+		}
+		goto out;
+	}
+
+	newpage = get_new_page(page, private);
+	if (!newpage)
+		return -ENOMEM;
+
+	/* TODO: check whether Ksm pages can be demoted? */
+	if (reason == MR_DEMOTION && !PageKsm(page))
+		set_page_demoted(newpage);
+
+	rc = __nomad_copy_and_remap(page, newpage, force, mode,
+					 &go_copy_and_remap, contxt);
+	if (!go_copy_and_remap && rc == MIGRATEPAGE_SUCCESS)
+		set_page_owner_migrate_reason(newpage, reason);
+	if (!go_copy_and_remap) {
+		goto out;
+	}
+
+	if (rc != -EAGAIN) {
+		/*
+		 * A page that has been migrated has all references
+		 * removed and will be freed. A page that has not been
+		 * migrated will have kept its references and be restored.
+		 */
+		list_del(&page->lru);
+	}
+
+	/*
+	 * If migration is successful, releases reference grabbed during
+	 * isolation. Otherwise, restore the page to right list unless
+	 * we want to retry.
+	 */
+	if (rc == MIGRATEPAGE_SUCCESS) {
+		/*
+		 * Compaction can migrate also non-LRU pages which are
+		 * not accounted to NR_ISOLATED_*. They can be recognized
+		 * as __PageMovable
+		 */
+		if (likely(!__PageMovable(page)))
+			mod_node_page_state(page_pgdat(page), NR_ISOLATED_ANON +
+					page_is_file_lru(page), -thp_nr_pages(page));
+
+		if (reason != MR_MEMORY_FAILURE) {
+			/*
+			 * We release the page in page_handle_poison.
+			 */
+			put_page(page);
+		}
+
+	} else {
+		// Never expected this to happen
+		BUG_ON(rc != -EAGAIN);
+
+		// TODO(lingfeng): for shadowed page, we should not reclaim the new page
+		if (put_new_page)
+			put_new_page(newpage, private);
+		else {
+			put_page(newpage);
+		}
+	}
+
+	return rc;
+
+out:
+	if (rc != -EAGAIN) {
+		/*
+		 * A page that has been migrated has all references
+		 * removed and will be freed. A page that has not been
+		 * migrated will have kept its references and be restored.
+		 */
+		list_del(&page->lru);
+	}
+
+		/*
+		 * Compaction can migrate also non-LRU pages which are
+		 * not accounted to NR_ISOLATED_*. They can be recognized
+		 * as __PageMovable
+		 */
+
+	if (rc == MIGRATEPAGE_SUCCESS) {
+		/*
+		 * Compaction can migrate also non-LRU pages which are
+		 * not accounted to NR_ISOLATED_*. They can be recognized
+		 * as __PageMovable
+		 */
+		if (likely(!__PageMovable(page)))
+			mod_node_page_state(page_pgdat(page),
+					    NR_ISOLATED_ANON + page_is_file_lru(page),
+					    -thp_nr_pages(page));
+
+		if (reason != MR_MEMORY_FAILURE)
+			/*
+			 * We release the page in page_handle_poison.
+			 */
+			put_page(page);
+	} else {
+		if (rc != -EAGAIN)
+			list_add_tail(&page->lru, ret);
+
+		if (put_new_page)
+			put_new_page(newpage, private);
+		else
+			put_page(newpage);
+	}
+	return rc;
+}
+
+int nomad_transit_pages(struct list_head *from, new_page_t get_new_page,
+			     free_page_t put_new_page, unsigned long private,
+			     enum migrate_mode mode, int reason,
+			     unsigned int *nr_succeeded,
+			     unsigned int *nr_retried, struct nomad_context *contxt)
+{
+	int retry = 1;
+	int thp_retry = 1;
+	int nr_failed = 0;
+	int nr_thp_succeeded = 0;
+	int nr_thp_failed = 0;
+	int nr_thp_split = 0;
+	int pass = 0;
+	bool is_thp = false;
+	struct page *page;
+	struct page *page2;
+	int swapwrite = current->flags & PF_SWAPWRITE;
+	int rc, nr_subpages;
+	LIST_HEAD(ret_pages);
+
+	(*nr_retried) = 0;
+
+	if (!swapwrite)
+		current->flags |= PF_SWAPWRITE;
+
+	for (pass = 0; pass < 10 && (retry || thp_retry); pass++) {
+		retry = 0;
+		thp_retry = 0;
+
+		list_for_each_entry_safe(page, page2, from, lru) {
+retry:
+			/*
+			 * THP statistics is based on the source huge page.
+			 * Capture required information that might get lost
+			 * during migration.
+			 */
+			is_thp = PageTransHuge(page) && !PageHuge(page);
+			nr_subpages = thp_nr_pages(page);
+			cond_resched();
+
+			if (PageHuge(page))
+				rc = unmap_and_move_huge_page(get_new_page,
+						put_new_page, private, page,
+						pass > 2, mode, reason,
+						&ret_pages);
+			else
+				rc = nomad_unmap_and_move(
+					get_new_page, put_new_page, private,
+					page, pass > 2, mode, reason,
+					&ret_pages, contxt);
+			/*
+			 * The rules are:
+			 *	Success: non hugetlb page will be freed, hugetlb
+			 *		 page will be put back
+			 *	-EAGAIN: stay on the from list
+			 *	-ENOMEM: stay on the from list
+			 *	Other errno: put on ret_pages list then splice to
+			 *		     from list
+			 */
+			switch(rc) {
+			/*
+			 * THP migration might be unsupported or the
+			 * allocation could've failed so we should
+			 * retry on the same page with the THP split
+			 * to base pages.
+			 *
+			 * Head page is retried immediately and tail
+			 * pages are added to the tail of the list so
+			 * we encounter them after the rest of the list
+			 * is processed.
+			 */
+			case -ENOSYS:
+				/* THP migration is unsupported */
+				if (is_thp) {
+					if (!try_split_thp(page, &page2, from)) {
+						nr_thp_split++;
+						goto retry;
+					}
+
+					nr_thp_failed++;
+					nr_failed += nr_subpages;
+					break;
+				}
+
+				/* Hugetlb migration is unsupported */
+				nr_failed++;
+				break;
+			case -ENOMEM:
+				/*
+				 * When memory is low, don't bother to try to migrate
+				 * other pages, just exit.
+				 */
+				if (is_thp) {
+					if (!try_split_thp(page, &page2, from)) {
+						nr_thp_split++;
+						goto retry;
+					}
+
+					nr_thp_failed++;
+					nr_failed += nr_subpages;
+					goto out;
+				}
+				nr_failed++;
+				count_vm_events(PGMIGRATE_NOMEM_FAIL, thp_nr_pages(page));
+				goto out;
+			case -EAGAIN:
+				if (is_thp) {
+					thp_retry++;
+					break;
+				}
+				(*nr_retried) += 1;
+				retry++;
+				break;
+			case MIGRATEPAGE_SUCCESS:
+				if (is_thp) {
+					nr_thp_succeeded++;
+					*nr_succeeded += nr_subpages;
+					break;
+				}
+				(*nr_succeeded)++;
+				break;
+			default:
+				/*
+				 * Permanent failure (-EBUSY, etc.):
+				 * unlike -EAGAIN case, the failed page is
+				 * removed from migration page list and not
+				 * retried in the next outer loop.
+				 */
+				if (is_thp) {
+					nr_thp_failed++;
+					nr_failed += nr_subpages;
+					break;
+				}
+				nr_failed++;
+				break;
+			}
+		}
+	}
+	nr_failed += retry + thp_retry;
+	nr_thp_failed += thp_retry;
+	rc = nr_failed;
+out:
+	/*
+	 * Put the permanent failure page back to migration list, they
+	 * will be put back to the right list by the caller.
+	 */
+	list_splice(&ret_pages, from);
+
+	if (!swapwrite)
+		current->flags &= ~PF_SWAPWRITE;
+
+	return rc;
+}
+EXPORT_SYMBOL(nomad_transit_pages);
+
+static int demotion_migrate_page(struct address_space *mapping,
+				 struct page *newpage, struct page *page,
+				 enum migrate_mode mode,
+				 struct demote_shadow_page_context *contxt)
+{
+	int rc;
+	int extra_count = 0;
+	BUG_ON(PageWriteback(page)); /* Writeback must be complete */
+
+	if (contxt->use_shadow_page) {
+		// we inc the ref count when we find a shadow page
+		extra_count = 1;
+	}
+	rc = migrate_page_move_mapping(mapping, newpage, page, extra_count);
+
+	if (rc != MIGRATEPAGE_SUCCESS)
+		return rc;
+
+	if (mode != MIGRATE_SYNC_NO_COPY) {
+		if (async_mod_glob_ctrl.initialized) {
+			if (async_mod_glob_ctrl.demote_shadow_page_breakup) {
+				bool use_shadow_page;
+				// if new page is the "shadow page" of page, we directly skip
+				// the copy
+				use_shadow_page =
+					async_mod_glob_ctrl
+						.demote_shadow_page_breakup(
+							page, contxt);
+				if (use_shadow_page) {
+					goto out;
+				}
+			}
+		}
+		migrate_page_copy(newpage, page);
+	} else
+		migrate_page_states(newpage, page);
+out:
+	return MIGRATEPAGE_SUCCESS;
+}
+
+static int demotion_move_to_new_page(struct page *newpage, struct page *page,
+				     enum migrate_mode mode,
+				     struct demote_shadow_page_context *contxt)
+{
+	struct address_space *mapping;
+	int rc = -EAGAIN;
+	bool is_lru = !__PageMovable(page);
+
+	VM_BUG_ON_PAGE(!PageLocked(page), page);
+	VM_BUG_ON_PAGE(!PageLocked(newpage), newpage);
+
+	mapping = page_mapping(page);
+
+	if (likely(is_lru)) {
+		if (!mapping)
+			rc = demotion_migrate_page(mapping, newpage, page, mode,
+						   contxt);
+		else if (mapping->a_ops->migratepage)
+			/*
+			 * Most pages have a mapping and most filesystems
+			 * provide a migratepage callback. Anonymous pages
+			 * are part of swap space which also has its own
+			 * migratepage callback. This is the most common path
+			 * for page migration.
+			 */
+			rc = mapping->a_ops->migratepage(mapping, newpage, page,
+							 mode);
+		else
+			rc = fallback_migrate_page(mapping, newpage, page,
+						   mode);
+	} else {
+		/*
+		 * In case of non-lru page, it could be released after
+		 * isolation step. In that case, we shouldn't try migration.
+		 */
+		VM_BUG_ON_PAGE(!PageIsolated(page), page);
+		if (!PageMovable(page)) {
+			rc = MIGRATEPAGE_SUCCESS;
+			__ClearPageIsolated(page);
+			goto out;
+		}
+
+		rc = mapping->a_ops->migratepage(mapping, newpage, page, mode);
+		WARN_ON_ONCE(rc == MIGRATEPAGE_SUCCESS && !PageIsolated(page));
+	}
+
+	/*
+	 * When successful, old pagecache page->mapping must be cleared before
+	 * page is freed; but stats require that PageAnon be left as PageAnon.
+	 */
+	if (rc == MIGRATEPAGE_SUCCESS) {
+		if (__PageMovable(page)) {
+			VM_BUG_ON_PAGE(!PageIsolated(page), page);
+
+			/*
+			 * We clear PG_movable under page_lock so any compactor
+			 * cannot try to migrate this page.
+			 */
+			__ClearPageIsolated(page);
+		}
+
+		/*
+		 * Anonymous and movable page->mapping will be cleared by
+		 * free_pages_prepare so don't reset it here for keeping
+		 * the type to work PageAnon, for example.
+		 */
+		if (!PageMappingFlags(page))
+			page->mapping = NULL;
+
+		if (likely(!is_zone_device_page(newpage)))
+			flush_dcache_page(newpage);
+	}
+out:
+	return rc;
+}
+
+static int __demotion_unmap_and_move(struct page *page, struct page *newpage, int force,
+			    enum migrate_mode mode,
+			    struct demote_shadow_page_context *contxt)
+{
+	int rc = -EAGAIN;
+	int page_was_mapped = 0;
+	struct anon_vma *anon_vma = NULL;
+	bool is_lru = !__PageMovable(page);
+
+	if (!trylock_page(page)) {
+		if (!force || mode == MIGRATE_ASYNC)
+			goto out;
+
+		/*
+		 * It's not safe for direct compaction to call lock_page.
+		 * For example, during page readahead pages are added locked
+		 * to the LRU. Later, when the IO completes the pages are
+		 * marked uptodate and unlocked. However, the queueing
+		 * could be merging multiple pages for one bio (e.g.
+		 * mpage_readahead). If an allocation happens for the
+		 * second or third page, the process can end up locking
+		 * the same page twice and deadlocking. Rather than
+		 * trying to be clever about what pages can be locked,
+		 * avoid the use of lock_page for direct compaction
+		 * altogether.
+		 */
+		if (current->flags & PF_MEMALLOC)
+			goto out;
+
+		lock_page(page);
+	}
+
+	if (PageWriteback(page)) {
+		/*
+		 * Only in the case of a full synchronous migration is it
+		 * necessary to wait for PageWriteback. In the async case,
+		 * the retry loop is too short and in the sync-light case,
+		 * the overhead of stalling is too much
+		 */
+		switch (mode) {
+		case MIGRATE_SYNC:
+		case MIGRATE_SYNC_NO_COPY:
+			break;
+		default:
+			rc = -EBUSY;
+			goto out_unlock;
+		}
+		if (!force)
+			goto out_unlock;
+		wait_on_page_writeback(page);
+	}
+
+	/*
+	 * By try_to_unmap(), page->mapcount goes down to 0 here. In this case,
+	 * we cannot notice that anon_vma is freed while we migrates a page.
+	 * This get_anon_vma() delays freeing anon_vma pointer until the end
+	 * of migration. File cache pages are no problem because of page_lock()
+	 * File Caches may use write_page() or lock_page() in migration, then,
+	 * just care Anon page here.
+	 *
+	 * Only page_get_anon_vma() understands the subtleties of
+	 * getting a hold on an anon_vma from outside one of its mms.
+	 * But if we cannot get anon_vma, then we won't need it anyway,
+	 * because that implies that the anon page is no longer mapped
+	 * (and cannot be remapped so long as we hold the page lock).
+	 */
+	if (PageAnon(page) && !PageKsm(page))
+		anon_vma = page_get_anon_vma(page);
+
+	/*
+	 * Block others from accessing the new page when we get around to
+	 * establishing additional references. We are usually the only one
+	 * holding a reference to newpage at this point. We used to have a BUG
+	 * here if trylock_page(newpage) fails, but would like to allow for
+	 * cases where there might be a race with the previous use of newpage.
+	 * This is much like races on refcount of oldpage: just don't BUG().
+	 */
+	if (unlikely(!trylock_page(newpage)))
+		goto out_unlock;
+
+	if (unlikely(!is_lru)) {
+		rc = demotion_move_to_new_page(newpage, page, mode, NULL);
+		goto out_unlock_both;
+	}
+
+	/*
+	 * Corner case handling:
+	 * 1. When a new swap-cache page is read into, it is added to the LRU
+	 * and treated as swapcache but it has no rmap yet.
+	 * Calling try_to_unmap() against a page->mapping==NULL page will
+	 * trigger a BUG.  So handle it here.
+	 * 2. An orphaned page (see truncate_cleanup_page) might have
+	 * fs-private metadata. The page can be picked up due to memory
+	 * offlining.  Everywhere else except page reclaim, the page is
+	 * invisible to the vm, so the page can not be migrated.  So try to
+	 * free the metadata, so the page can be freed.
+	 */
+	if (!page->mapping) {
+		VM_BUG_ON_PAGE(PageAnon(page), page);
+		if (page_has_private(page)) {
+			try_to_free_buffers(page);
+			goto out_unlock_both;
+		}
+	} else if (page_mapped(page)) {
+		/* Establish migration ptes */
+		VM_BUG_ON_PAGE(PageAnon(page) && !PageKsm(page) && !anon_vma,
+				page);
+		contxt->traversed_mapping_num = 0;
+		contxt->flags = (void *)(0);
+
+		nomad_try_to_migrate(page, 0,
+				      contxt);
+		page_was_mapped = 1;
+	}
+
+	if (!page_mapped(page))
+		rc = demotion_move_to_new_page(newpage, page, mode, contxt);
+
+	if (page_was_mapped){
+		contxt->old_page = page;
+		demotion_remove_migration_ptes(contxt,
+			rc == MIGRATEPAGE_SUCCESS ? newpage : page, false);
+	}
+
+
+out_unlock_both:
+	unlock_page(newpage);
+out_unlock:
+	/* Drop an anon_vma reference if we took one */
+	if (anon_vma)
+		put_anon_vma(anon_vma);
+	unlock_page(page);
+out:
+	/*
+	 * If migration is successful, decrease refcount of the newpage
+	 * which will not free the page because new page owner increased
+	 * refcounter. As well, if it is LRU page, add the page to LRU
+	 * list in here. Use the old state of the isolated source page to
+	 * determine if we migrated a LRU page. newpage was already unlocked
+	 * and possibly modified by its owner - don't rely on the page
+	 * state.
+	 */
+	if (rc == MIGRATEPAGE_SUCCESS) {
+		if (unlikely(!is_lru))
+			put_page(newpage);
+		else
+			putback_lru_page(newpage);
+	}
+
+	return rc;
+}
+
+static int demotion_unmap_and_move(new_page_t get_new_page,
+				   free_page_t put_new_page,
+				   unsigned long private, struct page *page,
+				   int force, enum migrate_mode mode,
+				   enum migrate_reason reason,
+				   struct list_head *ret)
+{
+	int rc = MIGRATEPAGE_SUCCESS;
+	struct page *newpage = NULL;
+	bool should_demote_page;
+	struct demote_shadow_page_context contxt = {
+		.use_shadow_page = 0,
+		.shadow_page_ref_num = 0,
+		.traversed_mapping_num = 0,
+		.was_writable_before_shadowed = false,
+		.made_writable = false,
+		.shadow_page = NULL,
+	};
+
+	if (!thp_migration_supported() && PageTransHuge(page))
+		return -ENOSYS;
+
+	if (page_count(page) == 1) {
+		/* page was freed from under us. So we are done. */
+		ClearPageActive(page);
+		ClearPageUnevictable(page);
+		if (unlikely(__PageMovable(page))) {
+			lock_page(page);
+			if (!PageMovable(page))
+				__ClearPageIsolated(page);
+			unlock_page(page);
+		}
+		goto out;
+	}
+
+	if (reason == MR_DEMOTION && !PageKsm(page)) {
+		should_demote_page = true;
+	} else {
+		should_demote_page = false;
+	}
+
+	if (should_demote_page && async_mod_glob_ctrl.initialized &&
+	    async_mod_glob_ctrl.demote_shadow_page_find) {
+		newpage = async_mod_glob_ctrl.demote_shadow_page_find(page,
+								      &contxt);
+		if (newpage) {
+			goto use_shadowpage;
+		}
+	}
+
+	newpage = get_new_page(page, private);
+	if (!newpage)
+		return -ENOMEM;
+use_shadowpage:
+	/* TODO: check whether Ksm pages can be demoted? */
+	if (should_demote_page)
+		set_page_demoted(newpage);
+
+	rc = __demotion_unmap_and_move(page, newpage, force, mode, &contxt);
+
+	if (likely(contxt.shadow_page_ref_num == 0)) {
+	} else if (contxt.shadow_page_ref_num == 1) {
+		// using a shadow page may fail, reclaim it
+		put_page(contxt.shadow_page);
+	} else {
+		pr_err("error shadow_page_ref_num is %d, rc? %d, not expected",
+		       contxt.shadow_page_ref_num, rc);
+		BUG();
+	}
+
+	if (rc == MIGRATEPAGE_SUCCESS)
+		set_page_owner_migrate_reason(newpage, reason);
+
+out:
+	if (rc != -EAGAIN) {
+		/*
+		 * A page that has been migrated has all references
+		 * removed and will be freed. A page that has not been
+		 * migrated will have kept its references and be restored.
+		 */
+		list_del(&page->lru);
+	}
+
+	/*
+	 * If migration is successful, releases reference grabbed during
+	 * isolation. Otherwise, restore the page to right list unless
+	 * we want to retry.
+	 */
+	if (rc == MIGRATEPAGE_SUCCESS) {
+		/*
+		 * Compaction can migrate also non-LRU pages which are
+		 * not accounted to NR_ISOLATED_*. They can be recognized
+		 * as __PageMovable
+		 */
+		if (likely(!__PageMovable(page)))
+			mod_node_page_state(page_pgdat(page), NR_ISOLATED_ANON +
+					page_is_file_lru(page), -thp_nr_pages(page));
+
+		if (reason != MR_MEMORY_FAILURE)
+			/*
+			 * We release the page in page_handle_poison.
+			 */
+			put_page(page);
+	} else {
+		if (rc != -EAGAIN)
+			list_add_tail(&page->lru, ret);
+
+		if (put_new_page)
+			put_new_page(newpage, private);
+		else
+			put_page(newpage);
+	}
+
+	return rc;
+}
+
+
+int nomad_demotion_migrate_pages(struct list_head *from, new_page_t get_new_page,
+		free_page_t put_new_page, unsigned long private,
+		enum migrate_mode mode, int reason, unsigned int *nr_succeeded)
+{
+	int retry = 1;
+	int thp_retry = 1;
+	int nr_failed = 0;
+	int nr_thp_succeeded = 0;
+	int nr_thp_failed = 0;
+	int nr_thp_split = 0;
+	int pass = 0;
+	bool is_thp = false;
+	struct page *page;
+	struct page *page2;
+	int swapwrite = current->flags & PF_SWAPWRITE;
+	int rc, nr_subpages;
+	LIST_HEAD(ret_pages);
+
+	trace_mm_migrate_pages_start(mode, reason);
+
+	if (!swapwrite)
+		current->flags |= PF_SWAPWRITE;
+
+	for (pass = 0; pass < 10 && (retry || thp_retry); pass++) {
+		retry = 0;
+		thp_retry = 0;
+
+		list_for_each_entry_safe(page, page2, from, lru) {
+retry:
+			/*
+			 * THP statistics is based on the source huge page.
+			 * Capture required information that might get lost
+			 * during migration.
+			 */
+			is_thp = PageTransHuge(page) && !PageHuge(page);
+			nr_subpages = thp_nr_pages(page);
+			cond_resched();
+
+			if (PageHuge(page))
+				rc = unmap_and_move_huge_page(get_new_page,
+						put_new_page, private, page,
+						pass > 2, mode, reason,
+						&ret_pages);
+			else
+				rc = demotion_unmap_and_move(get_new_page, put_new_page,
+						private, page, pass > 2, mode,
+						reason, &ret_pages);
+			/*
+			 * The rules are:
+			 *	Success: non hugetlb page will be freed, hugetlb
+			 *		 page will be put back
+			 *	-EAGAIN: stay on the from list
+			 *	-ENOMEM: stay on the from list
+			 *	Other errno: put on ret_pages list then splice to
+			 *		     from list
+			 */
+			switch(rc) {
+			/*
+			 * THP migration might be unsupported or the
+			 * allocation could've failed so we should
+			 * retry on the same page with the THP split
+			 * to base pages.
+			 *
+			 * Head page is retried immediately and tail
+			 * pages are added to the tail of the list so
+			 * we encounter them after the rest of the list
+			 * is processed.
+			 */
+			case -ENOSYS:
+				/* THP migration is unsupported */
+				if (is_thp) {
+					if (!try_split_thp(page, &page2, from)) {
+						nr_thp_split++;
+						goto retry;
+					}
+
+					nr_thp_failed++;
+					nr_failed += nr_subpages;
+					break;
+				}
+
+				/* Hugetlb migration is unsupported */
+				nr_failed++;
+				break;
+			case -ENOMEM:
+				/*
+				 * When memory is low, don't bother to try to migrate
+				 * other pages, just exit.
+				 */
+				if (is_thp) {
+					if (!try_split_thp(page, &page2, from)) {
+						nr_thp_split++;
+						goto retry;
+					}
+
+					nr_thp_failed++;
+					nr_failed += nr_subpages;
+					goto out;
+				}
+				nr_failed++;
+				count_vm_events(PGMIGRATE_NOMEM_FAIL, thp_nr_pages(page));
+				goto out;
+			case -EAGAIN:
+				if (is_thp) {
+					thp_retry++;
+					break;
+				}
+				retry++;
+				break;
+			case MIGRATEPAGE_SUCCESS:
+				if (is_thp) {
+					nr_thp_succeeded++;
+					*nr_succeeded += nr_subpages;
+					break;
+				}
+				(*nr_succeeded)++;
+				break;
+			default:
+				/*
+				 * Permanent failure (-EBUSY, etc.):
+				 * unlike -EAGAIN case, the failed page is
+				 * removed from migration page list and not
+				 * retried in the next outer loop.
+				 */
+				if (is_thp) {
+					nr_thp_failed++;
+					nr_failed += nr_subpages;
+					break;
+				}
+				nr_failed++;
+				break;
+			}
+		}
+	}
+	nr_failed += retry + thp_retry;
+	nr_thp_failed += thp_retry;
+	rc = nr_failed;
+out:
+	/*
+	 * Put the permanent failure page back to migration list, they
+	 * will be put back to the right list by the caller.
+	 */
+	list_splice(&ret_pages, from);
+
+	count_vm_events(PGMIGRATE_SUCCESS, *nr_succeeded);
+	count_vm_events(PGMIGRATE_FAIL, nr_failed);
+	count_vm_events(THP_MIGRATION_SUCCESS, nr_thp_succeeded);
+	count_vm_events(THP_MIGRATION_FAIL, nr_thp_failed);
+	count_vm_events(THP_MIGRATION_SPLIT, nr_thp_split);
+	trace_mm_migrate_pages(*nr_succeeded, nr_failed, nr_thp_succeeded,
+			       nr_thp_failed, nr_thp_split, mode, reason);
+
+	if (!swapwrite)
+		current->flags &= ~PF_SWAPWRITE;
+
+	return rc;
+}
diff --git a/mm/nomad/rmap.c b/mm/nomad/rmap.c
new file mode 100644
index 000000000000..092c2decaba6
--- /dev/null
+++ b/mm/nomad/rmap.c
@@ -0,0 +1,1189 @@
+#include <linux/mm.h>
+#include <linux/swap.h>
+#include <linux/swapops.h>
+#include <linux/rmap.h>
+#include <linux/mmu_notifier.h>
+#include <linux/hugetlb.h>
+#include <linux/sched/numa_balancing.h>
+
+#include <linux/nomad.h>
+
+#include "../internal.h"
+
+extern bool invalid_migration_vma(struct vm_area_struct *vma, void *arg);
+extern int page_not_mapped(struct page *page);
+
+static void remap_clean_page(struct page *old, struct page *page,
+			     struct page_vma_mapped_walk *pvmw,
+			     struct vm_area_struct *vma,
+			     struct nomad_remap_status *status)
+{
+	pte_t new_pteval, old_pteval;
+	struct page *new;
+	bool use_new_page = true;
+	if (PageKsm(page)) {
+		new = page;
+		pr_info("not expected. [%s]:[%d]", __FILE__, __LINE__);
+	} else {
+		new = page - old->index + linear_page_index(vma, pvmw->address);
+	}
+	pr_debug("[%s]:[%d], remap clean page is called.", __FILE__, __LINE__);
+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION
+	/* PMD-mapped THP migration entry */
+	if (!pvmw->pte) {
+		pr_info("not expected. [%s]:[%d]", __FILE__, __LINE__);
+		BUG();
+	}
+#endif
+	old_pteval = status->old_pteval;
+	if (pte_dirty(old_pteval)) {
+		// go back to previous page`
+		use_new_page = false;
+		status->use_new_page = false;
+	}
+
+	if (use_new_page) {
+		get_page(new);
+	} else {
+		get_page(old);
+	}
+	if (use_new_page) {
+		new_pteval =
+			pte_mkold(mk_pte(new, READ_ONCE(vma->vm_page_prot)));
+	} else {
+		new_pteval =
+			pte_mkold(mk_pte(old, READ_ONCE(vma->vm_page_prot)));
+	}
+
+	if (pte_swp_soft_dirty(old_pteval))
+		new_pteval = pte_mksoft_dirty(new_pteval);
+
+	/*
+	* Recheck VMA as permissions can change since migration started
+	*/
+
+	if (pte_soft_dirty(old_pteval)) {
+		new_pteval = pte_mksoft_dirty(new_pteval);
+	}
+	if (pte_uffd_wp(old_pteval)) {
+		new_pteval = pte_mkuffd_wp(new_pteval);
+	}
+
+	if (use_new_page && pte_write(old_pteval)) {
+		new_pteval = pte_wrprotect(new_pteval);
+
+		new_pteval = pte_mk_orig_writable(new_pteval);
+
+	} else if (!use_new_page && pte_write(old_pteval)) {
+		new_pteval = maybe_mkwrite(new_pteval, vma);
+	} else if (!pte_write(old_pteval)) {
+		new_pteval = pte_wrprotect(new_pteval);
+	} else {
+		BUG();
+	}
+
+	if (unlikely(is_device_private_page(new))) {
+		pr_info("not expected. [%s]:[%d]", __FILE__, __LINE__);
+		BUG();
+	}
+
+#ifdef CONFIG_HUGETLB_PAGE
+	if (PageHuge(new)) {
+		pr_info("not expected. [%s]:[%d]", __FILE__, __LINE__);
+		BUG();
+	} else
+#endif
+	{
+#ifdef CONFIG_NUMA_BALANCING
+		if (page_is_demoted(old) && vma_migratable(vma)) {
+			bool writable = pte_write(old_pteval);
+
+			new_pteval = pte_modify(new_pteval, PAGE_NONE);
+			if (writable)
+				new_pteval = pte_mk_savedwrite(new_pteval);
+		}
+#endif
+		set_pte_at(vma->vm_mm, pvmw->address, pvmw->pte, new_pteval);
+		// if the page is dirty, we still use the old page
+		if (use_new_page) {
+			if (PageAnon(new)) {
+				page_add_anon_rmap(new, vma, pvmw->address,
+						   false);
+			} else {
+				pr_info("not expected. [%s]:[%d]", __FILE__,
+					__LINE__);
+				page_add_file_rmap(new, false);
+			}
+			if (vma->vm_flags & VM_LOCKED &&
+			    !PageTransCompound(new))
+				mlock_vma_page(new);
+
+			if (PageTransHuge(page) && PageMlocked(page))
+				clear_page_mlock(page);
+		}
+	}
+
+	/* No need to invalidate - it was non-present before */
+	update_mmu_cache(vma, pvmw->address, pvmw->pte);
+}
+
+// Nomad's original implementation is baed on try_to_unmap. However, in
+// v5.14 try_to_migrate has been isolated from try_to_unmap to reduce
+// clutter. See commit: cd62734 and a98a2f0
+
+// diff --git a/try_to_unmap.c b/nomad_try_to_unmap.c
+// index 90481ec..44c4c30 100644
+// --- a/try_to_unmap.c
+// +++ b/nomad_try_to_unmap.c
+// @@ -1,8 +1,9 @@
+// -bool try_to_unmap(struct page *page, enum ttu_flags flags)
+// +bool demotion_try_to_unmap(struct page *page, enum ttu_flags flags,
+// +                          struct demote_shadow_page_context *context)
+//  {
+//         struct rmap_walk_control rwc = {
+// -               .rmap_one = try_to_unmap_one,
+// -               .arg = (void *)flags,
+// +               .rmap_one = demotion_try_to_unmap_one,
+// +               .arg = (void *)context,
+//                 .done = page_not_mapped,
+//                 .anon_lock = page_lock_anon_vma_read,
+//         };
+
+// diff --git a/try_to_unmap.c b/nomad_try_to_remap.c
+// index 90481ec..234a00c 100644
+// --- a/try_to_unmap.c
+// +++ b/nomad_try_to_remap.c
+// @@ -1,8 +1,8 @@
+// -bool try_to_unmap(struct page *page, enum ttu_flags flags)
+// +bool nomad_try_to_remap(struct page *page, struct nomad_remap_status *arg)
+//  {
+//         struct rmap_walk_control rwc = {
+// -               .rmap_one = try_to_unmap_one,
+// -               .arg = (void *)flags,
+// +               .rmap_one = nomad_remap_single_ref,
+// +               .arg = (void *)arg,
+//                 .done = page_not_mapped,
+//                 .anon_lock = page_lock_anon_vma_read,
+//         };
+// @@ -15,11 +15,11 @@ bool try_to_unmap(struct page *page, enum ttu_flags flags)
+//          * locking requirements of exec(), migration skips
+//          * temporary VMAs until after exec() completes.
+//          */
+// -       if ((flags & (TTU_MIGRATION|TTU_SPLIT_FREEZE))
+// +       if ((arg->flags & (TTU_MIGRATION|TTU_SPLIT_FREEZE))
+//             && !PageKsm(page) && PageAnon(page))
+//                 rwc.invalid_vma = invalid_migration_vma;
+//
+// -       if (flags & TTU_RMAP_LOCKED)
+// +       if (arg->flags & TTU_RMAP_LOCKED)
+//                 rmap_walk_locked(page, &rwc);
+//         else
+//                 rmap_walk(page, &rwc);
+
+// diff --git a/try_to_unmap_one.c b/nomad_try_to_unmap_one.c
+// index fa82cb9..aabc754 100644
+// --- a/try_to_unmap_one.c
+// +++ b/nomad_try_to_unmap_one.c
+// @@ -1,5 +1,6 @@
+// -static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+// -                    unsigned long address, void *arg)
+// +static bool demotion_try_to_unmap_one(struct page *page,
+// +                                     struct vm_area_struct *vma,
+// +                                     unsigned long address, void *arg)
+//  {
+//         struct mm_struct *mm = vma->vm_mm;
+//         struct page_vma_mapped_walk pvmw = {
+// @@ -11,8 +12,11 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+//         struct page *subpage;
+//         bool ret = true;
+//         struct mmu_notifier_range range;
+// -       enum ttu_flags flags = (enum ttu_flags)(long)arg;
+// +       struct demote_shadow_page_context *contxt =
+// +               (struct demote_shadow_page_context *)arg;
+// +       enum ttu_flags flags = (enum ttu_flags)(long)(contxt->flags);
+//
+// +       contxt->traversed_mapping_num += 1;
+//         /* munlock has nothing to gain from examining un-locked vmas */
+//         if ((flags & TTU_MUNLOCK) && !(vma->vm_flags & VM_LOCKED))
+//                 return true;
+// @@ -225,6 +229,12 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+//                                 break;
+//                         }
+//
+// +                       if (pte_orig_writable(pteval)) {
+// +                               contxt->was_writable_before_shadowed = true;
+// +                       }
+// +                       if (pte_write(pteval)) {
+// +                               contxt->made_writable = true;
+// +                       }
+//                         /*
+//                          * Store the pfn of the page in a special migration
+//                          * pte. do_swap_page() will wait until the migration
+
+// diff --git a/try_to_unmap_one.c b/nomad_try_to_remap_one.c
+// index fa82cb9..4e0abb1 100644
+// --- a/try_to_unmap_one.c
+// +++ b/nomad_try_to_remap_one.c
+// @@ -1,5 +1,6 @@
+// -static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+// -                    unsigned long address, void *arg)
+// +static bool nomad_remap_single_ref(struct page *page,
+// +                                       struct vm_area_struct *vma,
+// +                                       unsigned long address, void *arg)
+//  {
+//         struct mm_struct *mm = vma->vm_mm;
+//         struct page_vma_mapped_walk pvmw = {
+// @@ -11,7 +12,9 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+//         struct page *subpage;
+//         bool ret = true;
+//         struct mmu_notifier_range range;
+// -       enum ttu_flags flags = (enum ttu_flags)(long)arg;
+// +       struct nomad_remap_status *remap_arg =
+// +               (struct nomad_remap_status *)arg;
+// +       enum ttu_flags flags = remap_arg->flags;
+//
+//         /* munlock has nothing to gain from examining un-locked vmas */
+//         if ((flags & TTU_MUNLOCK) && !(vma->vm_flags & VM_LOCKED))
+// @@ -178,7 +181,8 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+//                 } else {
+//                         pteval = ptep_clear_flush(vma, address, pvmw.pte);
+//                 }
+// -
+// +               // this page is unmapped once, ref count decrement by 1
+// +               put_page(page);
+//                 /* Move the dirty bit to the page. Now the pte is gone. */
+//                 if (pte_dirty(pteval))
+//                         set_page_dirty(page);
+// @@ -215,8 +219,6 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+//                                                       address + PAGE_SIZE);
+//                 } else if (IS_ENABLED(CONFIG_MIGRATION) &&
+//                                 (flags & (TTU_MIGRATION|TTU_SPLIT_FREEZE))) {
+// -                       swp_entry_t entry;
+// -                       pte_t swp_pte;
+//
+//                         if (arch_unmap_one(mm, vma, address, pteval) < 0) {
+//                                 set_pte_at(mm, address, pvmw.pte, pteval);
+// @@ -224,24 +226,29 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+//                                 page_vma_mapped_walk_done(&pvmw);
+//                                 break;
+//                         }
+// -
+// -                       /*
+// -                        * Store the pfn of the page in a special migration
+// -                        * pte. do_swap_page() will wait until the migration
+// -                        * pte is removed and then restart fault handling.
+// -                        */
+// -                       entry = make_migration_entry(subpage,
+// -                                       pte_write(pteval));
+// -                       swp_pte = swp_entry_to_pte(entry);
+// -                       if (pte_soft_dirty(pteval))
+// -                               swp_pte = pte_swp_mksoft_dirty(swp_pte);
+// -                       if (pte_uffd_wp(pteval))
+// -                               swp_pte = pte_swp_mkuffd_wp(swp_pte);
+// -                       set_pte_at(mm, address, pvmw.pte, swp_pte);
+// -                       /*
+// -                        * No need to invalidate here it will synchronize on
+// -                        * against the special swap migration pte.
+// -                        */
+// +                       remap_arg->old_pteval = pteval;
+// +                       remap_clean_page(pvmw.page, remap_arg->new_page, &pvmw,
+// +                                        vma, remap_arg);
+// +                       // /*
+// +                       //  * Store the pfn of the page in a special migration
+// +                       //  * pte. do_swap_page() will wait until the migration
+// +                       //  * pte is removed and then restart fault handling.
+// +                       //  */
+// +                       // entry = make_migration_entry(subpage,
+// +                       //              pte_write(pteval));
+// +                       // swp_pte = swp_entry_to_pte(entry);
+// +                       // if (pte_soft_dirty(pteval)){
+// +                       //      swp_pte = pte_swp_mksoft_dirty(swp_pte);
+// +                       //      // the page has been modified during the page copy
+// +                       //      remap_arg->unmapped_clean_page = false;
+// +                       // }
+// +                       // if (pte_uffd_wp(pteval))
+// +                       //      swp_pte = pte_swp_mkuffd_wp(swp_pte);
+// +                       // set_pte_at(mm, address, pvmw.pte, swp_pte);
+// +                       // /*
+// +                       //  * No need to invalidate here it will synchronize on
+// +                       //  * against the special swap migration pte.
+// +                       //  */
+//                 } else if (PageAnon(page)) {
+//                         swp_entry_t entry = { .val = page_private(subpage) };
+//                         pte_t swp_pte;
+// @@ -330,8 +337,10 @@ discard:
+//                  *
+//                  * See Documentation/vm/mmu_notifier.rst
+//                  */
+// -               page_remove_rmap(subpage, PageHuge(page));
+// -               put_page(page);
+// +               if (remap_arg->use_new_page) {
+// +                       // if not clean, we will use the original page
+// +                       page_remove_rmap(subpage, PageHuge(page));
+// +               }
+//         }
+//
+//         mmu_notifier_invalidate_range_end(&range);
+
+// diff --git a/try_to_unmap_one.c b/try_to_migrate_one.c
+// index fa82cb9..1279d80 100644
+// --- a/try_to_unmap_one.c
+// +++ b/try_to_migrate_one.c
+// @@ -1,4 +1,4 @@
+// -static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+// +static bool try_to_migrate_one(struct page *page, struct vm_area_struct *vma,
+//                      unsigned long address, void *arg)
+//  {
+//         struct mm_struct *mm = vma->vm_mm;
+// @@ -13,18 +13,21 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+//         struct mmu_notifier_range range;
+//         enum ttu_flags flags = (enum ttu_flags)(long)arg;
+//
+// -       /* munlock has nothing to gain from examining un-locked vmas */
+// -       if ((flags & TTU_MUNLOCK) && !(vma->vm_flags & VM_LOCKED))
+// -               return true;
+// -
+// -       if (IS_ENABLED(CONFIG_MIGRATION) && (flags & TTU_MIGRATION) &&
+// -           is_zone_device_page(page) && !is_device_private_page(page))
+// -               return true;
+// +       /*
+// +        * When racing against e.g. zap_pte_range() on another cpu,
+// +        * in between its ptep_get_and_clear_full() and page_remove_rmap(),
+// +        * try_to_migrate() may return before page_mapped() has become false,
+// +        * if page table locking is skipped: use TTU_SYNC to wait for that.
+// +        */
+// +       if (flags & TTU_SYNC)
+// +               pvmw.flags = PVMW_SYNC;
+//
+// -       if (flags & TTU_SPLIT_HUGE_PMD) {
+// -               split_huge_pmd_address(vma, address,
+// -                               flags & TTU_SPLIT_FREEZE, page);
+// -       }
+// +       /*
+// +        * unmap_page() in mm/huge_memory.c is the only user of migration with
+// +        * TTU_SPLIT_HUGE_PMD and it wants to freeze.
+// +        */
+// +       if (flags & TTU_SPLIT_HUGE_PMD)
+// +               split_huge_pmd_address(vma, address, true, page);
+//
+//         /*
+//          * For THP, we have to assume the worse case ie pmd for invalidation.
+// @@ -34,9 +37,10 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+//          * Note that the page can not be free in this function as call of
+//          * try_to_unmap() must hold a reference on the page.
+//          */
+// +       range.end = PageKsm(page) ?
+// +                       address + PAGE_SIZE : vma_address_end(page, vma);
+//         mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,
+// -                               address,
+// -                               min(vma->vm_end, address + page_size(page)));
+// +                               address, range.end);
+//         if (PageHuge(page)) {
+//                 /*
+//                  * If sharing is possible, start and end will be adjusted
+// @@ -50,37 +54,15 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+//         while (page_vma_mapped_walk(&pvmw)) {
+//  #ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION
+//                 /* PMD-mapped THP migration entry */
+// -               if (!pvmw.pte && (flags & TTU_MIGRATION)) {
+// -                       VM_BUG_ON_PAGE(PageHuge(page) || !PageTransCompound(page), page);
+// +               if (!pvmw.pte) {
+// +                       VM_BUG_ON_PAGE(PageHuge(page) ||
+// +                                      !PageTransCompound(page), page);
+//
+//                         set_pmd_migration_entry(&pvmw, page);
+//                         continue;
+//                 }
+//  #endif
+//
+// -               /*
+// -                * If the page is mlock()d, we cannot swap it out.
+// -                * If it's recently referenced (perhaps page_referenced
+// -                * skipped over this mm) then we should reactivate it.
+// -                */
+// -               if (!(flags & TTU_IGNORE_MLOCK)) {
+// -                       if (vma->vm_flags & VM_LOCKED) {
+// -                               /* PTE-mapped THP are never mlocked */
+// -                               if (!PageTransCompound(page)) {
+// -                                       /*
+// -                                        * Holding pte lock, we do *not* need
+// -                                        * mmap_lock here
+// -                                        */
+// -                                       mlock_vma_page(page);
+// -                               }
+// -                               ret = false;
+// -                               page_vma_mapped_walk_done(&pvmw);
+// -                               break;
+// -                       }
+// -                       if (flags & TTU_MUNLOCK)
+// -                               continue;
+// -               }
+// -
+//                 /* Unexpected PMD-mapped THP? */
+//                 VM_BUG_ON_PAGE(!pvmw.pte, page);
+//
+// @@ -121,20 +103,28 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+//                         }
+//                 }
+//
+// -               if (IS_ENABLED(CONFIG_MIGRATION) &&
+// -                   (flags & TTU_MIGRATION) &&
+// -                   is_zone_device_page(page)) {
+// +               /* Nuke the page table entry. */
+// +               flush_cache_page(vma, address, pte_pfn(*pvmw.pte));
+// +               pteval = ptep_clear_flush(vma, address, pvmw.pte);
+// +
+// +               /* Move the dirty bit to the page. Now the pte is gone. */
+// +               if (pte_dirty(pteval))
+// +                       set_page_dirty(page);
+// +
+// +               /* Update high watermark before we lower rss */
+// +               update_hiwater_rss(mm);
+// +
+// +               if (is_zone_device_page(page)) {
+//                         swp_entry_t entry;
+//                         pte_t swp_pte;
+//
+// -                       pteval = ptep_get_and_clear(mm, pvmw.address, pvmw.pte);
+// -
+//                         /*
+//                          * Store the pfn of the page in a special migration
+//                          * pte. do_swap_page() will wait until the migration
+//                          * pte is removed and then restart fault handling.
+//                          */
+// -                       entry = make_migration_entry(page, 0);
+// +                       entry = make_readable_migration_entry(
+// +                                                       page_to_pfn(page));
+//                         swp_pte = swp_entry_to_pte(entry);
+//
+//                         /*
+// @@ -158,35 +148,7 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+//                          * memory are supported.
+//                          */
+//                         subpage = page;
+// -                       goto discard;
+// -               }
+// -
+// -               /* Nuke the page table entry. */
+// -               flush_cache_page(vma, address, pte_pfn(*pvmw.pte));
+// -               if (should_defer_flush(mm, flags)) {
+// -                       /*
+// -                        * We clear the PTE but do not flush so potentially
+// -                        * a remote CPU could still be writing to the page.
+// -                        * If the entry was previously clean then the
+// -                        * architecture must guarantee that a clear->dirty
+// -                        * transition on a cached TLB entry is written through
+// -                        * and traps if the PTE is unmapped.
+// -                        */
+// -                       pteval = ptep_get_and_clear(mm, address, pvmw.pte);
+// -
+// -                       set_tlb_ubc_flush_pending(mm, pte_dirty(pteval));
+// -               } else {
+// -                       pteval = ptep_clear_flush(vma, address, pvmw.pte);
+// -               }
+// -
+// -               /* Move the dirty bit to the page. Now the pte is gone. */
+// -               if (pte_dirty(pteval))
+// -                       set_page_dirty(page);
+// -
+// -               /* Update high watermark before we lower rss */
+// -               update_hiwater_rss(mm);
+// -
+// -               if (PageHWPoison(page) && !(flags & TTU_IGNORE_HWPOISON)) {
+// +               } else if (PageHWPoison(page)) {
+//                         pteval = swp_entry_to_pte(make_hwpoison_entry(subpage));
+//                         if (PageHuge(page)) {
+//                                 hugetlb_count_sub(compound_nr(page), mm);
+// @@ -213,8 +175,7 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+//                         /* We have to invalidate as we cleared the pte */
+//                         mmu_notifier_invalidate_range(mm, address,
+//                                                       address + PAGE_SIZE);
+// -               } else if (IS_ENABLED(CONFIG_MIGRATION) &&
+// -                               (flags & (TTU_MIGRATION|TTU_SPLIT_FREEZE))) {
+// +               } else {
+//                         swp_entry_t entry;
+//                         pte_t swp_pte;
+//
+// @@ -230,8 +191,13 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+//                          * pte. do_swap_page() will wait until the migration
+//                          * pte is removed and then restart fault handling.
+//                          */
+// -                       entry = make_migration_entry(subpage,
+// -                                       pte_write(pteval));
+// +                       if (pte_write(pteval))
+// +                               entry = make_writable_migration_entry(
+// +                                                       page_to_pfn(subpage));
+// +                       else
+// +                               entry = make_readable_migration_entry(
+// +                                                       page_to_pfn(subpage));
+// +
+//                         swp_pte = swp_entry_to_pte(entry);
+//                         if (pte_soft_dirty(pteval))
+//                                 swp_pte = pte_swp_mksoft_dirty(swp_pte);
+// @@ -242,87 +208,8 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
+//                          * No need to invalidate here it will synchronize on
+//                          * against the special swap migration pte.
+//                          */
+// -               } else if (PageAnon(page)) {
+// -                       swp_entry_t entry = { .val = page_private(subpage) };
+// -                       pte_t swp_pte;
+// -                       /*
+// -                        * Store the swap location in the pte.
+// -                        * See handle_pte_fault() ...
+// -                        */
+// -                       if (unlikely(PageSwapBacked(page) != PageSwapCache(page))) {
+// -                               WARN_ON_ONCE(1);
+// -                               ret = false;
+// -                               /* We have to invalidate as we cleared the pte */
+// -                               mmu_notifier_invalidate_range(mm, address,
+// -                                                       address + PAGE_SIZE);
+// -                               page_vma_mapped_walk_done(&pvmw);
+// -                               break;
+// -                       }
+// -
+// -                       /* MADV_FREE page check */
+// -                       if (!PageSwapBacked(page)) {
+// -                               if (!PageDirty(page)) {
+// -                                       /* Invalidate as we cleared the pte */
+// -                                       mmu_notifier_invalidate_range(mm,
+// -                                               address, address + PAGE_SIZE);
+// -                                       dec_mm_counter(mm, MM_ANONPAGES);
+// -                                       goto discard;
+// -                               }
+// -
+// -                               /*
+// -                                * If the page was redirtied, it cannot be
+// -                                * discarded. Remap the page to page table.
+// -                                */
+// -                               set_pte_at(mm, address, pvmw.pte, pteval);
+// -                               SetPageSwapBacked(page);
+// -                               ret = false;
+// -                               page_vma_mapped_walk_done(&pvmw);
+// -                               break;
+// -                       }
+// -
+// -                       if (swap_duplicate(entry) < 0) {
+// -                               set_pte_at(mm, address, pvmw.pte, pteval);
+// -                               ret = false;
+// -                               page_vma_mapped_walk_done(&pvmw);
+// -                               break;
+// -                       }
+// -                       if (arch_unmap_one(mm, vma, address, pteval) < 0) {
+// -                               set_pte_at(mm, address, pvmw.pte, pteval);
+// -                               ret = false;
+// -                               page_vma_mapped_walk_done(&pvmw);
+// -                               break;
+// -                       }
+// -                       if (list_empty(&mm->mmlist)) {
+// -                               spin_lock(&mmlist_lock);
+// -                               if (list_empty(&mm->mmlist))
+// -                                       list_add(&mm->mmlist, &init_mm.mmlist);
+// -                               spin_unlock(&mmlist_lock);
+// -                       }
+// -                       dec_mm_counter(mm, MM_ANONPAGES);
+// -                       inc_mm_counter(mm, MM_SWAPENTS);
+// -                       swp_pte = swp_entry_to_pte(entry);
+// -                       if (pte_soft_dirty(pteval))
+// -                               swp_pte = pte_swp_mksoft_dirty(swp_pte);
+// -                       if (pte_uffd_wp(pteval))
+// -                               swp_pte = pte_swp_mkuffd_wp(swp_pte);
+// -                       set_pte_at(mm, address, pvmw.pte, swp_pte);
+// -                       /* Invalidate as we cleared the pte */
+// -                       mmu_notifier_invalidate_range(mm, address,
+// -                                                     address + PAGE_SIZE);
+// -               } else {
+// -                       /*
+// -                        * This is a locked file-backed page, thus it cannot
+// -                        * be removed from the page cache and replaced by a new
+// -                        * page before mmu_notifier_invalidate_range_end, so no
+// -                        * concurrent thread might update its page table to
+// -                        * point at new page while a device still is using this
+// -                        * page.
+// -                        *
+// -                        * See Documentation/vm/mmu_notifier.rst
+// -                        */
+// -                       dec_mm_counter(mm, mm_counter_file(page));
+//                 }
+// -discard:
+// +
+//                 /*
+//                  * No need to call mmu_notifier_invalidate_range() it has be
+//                  * done above for all cases requiring it to happen under page
+
+/*
+ * @arg: enum ttu_flags will be passed to this argument.
+ *
+ * If TTU_SPLIT_HUGE_PMD is specified any PMD mappings will be split into PTEs
+ * containing migration entries.
+ */
+static bool nomad_try_to_migrate_one(struct page *page,
+				     struct vm_area_struct *vma,
+				     unsigned long address, void *arg)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	struct page_vma_mapped_walk pvmw = {
+		.page = page,
+		.vma = vma,
+		.address = address,
+	};
+	pte_t pteval;
+	struct page *subpage;
+	bool ret = true;
+	struct mmu_notifier_range range;
+	struct demote_shadow_page_context *contxt =
+		(struct demote_shadow_page_context *)arg;
+	enum ttu_flags flags = (enum ttu_flags)(long)(contxt->flags);
+
+	contxt->traversed_mapping_num += 1;
+
+	/*
+	 * When racing against e.g. zap_pte_range() on another cpu,
+	 * in between its ptep_get_and_clear_full() and page_remove_rmap(),
+	 * try_to_migrate() may return before page_mapped() has become false,
+	 * if page table locking is skipped: use TTU_SYNC to wait for that.
+	 */
+	if (flags & TTU_SYNC)
+		pvmw.flags = PVMW_SYNC;
+
+	/*
+	 * unmap_page() in mm/huge_memory.c is the only user of migration with
+	 * TTU_SPLIT_HUGE_PMD and it wants to freeze.
+	 */
+	if (flags & TTU_SPLIT_HUGE_PMD)
+		split_huge_pmd_address(vma, address, true, page);
+
+	/*
+	 * For THP, we have to assume the worse case ie pmd for invalidation.
+	 * For hugetlb, it could be much worse if we need to do pud
+	 * invalidation in the case of pmd sharing.
+	 *
+	 * Note that the page can not be free in this function as call of
+	 * try_to_unmap() must hold a reference on the page.
+	 */
+	range.end = PageKsm(page) ?
+			address + PAGE_SIZE : vma_address_end(page, vma);
+	mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,
+				address, range.end);
+	if (PageHuge(page)) {
+		/*
+		 * If sharing is possible, start and end will be adjusted
+		 * accordingly.
+		 */
+		adjust_range_if_pmd_sharing_possible(vma, &range.start,
+						     &range.end);
+	}
+	mmu_notifier_invalidate_range_start(&range);
+
+	while (page_vma_mapped_walk(&pvmw)) {
+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION
+		/* PMD-mapped THP migration entry */
+		if (!pvmw.pte) {
+			VM_BUG_ON_PAGE(PageHuge(page) ||
+				       !PageTransCompound(page), page);
+
+			set_pmd_migration_entry(&pvmw, page);
+			continue;
+		}
+#endif
+
+		/* Unexpected PMD-mapped THP? */
+		VM_BUG_ON_PAGE(!pvmw.pte, page);
+
+		subpage = page - page_to_pfn(page) + pte_pfn(*pvmw.pte);
+		address = pvmw.address;
+
+		if (PageHuge(page) && !PageAnon(page)) {
+			/*
+			 * To call huge_pmd_unshare, i_mmap_rwsem must be
+			 * held in write mode.  Caller needs to explicitly
+			 * do this outside rmap routines.
+			 */
+			VM_BUG_ON(!(flags & TTU_RMAP_LOCKED));
+			if (huge_pmd_unshare(mm, vma, &address, pvmw.pte)) {
+				/*
+				 * huge_pmd_unshare unmapped an entire PMD
+				 * page.  There is no way of knowing exactly
+				 * which PMDs may be cached for this mm, so
+				 * we must flush them all.  start/end were
+				 * already adjusted above to cover this range.
+				 */
+				flush_cache_range(vma, range.start, range.end);
+				flush_tlb_range(vma, range.start, range.end);
+				mmu_notifier_invalidate_range(mm, range.start,
+							      range.end);
+
+				/*
+				 * The ref count of the PMD page was dropped
+				 * which is part of the way map counting
+				 * is done for shared PMDs.  Return 'true'
+				 * here.  When there is no other sharing,
+				 * huge_pmd_unshare returns false and we will
+				 * unmap the actual page and drop map count
+				 * to zero.
+				 */
+				page_vma_mapped_walk_done(&pvmw);
+				break;
+			}
+		}
+
+		/* Nuke the page table entry. */
+		flush_cache_page(vma, address, pte_pfn(*pvmw.pte));
+		pteval = ptep_clear_flush(vma, address, pvmw.pte);
+
+		/* Move the dirty bit to the page. Now the pte is gone. */
+		if (pte_dirty(pteval))
+			set_page_dirty(page);
+
+		/* Update high watermark before we lower rss */
+		update_hiwater_rss(mm);
+
+		if (is_zone_device_page(page)) {
+			swp_entry_t entry;
+			pte_t swp_pte;
+
+			/*
+			 * Store the pfn of the page in a special migration
+			 * pte. do_swap_page() will wait until the migration
+			 * pte is removed and then restart fault handling.
+			 */
+			entry = make_readable_migration_entry(
+							page_to_pfn(page));
+			swp_pte = swp_entry_to_pte(entry);
+
+			/*
+			 * pteval maps a zone device page and is therefore
+			 * a swap pte.
+			 */
+			if (pte_swp_soft_dirty(pteval))
+				swp_pte = pte_swp_mksoft_dirty(swp_pte);
+			if (pte_swp_uffd_wp(pteval))
+				swp_pte = pte_swp_mkuffd_wp(swp_pte);
+			set_pte_at(mm, pvmw.address, pvmw.pte, swp_pte);
+			/*
+			 * No need to invalidate here it will synchronize on
+			 * against the special swap migration pte.
+			 *
+			 * The assignment to subpage above was computed from a
+			 * swap PTE which results in an invalid pointer.
+			 * Since only PAGE_SIZE pages can currently be
+			 * migrated, just set it to page. This will need to be
+			 * changed when hugepage migrations to device private
+			 * memory are supported.
+			 */
+			subpage = page;
+		} else if (PageHWPoison(page)) {
+			pteval = swp_entry_to_pte(make_hwpoison_entry(subpage));
+			if (PageHuge(page)) {
+				hugetlb_count_sub(compound_nr(page), mm);
+				set_huge_swap_pte_at(mm, address,
+						     pvmw.pte, pteval,
+						     vma_mmu_pagesize(vma));
+			} else {
+				dec_mm_counter(mm, mm_counter(page));
+				set_pte_at(mm, address, pvmw.pte, pteval);
+			}
+
+		} else if (pte_unused(pteval) && !userfaultfd_armed(vma)) {
+			/*
+			 * The guest indicated that the page content is of no
+			 * interest anymore. Simply discard the pte, vmscan
+			 * will take care of the rest.
+			 * A future reference will then fault in a new zero
+			 * page. When userfaultfd is active, we must not drop
+			 * this page though, as its main user (postcopy
+			 * migration) will not expect userfaults on already
+			 * copied pages.
+			 */
+			dec_mm_counter(mm, mm_counter(page));
+			/* We have to invalidate as we cleared the pte */
+			mmu_notifier_invalidate_range(mm, address,
+						      address + PAGE_SIZE);
+		} else {
+			swp_entry_t entry;
+			pte_t swp_pte;
+
+			if (arch_unmap_one(mm, vma, address, pteval) < 0) {
+				set_pte_at(mm, address, pvmw.pte, pteval);
+				ret = false;
+				page_vma_mapped_walk_done(&pvmw);
+				break;
+			}
+
+			if (pte_orig_writable(pteval)) {
+				contxt->was_writable_before_shadowed = true;
+			}
+			if (pte_write(pteval)) {
+				contxt->made_writable = true;
+			}
+
+			/*
+			 * Store the pfn of the page in a special migration
+			 * pte. do_swap_page() will wait until the migration
+			 * pte is removed and then restart fault handling.
+			 */
+			if (pte_write(pteval))
+				entry = make_writable_migration_entry(
+							page_to_pfn(subpage));
+			else
+				entry = make_readable_migration_entry(
+							page_to_pfn(subpage));
+
+			swp_pte = swp_entry_to_pte(entry);
+			if (pte_soft_dirty(pteval))
+				swp_pte = pte_swp_mksoft_dirty(swp_pte);
+			if (pte_uffd_wp(pteval))
+				swp_pte = pte_swp_mkuffd_wp(swp_pte);
+			set_pte_at(mm, address, pvmw.pte, swp_pte);
+			/*
+			 * No need to invalidate here it will synchronize on
+			 * against the special swap migration pte.
+			 */
+		}
+
+		/*
+		 * No need to call mmu_notifier_invalidate_range() it has be
+		 * done above for all cases requiring it to happen under page
+		 * table lock before mmu_notifier_invalidate_range_end()
+		 *
+		 * See Documentation/vm/mmu_notifier.rst
+		 */
+		page_remove_rmap(subpage, PageHuge(page));
+		put_page(page);
+	}
+
+	mmu_notifier_invalidate_range_end(&range);
+
+	return ret;
+}
+
+/**
+ * try_to_migrate - try to replace all page table mappings with swap entries
+ * @page: the page to replace page table entries for
+ * @flags: action and flags
+ *
+ * Tries to remove all the page table entries which are mapping this page and
+ * replace them with special swap entries. Caller must hold the page lock.
+ */
+bool nomad_try_to_migrate(struct page *page, enum ttu_flags flags,
+			  struct demote_shadow_page_context *context)
+{
+	struct rmap_walk_control rwc = {
+		.rmap_one = nomad_try_to_migrate_one,
+		.arg = (void *)context,
+		.done = page_not_mapped,
+		.anon_lock = page_lock_anon_vma_read,
+	};
+
+	/*
+	 * Migration always ignores mlock and only supports TTU_RMAP_LOCKED and
+	 * TTU_SPLIT_HUGE_PMD and TTU_SYNC flags.
+	 */
+	if (WARN_ON_ONCE(flags & ~(TTU_RMAP_LOCKED | TTU_SPLIT_HUGE_PMD |
+					TTU_SYNC)))
+		goto out;
+
+	if (is_zone_device_page(page) && !is_device_private_page(page))
+		goto out;
+
+	/*
+	 * During exec, a temporary VMA is setup and later moved.
+	 * The VMA is moved under the anon_vma lock but not the
+	 * page tables leading to a race where migration cannot
+	 * find the migration ptes. Rather than increasing the
+	 * locking requirements of exec(), migration skips
+	 * temporary VMAs until after exec() completes.
+	 */
+	if (!PageKsm(page) && PageAnon(page))
+		rwc.invalid_vma = invalid_migration_vma;
+
+	if (flags & TTU_RMAP_LOCKED)
+		rmap_walk_locked(page, &rwc);
+	else
+		rmap_walk(page, &rwc);
+
+out:
+	return !page_mapcount(page);
+}
+
+// Transit a page that has only one ref count to new page.
+// We only transit the page if the page is clean. If dirty,
+// we keep the original page.
+static bool nomad_try_to_remap_one(struct page *page,
+				   struct vm_area_struct *vma,
+				   unsigned long address, void *arg)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	struct page_vma_mapped_walk pvmw = {
+		.page = page,
+		.vma = vma,
+		.address = address,
+	};
+	pte_t pteval;
+	struct page *subpage;
+	bool ret = true;
+	struct mmu_notifier_range range;
+	struct nomad_remap_status *remap_arg = (struct nomad_remap_status *)arg;
+	enum ttu_flags flags = remap_arg->flags;
+
+	/*
+	 * When racing against e.g. zap_pte_range() on another cpu,
+	 * in between its ptep_get_and_clear_full() and page_remove_rmap(),
+	 * try_to_migrate() may return before page_mapped() has become false,
+	 * if page table locking is skipped: use TTU_SYNC to wait for that.
+	 */
+	if (flags & TTU_SYNC)
+		pvmw.flags = PVMW_SYNC;
+
+	/*
+	 * unmap_page() in mm/huge_memory.c is the only user of migration with
+	 * TTU_SPLIT_HUGE_PMD and it wants to freeze.
+	 */
+	if (flags & TTU_SPLIT_HUGE_PMD)
+		split_huge_pmd_address(vma, address, true, page);
+
+	/*
+	 * For THP, we have to assume the worse case ie pmd for invalidation.
+	 * For hugetlb, it could be much worse if we need to do pud
+	 * invalidation in the case of pmd sharing.
+	 *
+	 * Note that the page can not be free in this function as call of
+	 * try_to_unmap() must hold a reference on the page.
+	 */
+	range.end = PageKsm(page) ?
+			address + PAGE_SIZE : vma_address_end(page, vma);
+	mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,
+				address, range.end);
+	if (PageHuge(page)) {
+		/*
+		 * If sharing is possible, start and end will be adjusted
+		 * accordingly.
+		 */
+		adjust_range_if_pmd_sharing_possible(vma, &range.start,
+						     &range.end);
+	}
+	mmu_notifier_invalidate_range_start(&range);
+
+	while (page_vma_mapped_walk(&pvmw)) {
+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION
+		/* PMD-mapped THP migration entry */
+		if (!pvmw.pte) {
+			VM_BUG_ON_PAGE(PageHuge(page) ||
+				       !PageTransCompound(page), page);
+
+			set_pmd_migration_entry(&pvmw, page);
+			continue;
+		}
+#endif
+
+		/* Unexpected PMD-mapped THP? */
+		VM_BUG_ON_PAGE(!pvmw.pte, page);
+
+		subpage = page - page_to_pfn(page) + pte_pfn(*pvmw.pte);
+		address = pvmw.address;
+
+		if (PageHuge(page) && !PageAnon(page)) {
+			/*
+			 * To call huge_pmd_unshare, i_mmap_rwsem must be
+			 * held in write mode.  Caller needs to explicitly
+			 * do this outside rmap routines.
+			 */
+			VM_BUG_ON(!(flags & TTU_RMAP_LOCKED));
+			if (huge_pmd_unshare(mm, vma, &address, pvmw.pte)) {
+				/*
+				 * huge_pmd_unshare unmapped an entire PMD
+				 * page.  There is no way of knowing exactly
+				 * which PMDs may be cached for this mm, so
+				 * we must flush them all.  start/end were
+				 * already adjusted above to cover this range.
+				 */
+				flush_cache_range(vma, range.start, range.end);
+				flush_tlb_range(vma, range.start, range.end);
+				mmu_notifier_invalidate_range(mm, range.start,
+							      range.end);
+
+				/*
+				 * The ref count of the PMD page was dropped
+				 * which is part of the way map counting
+				 * is done for shared PMDs.  Return 'true'
+				 * here.  When there is no other sharing,
+				 * huge_pmd_unshare returns false and we will
+				 * unmap the actual page and drop map count
+				 * to zero.
+				 */
+				page_vma_mapped_walk_done(&pvmw);
+				break;
+			}
+		}
+
+		/* Nuke the page table entry. */
+		flush_cache_page(vma, address, pte_pfn(*pvmw.pte));
+		pteval = ptep_clear_flush(vma, address, pvmw.pte);
+
+		// this page is unmapped once, ref count decrement by 1
+		put_page(page);
+
+		/* Move the dirty bit to the page. Now the pte is gone. */
+		if (pte_dirty(pteval))
+			set_page_dirty(page);
+
+		/* Update high watermark before we lower rss */
+		update_hiwater_rss(mm);
+
+		if (is_zone_device_page(page)) {
+			swp_entry_t entry;
+			pte_t swp_pte;
+
+			/*
+			 * Store the pfn of the page in a special migration
+			 * pte. do_swap_page() will wait until the migration
+			 * pte is removed and then restart fault handling.
+			 */
+			entry = make_readable_migration_entry(
+							page_to_pfn(page));
+			swp_pte = swp_entry_to_pte(entry);
+
+			/*
+			 * pteval maps a zone device page and is therefore
+			 * a swap pte.
+			 */
+			if (pte_swp_soft_dirty(pteval))
+				swp_pte = pte_swp_mksoft_dirty(swp_pte);
+			if (pte_swp_uffd_wp(pteval))
+				swp_pte = pte_swp_mkuffd_wp(swp_pte);
+			set_pte_at(mm, pvmw.address, pvmw.pte, swp_pte);
+			/*
+			 * No need to invalidate here it will synchronize on
+			 * against the special swap migration pte.
+			 *
+			 * The assignment to subpage above was computed from a
+			 * swap PTE which results in an invalid pointer.
+			 * Since only PAGE_SIZE pages can currently be
+			 * migrated, just set it to page. This will need to be
+			 * changed when hugepage migrations to device private
+			 * memory are supported.
+			 */
+			subpage = page;
+		} else if (PageHWPoison(page)) {
+			pteval = swp_entry_to_pte(make_hwpoison_entry(subpage));
+			if (PageHuge(page)) {
+				hugetlb_count_sub(compound_nr(page), mm);
+				set_huge_swap_pte_at(mm, address,
+						     pvmw.pte, pteval,
+						     vma_mmu_pagesize(vma));
+			} else {
+				dec_mm_counter(mm, mm_counter(page));
+				set_pte_at(mm, address, pvmw.pte, pteval);
+			}
+
+		} else if (pte_unused(pteval) && !userfaultfd_armed(vma)) {
+			/*
+			 * The guest indicated that the page content is of no
+			 * interest anymore. Simply discard the pte, vmscan
+			 * will take care of the rest.
+			 * A future reference will then fault in a new zero
+			 * page. When userfaultfd is active, we must not drop
+			 * this page though, as its main user (postcopy
+			 * migration) will not expect userfaults on already
+			 * copied pages.
+			 */
+			dec_mm_counter(mm, mm_counter(page));
+			/* We have to invalidate as we cleared the pte */
+			mmu_notifier_invalidate_range(mm, address,
+						      address + PAGE_SIZE);
+		} else {
+			// swp_entry_t entry;
+			// pte_t swp_pte;
+
+			if (arch_unmap_one(mm, vma, address, pteval) < 0) {
+				set_pte_at(mm, address, pvmw.pte, pteval);
+				ret = false;
+				page_vma_mapped_walk_done(&pvmw);
+				break;
+			}
+
+			// /*
+			//  * Store the pfn of the page in a special migration
+			//  * pte. do_swap_page() will wait until the migration
+			//  * pte is removed and then restart fault handling.
+			//  */
+			// if (pte_write(pteval))
+			// 	entry = make_writable_migration_entry(
+			// 				page_to_pfn(subpage));
+			// else
+			// 	entry = make_readable_migration_entry(
+			// 				page_to_pfn(subpage));
+
+			// swp_pte = swp_entry_to_pte(entry);
+			// if (pte_soft_dirty(pteval))
+			// 	swp_pte = pte_swp_mksoft_dirty(swp_pte);
+			// if (pte_uffd_wp(pteval))
+			// 	swp_pte = pte_swp_mkuffd_wp(swp_pte);
+			// set_pte_at(mm, address, pvmw.pte, swp_pte);
+			// /*
+			//  * No need to invalidate here it will synchronize on
+			//  * against the special swap migration pte.
+			//  */
+
+			remap_arg->old_pteval = pteval;
+			remap_clean_page(pvmw.page, remap_arg->new_page, &pvmw,
+					 vma, remap_arg);
+		}
+
+		/*
+		 * No need to call mmu_notifier_invalidate_range() it has be
+		 * done above for all cases requiring it to happen under page
+		 * table lock before mmu_notifier_invalidate_range_end()
+		 *
+		 * See Documentation/vm/mmu_notifier.rst
+		 */
+		if (remap_arg->use_new_page) {
+			// if not clean, we will use the original page
+			page_remove_rmap(subpage, PageHuge(page));
+		}
+	}
+
+	mmu_notifier_invalidate_range_end(&range);
+
+	return ret;
+}
+
+bool nomad_try_to_remap(struct page *page, struct nomad_remap_status *context)
+{
+	struct rmap_walk_control rwc = {
+		.rmap_one = nomad_try_to_remap_one,
+		.arg = (void *)context,
+		.done = page_not_mapped,
+		.anon_lock = page_lock_anon_vma_read,
+	};
+	enum ttu_flags flags = (enum ttu_flags)(long)(context->flags);
+
+	/*
+	 * Migration always ignores mlock and only supports TTU_RMAP_LOCKED and
+	 * TTU_SPLIT_HUGE_PMD and TTU_SYNC flags.
+	 */
+	if (WARN_ON_ONCE(flags & ~(TTU_RMAP_LOCKED | TTU_SPLIT_HUGE_PMD |
+					TTU_SYNC)))
+		goto out;
+
+	if (is_zone_device_page(page) && !is_device_private_page(page))
+		goto out;
+
+	/*
+	 * During exec, a temporary VMA is setup and later moved.
+	 * The VMA is moved under the anon_vma lock but not the
+	 * page tables leading to a race where migration cannot
+	 * find the migration ptes. Rather than increasing the
+	 * locking requirements of exec(), migration skips
+	 * temporary VMAs until after exec() completes.
+	 */
+	if (!PageKsm(page) && PageAnon(page))
+		rwc.invalid_vma = invalid_migration_vma;
+
+	if (flags & TTU_RMAP_LOCKED)
+		rmap_walk_locked(page, &rwc);
+	else
+		rmap_walk(page, &rwc);
+
+out:
+	return !page_mapcount(page);
+}
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 264efa022fa9..8877794f01cb 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1239,6 +1239,14 @@ static int free_tail_pages_check(struct page *head_page, struct page *page)
 		 */
 		break;
 	default:
+#ifdef CONFIG_HTMM
+		/* from the third tail page ~  */
+		if (page - head_page < 132) {
+			ClearPageHtmm(page);
+			page->mapping = TAIL_MAPPING;
+			break;
+		}
+#endif
 		if (page->mapping != TAIL_MAPPING) {
 			bad_page(page, "corrupted mapping in tail page");
 			goto out;
@@ -3731,7 +3739,8 @@ struct page *rmqueue(struct zone *preferred_zone,
 	if (test_bit(ZONE_BOOSTED_WATERMARK, &zone->flags)) {
 		clear_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);
 		wakeup_kswapd(zone, 0, 0, zone_idx(zone));
-	}
+	} else if (!pgdat_toptier_balanced(zone->zone_pgdat, order, zone_idx(zone)))
+		wakeup_kswapd(zone, 0, 0, zone_idx(zone));
 
 	VM_BUG_ON_PAGE(page && bad_range(zone, page), page);
 	return page;
@@ -5871,6 +5880,7 @@ void si_meminfo_node(struct sysinfo *val, int nid)
 #endif
 	val->mem_unit = PAGE_SIZE;
 }
+EXPORT_SYMBOL(si_meminfo_node);
 #endif
 
 /*
@@ -7477,6 +7487,11 @@ static void __meminit pgdat_init_internals(struct pglist_data *pgdat)
 
 	pgdat_page_ext_init(pgdat);
 	lruvec_init(&pgdat->__lruvec);
+#ifdef CONFIG_HTMM /* pgdat_init_internals() */
+	pgdat->memcg_htmm_file = NULL;
+	spin_lock_init(&pgdat->kmigraterd_lock);
+	INIT_LIST_HEAD(&pgdat->kmigraterd_head);
+#endif
 }
 
 static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx, int nid,
@@ -8482,6 +8497,22 @@ static void __setup_per_zone_wmarks(void)
 		zone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;
 		zone->_watermark[WMARK_HIGH] = min_wmark_pages(zone) + tmp * 2;
 
+		if (numa_promotion_tiered_enabled) {
+			tmp = mult_frac(zone_managed_pages(zone), demote_scale_factor, 10000);
+
+			/*
+			 * Clamp demote watermark between twice high watermark
+			 * and max managed pages.
+			 */
+			if (tmp < 2 * zone->_watermark[WMARK_HIGH])
+				tmp = 2 * zone->_watermark[WMARK_HIGH];
+			if (tmp > zone_managed_pages(zone))
+				tmp = zone_managed_pages(zone);
+			zone->_watermark[WMARK_DEMOTE] = tmp;
+
+			zone->watermark_boost = 0;
+		}
+
 		spin_unlock_irqrestore(&zone->lock, flags);
 	}
 
@@ -8606,6 +8637,21 @@ int watermark_scale_factor_sysctl_handler(struct ctl_table *table, int write,
 	return 0;
 }
 
+int demote_scale_factor_sysctl_handler(struct ctl_table *table, int write,
+	void __user *buffer, size_t *length, loff_t *ppos)
+{
+	int rc;
+
+	rc = proc_dointvec_minmax(table, write, buffer, length, ppos);
+	if (rc)
+		return rc;
+
+	if (write)
+		setup_per_zone_wmarks();
+
+	return 0;
+}
+
 #ifdef CONFIG_NUMA
 static void setup_min_unmapped_ratio(void)
 {
diff --git a/mm/profile.h b/mm/profile.h
new file mode 100644
index 000000000000..82123681a3e1
--- /dev/null
+++ b/mm/profile.h
@@ -0,0 +1,34 @@
+#ifndef _LINUX_PROFILE_H
+#define _LINUX_PROFILE_H
+#include <linux/sched/cputime.h>
+#include <linux/mm.h>
+#include <linux/cleanup.h>
+
+static inline u64 task_cputime_adjusted_system(struct task_struct *t)
+{
+	u64 utime, stime = 0;
+	task_cputime_adjusted(t, &utime, &stime);
+	return stime;
+}
+struct vmstat_stopwatch {
+	u64 start;
+	enum vm_event_item item;
+};
+static inline struct vmstat_stopwatch
+vmstat_stopwatch_new(enum vm_event_item item)
+{
+	return (struct vmstat_stopwatch){
+		.item = item,
+		.start = task_cputime_adjusted_system(current),
+	};
+}
+static inline void vmstat_stopwatch_drop(struct vmstat_stopwatch *t)
+{
+	count_vm_events(t->item,
+			task_cputime_adjusted_system(current) - t->start);
+}
+DEFINE_CLASS(vmstat_stopwatch, struct vmstat_stopwatch,
+	     vmstat_stopwatch_drop(&_T), vmstat_stopwatch_new(item),
+	     enum vm_event_item item);
+
+#endif // _LINUX_PROFILE_H
diff --git a/mm/rmap.c b/mm/rmap.c
index 330b361a460e..59cc619950d7 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -860,9 +860,10 @@ int page_referenced(struct page *page,
 		    struct mem_cgroup *memcg,
 		    unsigned long *vm_flags)
 {
-	int we_locked = 0;
+	guard(vmstat_stopwatch)(PTEA_SCAN_NS);
+	int we_locked = 0, mapcount = total_mapcount(page);
 	struct page_referenced_arg pra = {
-		.mapcount = total_mapcount(page),
+		.mapcount = mapcount,
 		.memcg = memcg,
 	};
 	struct rmap_walk_control rwc = {
@@ -899,6 +900,7 @@ int page_referenced(struct page *page,
 	if (we_locked)
 		unlock_page(page);
 
+	count_vm_events(PTEA_SCANNED, mapcount);
 	return pra.referenced;
 }
 
@@ -1674,12 +1676,12 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
 	return ret;
 }
 
-static bool invalid_migration_vma(struct vm_area_struct *vma, void *arg)
+bool invalid_migration_vma(struct vm_area_struct *vma, void *arg)
 {
 	return vma_is_temporary_stack(vma);
 }
 
-static int page_not_mapped(struct page *page)
+int page_not_mapped(struct page *page)
 {
 	return !page_mapped(page);
 }
diff --git a/mm/swap.c b/mm/swap.c
index af3cad4e5378..e8fa1548ab11 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -113,6 +113,16 @@ static void __put_compound_page(struct page *page)
 
 void __put_page(struct page *page)
 {
+#ifdef CONFIG_NOMAD
+	// this function is called when the ref count
+	// of a page is dropped to 0, we check if it has
+	// a shadow page and release it
+	if (async_mod_glob_ctrl.initialized &&
+	    async_mod_glob_ctrl.release_shadow_page) {
+		async_mod_glob_ctrl.release_shadow_page(page, NULL, false);
+	}
+#endif
+
 	if (is_zone_device_page(page)) {
 		put_dev_pagemap(page->pgmap);
 
@@ -931,6 +941,15 @@ void release_pages(struct page **pages, int nr)
 		if (!put_page_testzero(page))
 			continue;
 
+#ifdef CONFIG_NOMAD
+		// this is a batch free page
+		if (async_mod_glob_ctrl.initialized &&
+		    async_mod_glob_ctrl.release_shadow_page) {
+			async_mod_glob_ctrl.release_shadow_page(page, NULL,
+								false);
+		}
+#endif
+
 		if (PageCompound(page)) {
 			if (lruvec) {
 				unlock_page_lruvec_irqrestore(lruvec, flags);
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 342a78a8658f..7b7c37c304b9 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -41,6 +41,7 @@
 #include <linux/kthread.h>
 #include <linux/freezer.h>
 #include <linux/memcontrol.h>
+#include <linux/mempolicy.h>
 #include <linux/migrate.h>
 #include <linux/delayacct.h>
 #include <linux/sysctl.h>
@@ -190,6 +191,7 @@ static void set_task_reclaim_state(struct task_struct *task,
 
 static LIST_HEAD(shrinker_list);
 static DECLARE_RWSEM(shrinker_rwsem);
+int demote_scale_factor = 200;
 
 #ifdef CONFIG_MEMCG
 static int shrinker_nr_max;
@@ -588,8 +590,8 @@ unsigned long zone_reclaimable_pages(struct zone *zone)
  * @lru: lru to use
  * @zone_idx: zones to consider (use MAX_NR_ZONES for the whole LRU list)
  */
-static unsigned long lruvec_lru_size(struct lruvec *lruvec, enum lru_list lru,
-				     int zone_idx)
+unsigned long lruvec_lru_size(struct lruvec *lruvec, enum lru_list lru,
+			      int zone_idx)
 {
 	unsigned long size = 0;
 	int zid;
@@ -1219,6 +1221,7 @@ void putback_lru_page(struct page *page)
 	lru_cache_add(page);
 	put_page(page);		/* drop ref from isolate */
 }
+EXPORT_SYMBOL(putback_lru_page);
 
 enum page_references {
 	PAGEREF_RECLAIM,
@@ -1323,6 +1326,18 @@ static struct page *alloc_demote_page(struct page *page, unsigned long node)
 			    __GFP_NOMEMALLOC | GFP_NOWAIT,
 		.nid = node
 	};
+#ifdef CONFIG_NOMAD
+	struct page *newpage =
+		alloc_migration_target(page, (unsigned long)&mtc);
+	if (!newpage && node != first_memory_node &&
+	    async_mod_glob_ctrl.initialized &&
+	    async_mod_glob_ctrl.reclaim_page) {
+		// reclaim 10 pages each time
+		async_mod_glob_ctrl.reclaim_page(node, 10);
+		newpage = alloc_migration_target(page, (unsigned long)&mtc);
+	}
+	return newpage;
+#endif
 
 	return alloc_migration_target(page, (unsigned long)&mtc);
 }
@@ -1335,9 +1350,12 @@ static struct page *alloc_demote_page(struct page *page, unsigned long node)
 static unsigned int demote_page_list(struct list_head *demote_pages,
 				     struct pglist_data *pgdat)
 {
+	guard(vmstat_stopwatch)(DEMOTE_NS);
 	int target_nid = next_demotion_node(pgdat->node_id);
 	unsigned int nr_succeeded;
 	int err;
+	bool file_lru;
+	__auto_type *migrate_pages_fn = migrate_pages;
 
 	if (list_empty(demote_pages))
 		return 0;
@@ -1345,16 +1363,28 @@ static unsigned int demote_page_list(struct list_head *demote_pages,
 	if (target_nid == NUMA_NO_NODE)
 		return 0;
 
+	file_lru = page_is_file_lru(lru_to_page(demote_pages));
+
+#ifdef CONFIG_NOMAD
+	if (async_mod_glob_ctrl.initialized)
+		migrate_pages_fn = nomad_demotion_migrate_pages;
+#endif
+
 	/* Demotion ignores all cpuset and mempolicy settings */
-	err = migrate_pages(demote_pages, alloc_demote_page, NULL,
-			    target_nid, MIGRATE_ASYNC, MR_DEMOTION,
-			    &nr_succeeded);
+	err = migrate_pages_fn(demote_pages, alloc_demote_page, NULL,
+			       target_nid, MIGRATE_ASYNC, MR_DEMOTION,
+			       &nr_succeeded);
 
 	if (current_is_kswapd())
 		__count_vm_events(PGDEMOTE_KSWAPD, nr_succeeded);
 	else
 		__count_vm_events(PGDEMOTE_DIRECT, nr_succeeded);
 
+	if (file_lru)
+		__count_vm_events(PGDEMOTE_FILE, nr_succeeded);
+	else
+		__count_vm_events(PGDEMOTE_ANON, nr_succeeded);
+
 	return nr_succeeded;
 }
 
@@ -2042,6 +2072,7 @@ int isolate_lru_page(struct page *page)
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(isolate_lru_page);
 
 /*
  * A direct reclaimer may isolate SWAP_CLUSTER_MAX pages from the LRU list and
@@ -2086,8 +2117,7 @@ static int too_many_isolated(struct pglist_data *pgdat, int file,
  *
  * Returns the number of pages moved to the given lruvec.
  */
-static unsigned int move_pages_to_lru(struct lruvec *lruvec,
-				      struct list_head *list)
+unsigned int move_pages_to_lru(struct lruvec *lruvec, struct list_head *list)
 {
 	int nr_pages, nr_moved = 0;
 	LIST_HEAD(pages_to_free);
@@ -2519,8 +2549,14 @@ static void get_scan_count(struct lruvec *lruvec, struct scan_control *sc,
 	unsigned long ap, fp;
 	enum lru_list lru;
 
-	/* If we have no swap space, do not bother scanning anon pages. */
-	if (!sc->may_swap || !can_reclaim_anon_pages(memcg, pgdat->node_id, sc)) {
+	/*
+	 * If we have no swap space, do not bother scanning anon pages.
+	 * However, anon pages on toptier node can be demoted via reclaim
+	 * when numa promotion is enabled. Disable the check to prevent
+	 * demotion for no swap space when numa promotion is enabled.
+	 */
+	if (!numa_promotion_tiered_enabled &&
+		(!sc->may_swap || !can_reclaim_anon_pages(memcg, pgdat->node_id, sc))) {
 		scan_balance = SCAN_FILE;
 		goto out;
 	}
@@ -2720,6 +2756,7 @@ static bool can_age_anon_pages(struct pglist_data *pgdat,
 
 static void shrink_lruvec(struct lruvec *lruvec, struct scan_control *sc)
 {
+	guard(vmstat_stopwatch)(LRU_ROTATE_NS);
 	unsigned long nr[NR_LRU_LISTS];
 	unsigned long targets[NR_LRU_LISTS];
 	unsigned long nr_to_scan;
@@ -2749,6 +2786,13 @@ static void shrink_lruvec(struct lruvec *lruvec, struct scan_control *sc)
 				sc->priority == DEF_PRIORITY);
 
 	blk_start_plug(&plug);
+#ifdef CONFIG_NOMAD
+	if (async_mod_glob_ctrl.initialized &&
+	    async_mod_glob_ctrl.reclaim_page) {
+		async_mod_glob_ctrl.reclaim_page(lruvec->pgdat->node_id,
+						 nr_to_reclaim / 2);
+	}
+#endif
 	while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||
 					nr[LRU_INACTIVE_FILE]) {
 		unsigned long nr_anon, nr_file, percentage;
@@ -3053,7 +3097,10 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 			if (!managed_zone(zone))
 				continue;
 
-			total_high_wmark += high_wmark_pages(zone);
+			if (numa_promotion_tiered_enabled && node_is_toptier(pgdat->node_id))
+				total_high_wmark += demote_wmark_pages(zone);
+			else
+				total_high_wmark += high_wmark_pages(zone);
 		}
 
 		/*
@@ -3710,6 +3757,10 @@ static bool pgdat_balanced(pg_data_t *pgdat, int order, int highest_zoneidx)
 	unsigned long mark = -1;
 	struct zone *zone;
 
+	if (numa_promotion_tiered_enabled && node_is_toptier(pgdat->node_id) &&
+			highest_zoneidx >= ZONE_NORMAL)
+		return pgdat_toptier_balanced(pgdat, 0, highest_zoneidx);
+
 	/*
 	 * Check watermarks bottom-up as lower zones are more likely to
 	 * meet watermarks.
@@ -3736,6 +3787,30 @@ static bool pgdat_balanced(pg_data_t *pgdat, int order, int highest_zoneidx)
 	return false;
 }
 
+bool pgdat_toptier_balanced(pg_data_t *pgdat, int order, int zone_idx)
+{
+	unsigned long mark;
+	struct zone *zone;
+
+	if (!node_is_toptier(pgdat->node_id) ||
+		!numa_promotion_tiered_enabled ||
+		order > 0 || zone_idx < ZONE_NORMAL) {
+		return true;
+	}
+
+	zone = pgdat->node_zones + ZONE_NORMAL;
+
+	if (!managed_zone(zone))
+		return true;
+
+	mark = min(demote_wmark_pages(zone), zone_managed_pages(zone));
+
+	if (zone_page_state(zone, NR_FREE_PAGES) < mark)
+		return false;
+
+	return true;
+}
+
 /* Clear pgdat state for congested, dirty or under writeback. */
 static void clear_pgdat_congested(pg_data_t *pgdat)
 {
@@ -3804,7 +3879,10 @@ static bool kswapd_shrink_node(pg_data_t *pgdat,
 		if (!managed_zone(zone))
 			continue;
 
-		sc->nr_to_reclaim += max(high_wmark_pages(zone), SWAP_CLUSTER_MAX);
+		if (numa_promotion_tiered_enabled && node_is_toptier(pgdat->node_id))
+			sc->nr_to_reclaim += max(demote_wmark_pages(zone), SWAP_CLUSTER_MAX);
+		else
+			sc->nr_to_reclaim += max(high_wmark_pages(zone), SWAP_CLUSTER_MAX);
 	}
 
 	/*
@@ -4168,8 +4246,23 @@ static void kswapd_try_to_sleep(pg_data_t *pgdat, int alloc_order, int reclaim_o
 		 */
 		set_pgdat_percpu_threshold(pgdat, calculate_normal_threshold);
 
-		if (!kthread_should_stop())
-			schedule();
+		if (!kthread_should_stop()) {
+			/*
+			 * In numa promotion modes, try harder to recover from
+			 * kswapd failures, because direct reclaiming may be
+			 * not triggered.
+			 */
+			if (numa_promotion_tiered_enabled &&
+						node_is_toptier(pgdat->node_id) &&
+					pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES) {
+				remaining = schedule_timeout(10 * HZ);
+				if (!remaining) {
+					pgdat->kswapd_highest_zoneidx = ZONE_MOVABLE;
+					pgdat->kswapd_order = 0;
+				}
+			} else
+				schedule();
+		}
 
 		set_pgdat_percpu_threshold(pgdat, calculate_pressure_threshold);
 	} else {
diff --git a/mm/vmstat.c b/mm/vmstat.c
index 8ce2620344b2..80eafa3cb723 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -28,6 +28,7 @@
 #include <linux/mm_inline.h>
 #include <linux/page_ext.h>
 #include <linux/page_owner.h>
+#include <linux/migrate.h>
 
 #include "internal.h"
 
@@ -135,6 +136,32 @@ void all_vm_events(unsigned long *ret)
 }
 EXPORT_SYMBOL_GPL(all_vm_events);
 
+void clear_all_vm_events(void)
+{
+	int cpu;
+
+	for_each_online_cpu(cpu) {
+		struct vm_event_state *this = &per_cpu(vm_event_states, cpu);
+		int sz = NR_VM_EVENT_ITEMS * sizeof(unsigned long);
+
+		memset(this->event, 0, sz);
+	}
+}
+EXPORT_SYMBOL_GPL(clear_all_vm_events);
+
+/*
+ * This is the node to reset all vm events in /proc/vmstat
+ * via /proc/sys/vm/clear_all_vm_events
+ */
+int sysctl_clearvmevents_handler(struct ctl_table *table, int write,
+	void __user *buffer, size_t *length, loff_t *ppos)
+{
+	if (write)
+		clear_all_vm_events();
+
+	return 0;
+}
+
 /*
  * Fold the foreign cpu events into our own.
  *
@@ -1267,6 +1294,8 @@ const char * const vmstat_text[] = {
 	"pgsteal_direct",
 	"pgdemote_kswapd",
 	"pgdemote_direct",
+	"pgdemote_file",
+	"pgdemote_anon",
 	"pgscan_kswapd",
 	"pgscan_direct",
 	"pgscan_direct_throttle",
@@ -1297,14 +1326,36 @@ const char * const vmstat_text[] = {
 	"numa_hint_faults",
 	"numa_hint_faults_local",
 	"numa_pages_migrated",
+	"pgpromote_candidate",
+	"pgpromote_candidate_demoted",
+	"pgpromote_candidate_anon",
+	"pgpromote_candidate_file",
+	"pgpromote_tried",
+	"pgpromote_file",
+	"pgpromote_anon",
 #endif
 #ifdef CONFIG_MIGRATION
 	"pgmigrate_success",
 	"pgmigrate_fail",
+	"pgmigrate_fail_dst_node_full",
+	"pgmigrate_fail_numa_isolate",
+	"pgmigrate_fail_nomem",
+	"pgmigrate_fail_refcount",
 	"thp_migration_success",
 	"thp_migration_fail",
 	"thp_migration_split",
 #endif
+	"pebs_nr_sampled",
+	"pebs_nr_sampled_fmem",
+	"pebs_nr_sampled_smem",
+	"ptea_scan_ns",
+	"ptea_scanned",
+	"lru_rotate_ns",
+	"demote_ns",
+	"hint_fault_ns",
+	"promote_ns",
+	"sampling_ns",
+	"ptext_ns",
 #ifdef CONFIG_COMPACTION
 	"compact_migrate_scanned",
 	"compact_free_scanned",
@@ -1361,6 +1412,15 @@ const char * const vmstat_text[] = {
 #ifdef CONFIG_BALLOON_COMPACTION
 	"balloon_migrate",
 #endif
+#ifdef CONFIG_HTMM
+	"htmm_nr_promoted",
+	"htmm_nr_demoted",
+	"htmm_missed_dramread",
+	"htmm_missed_nvmread",
+	"htmm_missed_write",
+	"htmm_alloc_dram",
+	"htmm_alloc_nvm",
+#endif
 #endif /* CONFIG_MEMORY_BALLOON */
 #ifdef CONFIG_DEBUG_TLBFLUSH
 	"nr_tlb_remote_flush",
@@ -1642,7 +1702,9 @@ static void zoneinfo_show_print(struct seq_file *m, pg_data_t *pgdat,
 							struct zone *zone)
 {
 	int i;
-	seq_printf(m, "Node %d, zone %8s", pgdat->node_id, zone->name);
+	seq_printf(m, "Node %d, zone %8s, toptier %d next_demotion_node %d",
+			pgdat->node_id, zone->name, node_is_toptier(pgdat->node_id),
+			next_demotion_node(pgdat->node_id));
 	if (is_zone_first_populated(pgdat, zone)) {
 		seq_printf(m, "\n  per-node stats");
 		for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
@@ -1659,6 +1721,7 @@ static void zoneinfo_show_print(struct seq_file *m, pg_data_t *pgdat,
 		   "\n        min      %lu"
 		   "\n        low      %lu"
 		   "\n        high     %lu"
+		   "\n        demote   %lu"
 		   "\n        spanned  %lu"
 		   "\n        present  %lu"
 		   "\n        managed  %lu"
@@ -1667,6 +1730,7 @@ static void zoneinfo_show_print(struct seq_file *m, pg_data_t *pgdat,
 		   min_wmark_pages(zone),
 		   low_wmark_pages(zone),
 		   high_wmark_pages(zone),
+		   node_is_toptier(pgdat->node_id) ? demote_wmark_pages(zone) : 0,
 		   zone->spanned_pages,
 		   zone->present_pages,
 		   zone_managed_pages(zone),
diff --git a/scripts/Makefile.lib b/scripts/Makefile.lib
index 0a8a4689c3eb..91875f11b613 100644
--- a/scripts/Makefile.lib
+++ b/scripts/Makefile.lib
@@ -474,7 +474,7 @@ quiet_cmd_xzmisc = XZMISC  $@
 # be used because it would require zstd to allocate a 128 MB buffer.
 
 quiet_cmd_zstd = ZSTD    $@
-      cmd_zstd = { cat $(real-prereqs) | $(ZSTD) -19; $(size_append); } > $@
+      cmd_zstd = { cat $(real-prereqs) | $(ZSTD) -T0; $(size_append); } > $@
 
 quiet_cmd_zstd22 = ZSTD22  $@
       cmd_zstd22 = { cat $(real-prereqs) | $(ZSTD) -22 --ultra; $(size_append); } > $@
