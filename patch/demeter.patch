 .clang-format                          |    3 +
 Makefile                               |    4 +-
 arch/x86/boot/compressed/Makefile      |    2 +-
 arch/x86/entry/syscalls/syscall_64.tbl |    3 +
 arch/x86/events/core.c                 |    5 +-
 arch/x86/events/intel/core.c           |    6 +
 arch/x86/events/intel/ds.c             |   29 +-
 arch/x86/events/perf_event.h           |    1 +
 arch/x86/include/asm/intel_ds.h        |    4 +-
 arch/x86/include/asm/kvm_host.h        |    1 +
 arch/x86/kvm/pmu.c                     |   15 +
 arch/x86/kvm/pmu.h                     |    2 +-
 arch/x86/kvm/vmx/pmu_intel.c           |    7 +
 arch/x86/kvm/vmx/vmx.c                 |   24 +-
 arch/x86/kvm/x86.c                     |    1 +
 drivers/virtio/virtio_balloon.c        |   22 +
 include/linux/nodemask.h               |    6 +
 include/linux/stddef.h                 |    5 -
 include/linux/types.h                  |   21 +-
 include/linux/vm_event_item.h          |   15 +
 include/linux/vmstat.h                 |    7 +
 include/uapi/linux/perf_event.h        |    7 +
 kernel/events/core.c                   |    1 +
 kernel/kthread.c                       |    1 +
 kernel/pid.c                           |    1 +
 kernel/sysctl.c                        |    9 +
 kernel/trace/ring_buffer.c             |  119 ++
 mm/Kconfig                             |    2 +
 mm/Makefile                            |    4 +
 mm/exchange.c                          | 1170 +++++++++++
 mm/exchange_test.c                     |  564 +++++
 mm/gup.c                               |    1 +
 mm/demeter/Kconfig                      |   21 +
 mm/demeter/Makefile                     |   14 +
 mm/demeter/balloon.c                    |  839 ++++++++
 mm/demeter/core.c                       |  908 ++++++++
 mm/demeter/cwisstable.h                 | 3537 ++++++++++++++++++++++++++++++++
 mm/demeter/error.h                      |   82 +
 mm/demeter/demeter.h                     |   22 +
 mm/demeter/hashmap.h                    |   64 +
 mm/demeter/module.c                     |  154 ++
 mm/demeter/module.h                     |   60 +
 mm/demeter/mpsc.h                       |   91 +
 mm/demeter/pebs.h                       |   16 +
 mm/demeter/range_tree.h                 |  335 +++
 mm/demeter/sysfs.c                      |  257 +++
 mm/demeter/vector.c                     |  140 ++
 mm/demeter/vector.h                     |   54 +
 mm/migrate.c                           |    8 +-
 mm/mm_init.c                           |    1 +
 mm/shmem.c                             |    1 +
 mm/show_mem.c                          |    1 +
 mm/swap.c                              |    1 +
 mm/vmscan.c                            |    2 +
 mm/vmstat.c                            |   41 +
 scripts/Makefile.lib                   |    3 +
 56 files changed, 8664 insertions(+), 50 deletions(-)

diff --git a/.clang-format b/.clang-format
index ccc9b93972a9..67db0069a01a 100644
--- a/.clang-format
+++ b/.clang-format
@@ -687,6 +687,9 @@ ForEachMacros:
   - 'xbc_node_for_each_key_value'
   - 'xbc_node_for_each_subkey'
   - 'zorro_for_each_dev'
+  - 'mpsc_for_each'
+  - 'vma_for_each'
+  - 'folio_for_each'
 
 IncludeBlocks: Preserve
 IncludeCategories:
diff --git a/Makefile b/Makefile
index 3d10e3aadeda..76d90e50a95e 100644
--- a/Makefile
+++ b/Makefile
@@ -453,7 +453,7 @@ HOSTRUSTC = rustc
 HOSTPKG_CONFIG	= pkg-config
 
 KBUILD_USERHOSTCFLAGS := -Wall -Wmissing-prototypes -Wstrict-prototypes \
-			 -O2 -fomit-frame-pointer -std=gnu11
+			 -O2 -fomit-frame-pointer -std=gnu23
 KBUILD_USERCFLAGS  := $(KBUILD_USERHOSTCFLAGS) $(USERCFLAGS)
 KBUILD_USERLDFLAGS := $(USERLDFLAGS)
 
@@ -559,7 +559,7 @@ LINUXINCLUDE    := \
 KBUILD_AFLAGS   := -D__ASSEMBLY__ -fno-PIE
 
 KBUILD_CFLAGS :=
-KBUILD_CFLAGS += -std=gnu11
+KBUILD_CFLAGS += -std=gnu23
 KBUILD_CFLAGS += -fshort-wchar
 KBUILD_CFLAGS += -funsigned-char
 KBUILD_CFLAGS += -fno-common
diff --git a/arch/x86/boot/compressed/Makefile b/arch/x86/boot/compressed/Makefile
index f2051644de94..3b1355da2145 100644
--- a/arch/x86/boot/compressed/Makefile
+++ b/arch/x86/boot/compressed/Makefile
@@ -136,7 +136,7 @@ $(obj)/vmlinux.bin.lzo: $(vmlinux.bin.all-y) FORCE
 $(obj)/vmlinux.bin.lz4: $(vmlinux.bin.all-y) FORCE
 	$(call if_changed,lz4_with_size)
 $(obj)/vmlinux.bin.zst: $(vmlinux.bin.all-y) FORCE
-	$(call if_changed,zstd22_with_size)
+	$(call if_changed,zstd_with_size)
 
 suffix-$(CONFIG_KERNEL_GZIP)	:= gz
 suffix-$(CONFIG_KERNEL_BZIP2)	:= bz2
diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index a396f6e6ab5b..786d7c9a8cd5 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -385,6 +385,9 @@
 461	common	lsm_list_modules	sys_lsm_list_modules
 462 	common  mseal			sys_mseal
 
+510	64	count_node_folios	sys_count_node_folios
+511	64	exchange_folios	sys_exchange_folios
+
 #
 # Due to a historical design error, certain syscalls are numbered differently
 # in x32 as compared to native x86_64.  These syscalls have numbers 512-547.
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 5b0dd07b1ef1..bd90b87125bd 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -1365,8 +1365,9 @@ DEFINE_PER_CPU(u64 [X86_PMC_IDX_MAX], pmc_prev_left);
 int x86_perf_event_set_period(struct perf_event *event)
 {
 	struct hw_perf_event *hwc = &event->hw;
-	s64 left = local64_read(&hwc->period_left);
-	s64 period = hwc->sample_period;
+	int shift = 64 - x86_pmu.cntval_bits;
+	s64 left = (s64)(local64_read(&hwc->period_left) << shift) >> shift;
+	s64 period = (s64)(hwc->sample_period << shift) >> shift;
 	int ret = 0, idx = hwc->idx;
 
 	if (unlikely(!hwc->event_base))
diff --git a/arch/x86/events/intel/core.c b/arch/x86/events/intel/core.c
index 38c1b1f1deaa..1f1a437e65a3 100644
--- a/arch/x86/events/intel/core.c
+++ b/arch/x86/events/intel/core.c
@@ -4172,6 +4172,12 @@ static struct perf_guest_switch_msr *intel_guest_get_msrs(int *nr, void *data)
 		};
 	}
 
+	arr[(*nr)++] = (struct perf_guest_switch_msr){
+		.msr = MSR_PEBS_LD_LAT_THRESHOLD,
+		.host = (unsigned long)cpuc->pebs_load_latency_threshold,
+		.guest = kvm_pmu->pebs_load_latency_threshold,
+	};
+
 	pebs_enable = (*nr)++;
 	arr[pebs_enable] = (struct perf_guest_switch_msr){
 		.msr = MSR_IA32_PEBS_ENABLE,
diff --git a/arch/x86/events/intel/ds.c b/arch/x86/events/intel/ds.c
index e010bfed8417..e54adda89628 100644
--- a/arch/x86/events/intel/ds.c
+++ b/arch/x86/events/intel/ds.c
@@ -844,12 +844,13 @@ int intel_pmu_drain_bts_buffer(void)
 	return 1;
 }
 
-static inline void intel_pmu_drain_pebs_buffer(void)
+void intel_pmu_drain_pebs_buffer(void)
 {
 	struct perf_sample_data data;
 
 	x86_pmu.drain_pebs(NULL, &data);
 }
+EXPORT_SYMBOL_GPL(intel_pmu_drain_pebs_buffer);
 
 /*
  * PEBS
@@ -1178,12 +1179,6 @@ static void adaptive_pebs_record_size_update(void)
 	cpuc->pebs_record_size = sz;
 }
 
-#define PERF_PEBS_MEMINFO_TYPE	(PERF_SAMPLE_ADDR | PERF_SAMPLE_DATA_SRC |   \
-				PERF_SAMPLE_PHYS_ADDR |			     \
-				PERF_SAMPLE_WEIGHT_TYPE |		     \
-				PERF_SAMPLE_TRANSACTION |		     \
-				PERF_SAMPLE_DATA_PAGE_SIZE)
-
 static u64 pebs_update_adaptive_cfg(struct perf_event *event)
 {
 	struct perf_event_attr *attr = &event->attr;
@@ -1347,6 +1342,10 @@ void intel_pmu_pebs_enable(struct perf_event *event)
 	else if (event->hw.flags & PERF_X86_EVENT_PEBS_ST)
 		cpuc->pebs_enabled |= 1ULL << 63;
 
+	if (hwc->extra_reg.reg == MSR_PEBS_LD_LAT_THRESHOLD &&
+	    hwc->extra_reg.config)
+		cpuc->pebs_load_latency_threshold = hwc->extra_reg.config;
+
 	if (x86_pmu.intel_cap.pebs_baseline) {
 		hwc->config |= ICL_EVENTSEL_ADAPTIVE;
 		if (pebs_data_cfg != cpuc->active_pebs_data_cfg) {
@@ -2049,7 +2048,7 @@ __intel_pmu_pebs_event(struct perf_event *event,
 	struct x86_perf_regs perf_regs;
 	struct pt_regs *regs = &perf_regs.regs;
 	void *at = get_next_pebs_record_by_bit(base, top, bit);
-	static struct pt_regs dummy_iregs;
+	struct pt_regs dummy_iregs;
 
 	if (hwc->flags & PERF_X86_EVENT_AUTO_RELOAD) {
 		/*
@@ -2065,9 +2064,17 @@ __intel_pmu_pebs_event(struct perf_event *event,
 	if (!iregs)
 		iregs = &dummy_iregs;
 
+	// We shoule not hardcode the overflow_handler perf_event_output
+	perf_overflow_handler_t	overflow_handler = READ_ONCE(event->overflow_handler);
+	// In setup_pebs_adaptive_sample_data(), regs is only useful
+	// when PERF_SAMPLE_REGS_INTR is set. Borrow it to store index.
+	bool batch_index = iregs == &dummy_iregs &&
+		    !(event->attr.sample_type & PERF_SAMPLE_REGS_INTR);
 	while (count > 1) {
 		setup_sample(event, iregs, at, data, regs);
-		perf_event_output(event, data, regs);
+		if (batch_index)
+			regs->cx = count - 1;
+		overflow_handler(event, data, regs);
 		at += cpuc->pebs_record_size;
 		at = get_next_pebs_record_by_bit(at, top, bit);
 		count--;
@@ -2081,7 +2088,9 @@ __intel_pmu_pebs_event(struct perf_event *event,
 		 * last record the same as other PEBS records, and doesn't
 		 * invoke the generic overflow handler.
 		 */
-		perf_event_output(event, data, regs);
+		if (batch_index)
+			regs->cx = count - 1;
+		overflow_handler(event, data, regs);
 	} else {
 		/*
 		 * All but the last records are processed.
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index 72b022a1e16c..5e9f59dd710e 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -276,6 +276,7 @@ struct cpu_hw_events {
 	u64			pebs_data_cfg;
 	u64			active_pebs_data_cfg;
 	int			pebs_record_size;
+	u64			pebs_load_latency_threshold;
 
 	/* Intel Fixed counter configuration */
 	u64			fixed_ctrl_val;
diff --git a/arch/x86/include/asm/intel_ds.h b/arch/x86/include/asm/intel_ds.h
index 2f9eeb5c3069..b538fbe77fda 100644
--- a/arch/x86/include/asm/intel_ds.h
+++ b/arch/x86/include/asm/intel_ds.h
@@ -3,8 +3,8 @@
 
 #include <linux/percpu-defs.h>
 
-#define BTS_BUFFER_SIZE		(PAGE_SIZE << 4)
-#define PEBS_BUFFER_SIZE	(PAGE_SIZE << 4)
+#define BTS_BUFFER_SIZE (PAGE_SIZE << 10)
+#define PEBS_BUFFER_SIZE (PAGE_SIZE << 10)
 
 /* The maximal number of PEBS events: */
 #define MAX_PEBS_EVENTS_FMT4	8
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f8ca74e7678f..033877ecc4c6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -574,6 +574,7 @@ struct kvm_pmu {
 	u64 pebs_enable_mask;
 	u64 pebs_data_cfg;
 	u64 pebs_data_cfg_mask;
+	u64 pebs_load_latency_threshold;
 
 	/*
 	 * If a guest counter is cross-mapped to host counter with different
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index a593b03c9aed..1d5c9e1e0a99 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -210,6 +210,21 @@ static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 		 * in the PEBS record is calibrated on the guest side.
 		 */
 		attr.precise_ip = pmc_get_pebs_precise_level(pmc);
+
+		/*
+		 * Ad-hoc patching of extra register needed by load latency event:
+		 * Need to set MSR_PEBS_LD_LAT_THRESHOLD via attr.config1
+		 * for MEM_TANS_RETIRED.LOAD_LATENCY_GT_*
+		 * see: x86_pmu_extra_regs() and intel_icl_extra_regs
+		 */
+		if (0x01cd == (attr.config & (ARCH_PERFMON_EVENTSEL_EVENT |
+					      ARCH_PERFMON_EVENTSEL_UMASK)) &&
+		    pmu->pebs_load_latency_threshold) {
+			attr.config1 = pmu->pebs_load_latency_threshold;
+			attr.precise_ip = 3;
+		}
+		if (pmu->pebs_data_cfg & PEBS_DATACFG_MEMINFO)
+			attr.sample_type |= PERF_PEBS_MEMINFO_TYPE;
 	}
 
 	event = perf_event_create_kernel_counter(&attr, -1, current,
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index 4d52b0b539ba..0099fdcb2194 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -213,7 +213,7 @@ static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 		return;
 	}
 
-	kvm_pmu_cap.version = min(kvm_pmu_cap.version, 2);
+	// kvm_pmu_cap.version = min(kvm_pmu_cap.version, 5);
 	kvm_pmu_cap.num_counters_gp = min(kvm_pmu_cap.num_counters_gp,
 					  pmu_ops->MAX_NR_GP_COUNTERS);
 	kvm_pmu_cap.num_counters_fixed = min(kvm_pmu_cap.num_counters_fixed,
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index be40474de6e4..ffd2fa5dc0d6 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -163,6 +163,7 @@ static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 		ret = guest_cpuid_has(vcpu, X86_FEATURE_DS);
 		break;
 	case MSR_PEBS_DATA_CFG:
+	case MSR_PEBS_LD_LAT_THRESHOLD:
 		perf_capabilities = vcpu_get_perf_capabilities(vcpu);
 		ret = (perf_capabilities & PERF_CAP_PEBS_BASELINE) &&
 			((perf_capabilities & PERF_CAP_PEBS_FORMAT) > 3);
@@ -314,6 +315,9 @@ static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	case MSR_PEBS_DATA_CFG:
 		msr_info->data = pmu->pebs_data_cfg;
 		break;
+	case MSR_PEBS_LD_LAT_THRESHOLD:
+		msr_info->data = pmu->pebs_load_latency_threshold;
+		break;
 	default:
 		if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
 		    (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
@@ -376,6 +380,9 @@ static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 
 		pmu->pebs_data_cfg = data;
 		break;
+	case MSR_PEBS_LD_LAT_THRESHOLD:
+		pmu->pebs_load_latency_threshold = data;
+		break;
 	default:
 		if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
 		    (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index b3c83c06f826..3816f2dc00d8 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -7940,28 +7940,8 @@ static __init u64 vmx_get_perf_capabilities(void)
 
 	if (vmx_pebs_supported()) {
 		perf_cap |= host_perf_cap & PERF_CAP_PEBS_MASK;
-
-		/*
-		 * Disallow adaptive PEBS as it is functionally broken, can be
-		 * used by the guest to read *host* LBRs, and can be used to
-		 * bypass userspace event filters.  To correctly and safely
-		 * support adaptive PEBS, KVM needs to:
-		 *
-		 * 1. Account for the ADAPTIVE flag when (re)programming fixed
-		 *    counters.
-		 *
-		 * 2. Gain support from perf (or take direct control of counter
-		 *    programming) to support events without adaptive PEBS
-		 *    enabled for the hardware counter.
-		 *
-		 * 3. Ensure LBR MSRs cannot hold host data on VM-Entry with
-		 *    adaptive PEBS enabled and MSR_PEBS_DATA_CFG.LBRS=1.
-		 *
-		 * 4. Document which PMU events are effectively exposed to the
-		 *    guest via adaptive PEBS, and make adaptive PEBS mutually
-		 *    exclusive with KVM_SET_PMU_EVENT_FILTER if necessary.
-		 */
-		perf_cap &= ~PERF_CAP_PEBS_BASELINE;
+		if ((perf_cap & PERF_CAP_PEBS_FORMAT) < 4)
+			perf_cap &= ~PERF_CAP_PEBS_BASELINE;
 	}
 
 	return perf_cap;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 0763a0f72a06..74ab82778d8b 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -1463,6 +1463,7 @@ static const u32 msrs_to_save_pmu[] = {
 	MSR_CORE_PERF_FIXED_CTR_CTRL, MSR_CORE_PERF_GLOBAL_STATUS,
 	MSR_CORE_PERF_GLOBAL_CTRL, MSR_CORE_PERF_GLOBAL_OVF_CTRL,
 	MSR_IA32_PEBS_ENABLE, MSR_IA32_DS_AREA, MSR_PEBS_DATA_CFG,
+	MSR_PEBS_LD_LAT_THRESHOLD,
 
 	/* This part of MSRs should match KVM_INTEL_PMC_MAX_GENERIC. */
 	MSR_ARCH_PERFMON_PERFCTR0, MSR_ARCH_PERFMON_PERFCTR1,
diff --git a/drivers/virtio/virtio_balloon.c b/drivers/virtio/virtio_balloon.c
index c0a63638f95e..7e1338d7a676 100644
--- a/drivers/virtio/virtio_balloon.c
+++ b/drivers/virtio/virtio_balloon.c
@@ -134,6 +134,25 @@ static const struct virtio_device_id id_table[] = {
 	{ 0 },
 };
 
+struct virtio_balloon *__global_instance = NULL;
+ulong node_avail_pages(int nid) {
+	struct virtio_balloon *vb = __global_instance;
+	if (!vb)
+		return node_present_pages(nid);
+
+	struct balloon_dev_info *vb_dev_info = &vb->vb_dev_info;
+	struct page *page;
+	unsigned long flags;
+	size_t n_pages = 0;
+	spin_lock_irqsave(&vb_dev_info->pages_lock, flags);
+	list_for_each_entry(page, &vb_dev_info->pages, lru)
+		if (page_to_nid(page) == nid)
+			n_pages += VIRTIO_BALLOON_PAGES_PER_PAGE;
+	spin_unlock_irqrestore(&vb_dev_info->pages_lock, flags);
+	return node_present_pages(nid) - n_pages;
+}
+EXPORT_SYMBOL_GPL(node_avail_pages);
+
 static u32 page_to_balloon_pfn(struct page *page)
 {
 	unsigned long pfn = page_to_pfn(page);
@@ -1059,6 +1078,7 @@ static int virtballoon_probe(struct virtio_device *vdev)
 
 	if (towards_target(vb))
 		virtballoon_changed(vdev);
+	__global_instance = vb;
 	return 0;
 
 out_unregister_oom:
@@ -1074,6 +1094,7 @@ static int virtballoon_probe(struct virtio_device *vdev)
 	vdev->config->del_vqs(vdev);
 out_free_vb:
 	kfree(vb);
+	__global_instance = NULL;
 out:
 	return err;
 }
@@ -1098,6 +1119,7 @@ static void remove_common(struct virtio_balloon *vb)
 static void virtballoon_remove(struct virtio_device *vdev)
 {
 	struct virtio_balloon *vb = vdev->priv;
+	__global_instance = NULL;
 
 	if (virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_REPORTING))
 		page_reporting_unregister(&vb->pr_dev_info);
diff --git a/include/linux/nodemask.h b/include/linux/nodemask.h
index b61438313a73..e65274f89247 100644
--- a/include/linux/nodemask.h
+++ b/include/linux/nodemask.h
@@ -266,6 +266,12 @@ static inline unsigned int __first_node(const nodemask_t *srcp)
 	return min_t(unsigned int, MAX_NUMNODES, find_first_bit(srcp->bits, MAX_NUMNODES));
 }
 
+#define last_node(src) __last_node(&(src))
+static inline unsigned int __last_node(const nodemask_t *srcp)
+{
+	return min_t(unsigned int, MAX_NUMNODES, find_last_bit(srcp->bits, MAX_NUMNODES));
+}
+
 #define next_node(n, src) __next_node((n), &(src))
 static inline unsigned int __next_node(int n, const nodemask_t *srcp)
 {
diff --git a/include/linux/stddef.h b/include/linux/stddef.h
index 929d67710cc5..5e8a81e8d3a6 100644
--- a/include/linux/stddef.h
+++ b/include/linux/stddef.h
@@ -7,11 +7,6 @@
 #undef NULL
 #define NULL ((void *)0)
 
-enum {
-	false	= 0,
-	true	= 1
-};
-
 #undef offsetof
 #define offsetof(TYPE, MEMBER)	__builtin_offsetof(TYPE, MEMBER)
 
diff --git a/include/linux/types.h b/include/linux/types.h
index 2bc8766ba20c..a2410bf2599d 100644
--- a/include/linux/types.h
+++ b/include/linux/types.h
@@ -32,7 +32,26 @@ typedef __kernel_timer_t	timer_t;
 typedef __kernel_clockid_t	clockid_t;
 typedef __kernel_mqd_t		mqd_t;
 
-typedef _Bool			bool;
+#define __bool_true_false_are_defined 1
+
+#if defined(__STDC_VERSION__) && __STDC_VERSION__ > 201710L
+/* FIXME: We should be issuing a deprecation warning here, but cannot yet due
+ * to system headers which include this header file unconditionally.
+ */
+#elif !defined(__cplusplus)
+#define bool _Bool
+#define true 1
+#define false 0
+#elif defined(__GNUC__) && !defined(__STRICT_ANSI__)
+/* Define _Bool as a GNU extension. */
+#define _Bool bool
+#if defined(__cplusplus) && __cplusplus < 201103L
+/* For C++98, define bool, false, true as a GNU extension. */
+#define bool bool
+#define false false
+#define true true
+#endif
+#endif
 
 typedef __kernel_uid32_t	uid_t;
 typedef __kernel_gid32_t	gid_t;
diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 747943bc8cc2..2fcb84714ca4 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -70,6 +70,21 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		THP_MIGRATION_FAIL,
 		THP_MIGRATION_SPLIT,
 #endif
+		PEBS_NR_SAMPLED,
+		PEBS_NR_SAMPLED_FMEM,
+		PEBS_NR_SAMPLED_SMEM,
+		PEBS_NR_DISCARDED,
+		PEBS_NR_DISCARDED_NULL,
+		PEBS_NR_DISCARDED_PID,
+		PEBS_NR_DISCARDED_ERROR,
+		PEBS_NR_DISCARDED_IGNORE,
+		FOLIO_EXCHANGE,
+		FOLIO_EXCHANGE_SUCCESS,
+		FOLIO_EXCHANGE_FAILED,
+		FOLIO_EXCHANGE_FAILED_ISOLATE,
+		FOLIO_EXCHANGE_FAILED_LOCK,
+		FOLIO_EXCHANGE_FAILED_SUPPORT,
+		FOLIO_EXCHANGE_FAILED_MOVE,
 #ifdef CONFIG_COMPACTION
 		COMPACTMIGRATE_SCANNED, COMPACTFREE_SCANNED,
 		COMPACTISOLATED,
diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 735eae6e272c..7c65a3c60a1a 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -11,6 +11,8 @@
 #include <linux/mmdebug.h>
 
 extern int sysctl_stat_interval;
+extern int sysctl_clearvmevents_handler(struct ctl_table *table, int write,
+	void __user *buffer, size_t *length, loff_t *ppos);
 
 #ifdef CONFIG_NUMA
 #define ENABLE_NUMA_STAT   1
@@ -83,6 +85,8 @@ static inline void count_vm_events(enum vm_event_item item, long delta)
 
 extern void all_vm_events(unsigned long *);
 
+extern void clear_all_vm_events(void);
+
 extern void vm_events_fold_cpu(int cpu);
 
 #else
@@ -103,6 +107,9 @@ static inline void __count_vm_events(enum vm_event_item item, long delta)
 static inline void all_vm_events(unsigned long *ret)
 {
 }
+static inline void clear_all_vm_events(void)
+{
+}
 static inline void vm_events_fold_cpu(int cpu)
 {
 }
diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 3a64499b0f5d..197c0f1f07f9 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -167,6 +167,13 @@ enum perf_event_sample_format {
 };
 
 #define PERF_SAMPLE_WEIGHT_TYPE	(PERF_SAMPLE_WEIGHT | PERF_SAMPLE_WEIGHT_STRUCT)
+
+#define PERF_PEBS_MEMINFO_TYPE	(PERF_SAMPLE_ADDR | PERF_SAMPLE_DATA_SRC |   \
+				PERF_SAMPLE_PHYS_ADDR |			     \
+				PERF_SAMPLE_WEIGHT_TYPE |		     \
+				PERF_SAMPLE_TRANSACTION |		     \
+				PERF_SAMPLE_DATA_PAGE_SIZE)
+
 /*
  * values to program into branch_sample_type when PERF_SAMPLE_BRANCH is set
  *
diff --git a/kernel/events/core.c b/kernel/events/core.c
index 8f908f077935..93b40cb8f1f1 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -7857,6 +7857,7 @@ void perf_prepare_sample(struct perf_sample_data *data,
 		data->sample_flags |= PERF_SAMPLE_AUX;
 	}
 }
+EXPORT_SYMBOL_GPL(perf_prepare_sample);
 
 void perf_prepare_header(struct perf_event_header *header,
 			 struct perf_sample_data *data,
diff --git a/kernel/kthread.c b/kernel/kthread.c
index f7be976ff88a..ee86f426b65f 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -107,6 +107,7 @@ void get_kthread_comm(char *buf, size_t buf_size, struct task_struct *tsk)
 
 	strscpy_pad(buf, kthread->full_name, buf_size);
 }
+EXPORT_SYMBOL(get_kthread_comm);
 
 bool set_kthread_struct(struct task_struct *p)
 {
diff --git a/kernel/pid.c b/kernel/pid.c
index da76ed1873f7..d34d53bff0c8 100644
--- a/kernel/pid.c
+++ b/kernel/pid.c
@@ -447,6 +447,7 @@ struct task_struct *find_get_task_by_vpid(pid_t nr)
 
 	return task;
 }
+EXPORT_SYMBOL_GPL(find_get_task_by_vpid);
 
 struct pid *get_task_pid(struct task_struct *task, enum pid_type type)
 {
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index e0b917328cf9..d0d4d20f9e6f 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -135,6 +135,8 @@ static enum sysctl_writes_mode sysctl_writes_strict = SYSCTL_WRITES_STRICT;
 int sysctl_legacy_va_layout;
 #endif
 
+static int sysctl_clear_vm_events;
+
 #endif /* CONFIG_SYSCTL */
 
 /*
@@ -2239,6 +2241,13 @@ static struct ctl_table vm_table[] = {
 		.extra2		= (void *)&mmap_rnd_compat_bits_max,
 	},
 #endif
+	{
+		.procname	= "clear_all_vm_events",
+		.data		= &sysctl_clear_vm_events,
+		.maxlen		= sizeof(int),
+		.mode		= 0200,
+		.proc_handler	= sysctl_clearvmevents_handler,
+	},
 };
 
 int __init sysctl_init_bases(void)
diff --git a/kernel/trace/ring_buffer.c b/kernel/trace/ring_buffer.c
index 28853966aa9a..8118e69c220a 100644
--- a/kernel/trace/ring_buffer.c
+++ b/kernel/trace/ring_buffer.c
@@ -968,6 +968,125 @@ int ring_buffer_wait(struct trace_buffer *buffer, int cpu, int full,
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(ring_buffer_wait);
+
+struct __wqe {
+	struct wait_queue_entry entry;
+	struct wait_queue_head *head;
+};
+static inline void __wqe_dtor(struct __wqe *wqe)
+{
+	remove_wait_queue(wqe->head, &wqe->entry);
+}
+#define __make_wqe(name, wqh)                                    \
+	struct __wqe __cleanup(__wqe_dtor) name = {              \
+		.head = ({                                       \
+			init_wait(&name.entry);                  \
+			name.entry.func = default_wake_function; \
+			add_wait_queue(wqh, &name.entry);        \
+			wqh;                                     \
+		}),                                              \
+	}
+#define __check(cond, ret)           \
+	({                           \
+		if (cond) {          \
+			__ret = ret; \
+			break;       \
+		}                    \
+	})
+#define select2(wqh0, cond0, wqh1, cond1)                                      \
+	({                                                                     \
+		long __ret = -EAGAIN;                                          \
+		might_sleep();                                                 \
+		do {                                                           \
+			__check(cond0, 0);                                     \
+			__check(cond1, 1);                                     \
+			{                                                      \
+				__make_wqe(__wqe0, wqh0);                      \
+				__make_wqe(__wqe1, wqh1);                      \
+				for (;;) {                                     \
+					set_current_state(TASK_INTERRUPTIBLE); \
+					__check(cond0, 0);                     \
+					__check(cond1, 1);                     \
+					schedule();                            \
+					__check(signal_pending(current),       \
+						-ERESTARTSYS);                 \
+				}                                              \
+			}                                                      \
+			__set_current_state(TASK_RUNNING);                     \
+		} while (false);                                               \
+		__ret;                                                         \
+	})
+
+#define select3(wqh0, cond0, wqh1, cond1, wqh2, cond2)                         \
+	({                                                                     \
+		long __ret = -EAGAIN;                                          \
+		might_sleep();                                                 \
+		do {                                                           \
+			__check(cond0, 0);                                     \
+			__check(cond1, 1);                                     \
+			__check(cond2, 2);                                     \
+			{                                                      \
+				__make_wqe(__wqe0, wqh0);                      \
+				__make_wqe(__wqe1, wqh1);                      \
+				__make_wqe(__wqe2, wqh2);                      \
+				for (;;) {                                     \
+					set_current_state(TASK_INTERRUPTIBLE); \
+					__check(cond0, 0);                     \
+					__check(cond1, 1);                     \
+					__check(cond2, 2);                     \
+					schedule();                            \
+					__check(signal_pending(current),       \
+						-ERESTARTSYS);                 \
+				}                                              \
+			}                                                      \
+			__set_current_state(TASK_RUNNING);                     \
+		} while (false);                                               \
+		__ret;                                                         \
+	})
+
+int ring_buffer_select2(struct trace_buffer *b0, ring_buffer_cond_fn c0,
+			void *d0, struct trace_buffer *b1,
+			ring_buffer_cond_fn c1, void *d1)
+{
+	struct rb_irq_work *rbwork0 = &b0->irq_work, *rbwork1 = &b1->irq_work;
+	struct wait_queue_head *waitq0 = &rbwork0->waiters,
+			       *waitq1 = &rbwork1->waiters;
+	BUG_ON(!c0 || !c1);
+	return select2(
+		waitq0,
+		rb_wait_cond(rbwork0, b0, RING_BUFFER_ALL_CPUS, 0, c0, d0),
+		waitq1,
+		rb_wait_cond(rbwork1, b1, RING_BUFFER_ALL_CPUS, 0, c1, d1));
+};
+EXPORT_SYMBOL_GPL(ring_buffer_select2);
+
+int ring_buffer_select3(struct trace_buffer *b0, ring_buffer_cond_fn c0,
+			void *d0, struct trace_buffer *b1,
+			ring_buffer_cond_fn c1, void *d1,
+			struct trace_buffer *b2, ring_buffer_cond_fn c2,
+			void *d2)
+{
+	struct rb_irq_work *rbwork0 = &b0->irq_work, *rbwork1 = &b1->irq_work,
+			   *rbwork2 = &b2->irq_work;
+	struct wait_queue_head *waitq0 = &rbwork0->waiters,
+			       *waitq1 = &rbwork1->waiters,
+			       *waitq2 = &rbwork2->waiters;
+	BUG_ON(!c0 || !c1 || !c2);
+	return select3(
+		waitq0,
+		rb_wait_cond(rbwork0, b0, RING_BUFFER_ALL_CPUS, 0, c0, d0),
+		waitq1,
+		rb_wait_cond(rbwork1, b1, RING_BUFFER_ALL_CPUS, 0, c1, d1),
+		waitq2,
+		rb_wait_cond(rbwork2, b2, RING_BUFFER_ALL_CPUS, 0, c2, d2));
+};
+EXPORT_SYMBOL_GPL(ring_buffer_select3);
+
+#undef select3
+#undef select2
+#undef __check
+#undef __make_wqe
 
 /**
  * ring_buffer_poll_wait - poll on buffer input
diff --git a/mm/Kconfig b/mm/Kconfig
index b4cb45255a54..18122eb296eb 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -1251,4 +1251,6 @@ config EXECMEM
 
 source "mm/damon/Kconfig"
 
+source "mm/demeter/Kconfig"
+
 endmenu
diff --git a/mm/Makefile b/mm/Makefile
index 8fb85acda1b1..6d32fbec549e 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -139,3 +139,7 @@ obj-$(CONFIG_HAVE_BOOTMEM_INFO_NODE) += bootmem_info.o
 obj-$(CONFIG_GENERIC_IOREMAP) += ioremap.o
 obj-$(CONFIG_SHRINKER_DEBUG) += shrinker_debug.o
 obj-$(CONFIG_EXECMEM) += execmem.o
+
+obj-$(CONFIG_EXCHANGE) += exchange.o
+obj-$(CONFIG_EXCHANGE_TEST) += exchange_test.o
+obj-$(CONFIG_DEMETER) += demeter/
diff --git a/mm/exchange.c b/mm/exchange.c
new file mode 100644
index 000000000000..afa3d4822db9
--- /dev/null
+++ b/mm/exchange.c
@@ -0,0 +1,1170 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Folio exchange functionality - linux/mm/exchange.c
+ *
+ * Copyright (C) 2021-2024 Junliang Hu
+ *
+ * Folio exchange is developed for tiered memory management.
+ *
+ * Author: Junliang Hu <jlhu@cse.cuhk.edu.hk>
+ *
+ */
+
+#include <linux/migrate.h>
+#include <linux/export.h>
+#include <linux/swap.h>
+#include <linux/swapops.h>
+#include <linux/pagemap.h>
+#include <linux/buffer_head.h>
+#include <linux/mm_inline.h>
+#include <linux/nsproxy.h>
+#include <linux/ksm.h>
+#include <linux/rmap.h>
+#include <linux/topology.h>
+#include <linux/cpu.h>
+#include <linux/cpuset.h>
+#include <linux/writeback.h>
+#include <linux/mempolicy.h>
+#include <linux/vmalloc.h>
+#include <linux/security.h>
+#include <linux/backing-dev.h>
+#include <linux/compaction.h>
+#include <linux/syscalls.h>
+#include <linux/compat.h>
+#include <linux/hugetlb.h>
+#include <linux/hugetlb_cgroup.h>
+#include <linux/gfp.h>
+#include <linux/pfn_t.h>
+#include <linux/memremap.h>
+#include <linux/userfaultfd_k.h>
+#include <linux/balloon_compaction.h>
+#include <linux/page_idle.h>
+#include <linux/page_owner.h>
+#include <linux/sched/mm.h>
+#include <linux/ptrace.h>
+#include <linux/oom.h>
+#include <linux/memory.h>
+#include <linux/random.h>
+#include <linux/sched/sysctl.h>
+#include <linux/memory-tiers.h>
+
+#include <asm/tlbflush.h>
+
+#include <trace/events/migrate.h>
+
+#include "internal.h"
+
+DEFINE_CLASS(kmap, void *, kunmap(_T), kmap(page), struct page *page);
+DEFINE_LOCK_GUARD_1(mmap_read_lock, struct mm_struct, mmap_read_lock(_T->lock),
+		    mmap_read_unlock(_T->lock));
+DEFINE_LOCK_GUARD_1(xas_lock, struct xa_state, xas_lock(_T->lock),
+		    xas_unlock(_T->lock));
+DEFINE_LOCK_GUARD_0(lru_cache, lru_cache_disable(), lru_cache_enable());
+
+#define swapbits(a, b, mask)                               \
+	do {                                               \
+		typeof(a) __t1 = (a), __t2 = (b);          \
+		typeof(b) __diff = (__t1 ^ __t2) & (mask); \
+		(a) = __t1 ^ __diff;                       \
+		(b) = __t2 ^ __diff;                       \
+	} while (0)
+
+// clang-format off
+#define FOLIO_SWAP_FLAG(name, page)						\
+static __always_inline void folio_swap_##name(struct folio *l, struct folio *r) \
+{ swapbits(*folio_flags(l, page), *folio_flags(r, page), 1ul << PG_##name); }
+
+#define DEFINE_COPY(name, var) \
+	typeof(var) name = (var)
+
+// FOLIO_SWAP_FLAG(locked, FOLIO_PF_NO_TAIL);
+FOLIO_SWAP_FLAG(writeback, FOLIO_PF_NO_TAIL);
+FOLIO_SWAP_FLAG(referenced, FOLIO_PF_HEAD);
+FOLIO_SWAP_FLAG(uptodate, FOLIO_PF_NO_TAIL);
+FOLIO_SWAP_FLAG(dirty, FOLIO_PF_HEAD);
+// FOLIO_SWAP_FLAG(lru, FOLIO_PF_HEAD);
+// FOLIO_SWAP_FLAG(head, FOLIO_PF_ANY);
+// FOLIO_SWAP_FLAG(waiters, FOLIO_PF_HEAD);
+FOLIO_SWAP_FLAG(active, FOLIO_PF_HEAD);
+FOLIO_SWAP_FLAG(workingset, FOLIO_PF_HEAD);
+FOLIO_SWAP_FLAG(error, FOLIO_PF_NO_TAIL);
+// FOLIO_SWAP_FLAG(slab, FOLIO_PF_NO_TAIL);
+// FOLIO_SWAP_FLAG(owner_priv_1, FOLIO_PF_ANY);
+	FOLIO_SWAP_FLAG(swapcache, FOLIO_PF_NO_TAIL);
+	FOLIO_SWAP_FLAG(checked, FOLIO_PF_NO_COMPOUND);
+// FOLIO_SWAP_FLAG(arch_1, );
+// FOLIO_SWAP_FLAG(reserved, FOLIO_PF_NO_COMPOUND);
+FOLIO_SWAP_FLAG(private, FOLIO_PF_ANY);
+// FOLIO_SWAP_FLAG(private_2, FOLIO_PF_ANY);
+FOLIO_SWAP_FLAG(mappedtodisk, FOLIO_PF_NO_TAIL);
+// FOLIO_SWAP_FLAG(reclaim, FOLIO_PF_NO_TAIL);
+	// FOLIO_SWAP_FLAG(isolated, FOLIO_PF_ANY);
+	FOLIO_SWAP_FLAG(readahead, FOLIO_PF_NO_TAIL);
+FOLIO_SWAP_FLAG(swapbacked, FOLIO_PF_NO_TAIL);
+FOLIO_SWAP_FLAG(unevictable, FOLIO_PF_HEAD);
+// FOLIO_SWAP_FLAG(mlocked, FOLIO_PF_NO_TAIL);
+// FOLIO_SWAP_FLAG(uncached, FOLIO_PF_NO_COMPOUND);
+// FOLIO_SWAP_FLAG(hwpoison, FOLIO_PF_ANY);
+#ifdef CONFIG_PAGE_IDLE_FLAG
+FOLIO_SWAP_FLAG(young, FOLIO_PF_HEAD);
+FOLIO_SWAP_FLAG(idle, FOLIO_PF_HEAD);
+#endif
+// clang-format on
+
+static struct workqueue_struct *exchange_wq;
+
+int writeout(struct address_space *mapping, struct folio *folio);
+bool buffer_migrate_lock_buffers(struct buffer_head *head,
+				 enum migrate_mode mode);
+int folio_expected_refs(struct address_space *mapping, struct folio *folio);
+
+void migrate_folio_done(struct folio *src, enum migrate_reason reason);
+
+void buffer_migrate_unlock_buffers(struct buffer_head *head)
+{
+	struct buffer_head *bh = head;
+	do {
+		unlock_buffer(bh);
+		bh = bh->b_this_page;
+	} while (bh != head);
+}
+
+// We might hold the buffer locks, if failed, we should unlock them by calling
+// folio_exchange_fs_finish().
+int folio_exchange_fs_prepare(struct folio *folio, enum migrate_mode mode,
+			      bool *lock_acquired)
+{
+	// pr_info("%s: folio=%p mode=%d", __func__, folio, mode);
+	struct address_space *mapping = folio_mapping(folio);
+	// anon folio does not need this step
+	if (!mapping)
+		return MIGRATEPAGE_SUCCESS;
+	DEFINE_COPY(migrate_fn, mapping->a_ops->migrate_folio);
+	if (!migrate_fn) { // fallback_migrate_folio()
+		if (folio_test_dirty(folio)) {
+			switch (mode) {
+			case MIGRATE_SYNC:
+			case MIGRATE_SYNC_NO_COPY:
+				break;
+			default:
+				return -EBUSY;
+			}
+			return writeout(mapping, folio);
+		}
+		if (!filemap_release_folio(folio, GFP_KERNEL))
+			return mode == MIGRATE_SYNC ? -EAGAIN : -EBUSY;
+		return MIGRATEPAGE_SUCCESS;
+	} else if (migrate_fn == migrate_folio) { // shmem
+		return MIGRATEPAGE_SUCCESS;
+	} else if (migrate_fn == buffer_migrate_folio) { // ext4
+		struct buffer_head *head = folio_buffers(folio);
+		if (!head)
+			return MIGRATEPAGE_SUCCESS;
+		if (folio_expected_refs(mapping, folio) !=
+		    folio_ref_count(folio)) {
+			pr_err("%s: folio=%p ref_count=%d expected_refs=%d",
+			       __func__, folio, folio_ref_count(folio),
+			       folio_expected_refs(mapping, folio));
+			return -EAGAIN;
+		}
+		if (!buffer_migrate_lock_buffers(head, mode))
+			return -EAGAIN;
+		*lock_acquired = true;
+	} else if (migrate_fn == filemap_migrate_folio) { // f2fs
+		return MIGRATEPAGE_SUCCESS;
+	} else {
+		BUG();
+	}
+	return MIGRATEPAGE_SUCCESS;
+}
+
+// This should be called even folio_exchange_mapping fails.
+void folio_exchange_fs_finish(struct folio *folio, enum migrate_mode mode,
+			      bool lock_acquired)
+{
+	struct address_space *mapping = folio_mapping(folio);
+	if (!mapping)
+		return;
+	DEFINE_COPY(migrate_fn, mapping->a_ops->migrate_folio);
+	if (!migrate_fn) {
+	} else if (migrate_fn == migrate_folio) {
+	} else if (migrate_fn == buffer_migrate_folio) {
+		if (lock_acquired)
+			buffer_migrate_unlock_buffers(folio_buffers(folio));
+	} else if (migrate_fn == filemap_migrate_folio) {
+	} else {
+		BUG();
+	}
+}
+
+void folio_exchange_mapping_anon_anon(struct folio *old, struct folio *new)
+{
+	// pr_info("%s: old=%p new=%p", __func__, old, new);
+	VM_BUG_ON_FOLIO(folio_mapping(old), old);
+	VM_BUG_ON_FOLIO(folio_mapping(new), new);
+
+	swap(old->index, new->index);
+	swap(old->mapping, new->mapping);
+	folio_swap_swapbacked(old, new);
+
+	// pr_info("%s: old=%p new=%p  mapping=%p<->%p index=0x%lx<->0x%lx  success",
+	// 	__func__, old, new, new->mapping, old->mapping, new->index,
+	// 	old->index);
+}
+
+void folio_exchange_mapping_update_stats(struct folio *old, struct folio *new)
+{
+	long nr = folio_nr_pages(old);
+	VM_BUG_ON_FOLIO(nr != folio_nr_pages(new), anon);
+
+	struct zone *old_zone = folio_zone(old);
+	struct zone *new_zone = folio_zone(new);
+	if (old_zone == new_zone)
+		return;
+
+	struct lruvec *old_lruvec =
+		mem_cgroup_lruvec(folio_memcg(old), old_zone->zone_pgdat);
+	struct lruvec *new_lruvec =
+		mem_cgroup_lruvec(folio_memcg(new), new_zone->zone_pgdat);
+
+	// The total page count remains the same
+	// __mod_lruvec_state(old_lruvec, NR_FILE_PAGES, -nr);
+	// __mod_lruvec_state(new_lruvec, NR_FILE_PAGES, nr);
+
+	// The new folio is a shmem folio, which means we moved a shmem folio
+	// from old lruvec to the new lruvec.
+	if (folio_test_swapbacked(new) && !folio_test_swapcache(new)) {
+		__mod_lruvec_state(old_lruvec, NR_SHMEM, -nr);
+		__mod_lruvec_state(new_lruvec, NR_SHMEM, nr);
+		if (folio_test_pmd_mappable(new)) {
+			__mod_lruvec_state(old_lruvec, NR_SHMEM_THPS, -nr);
+			__mod_lruvec_state(new_lruvec, NR_SHMEM_THPS, nr);
+		}
+	}
+#ifdef CONFIG_SWAP
+	if (folio_test_swapcache(new)) {
+		__mod_lruvec_state(old_lruvec, NR_SWAPCACHE, -nr);
+		__mod_lruvec_state(new_lruvec, NR_SWAPCACHE, nr);
+	}
+#endif
+	if (folio_test_dirty(new) &&
+	    mapping_can_writeback(folio_mapping(new))) {
+		__mod_lruvec_state(old_lruvec, NR_FILE_DIRTY, -nr);
+		__mod_zone_page_state(old_zone, NR_ZONE_WRITE_PENDING, -nr);
+		__mod_lruvec_state(new_lruvec, NR_FILE_DIRTY, nr);
+		__mod_zone_page_state(new_zone, NR_ZONE_WRITE_PENDING, nr);
+	}
+}
+
+// We moved the swapcache/private handling out of this function
+void folio_exchange_mapping_file_anon(struct folio *file, struct folio *anon)
+{
+	// pr_info("%s: file=%p anon=%p", __func__, file, anon);
+	VM_BUG_ON_FOLIO(folio_mapping(anon), anon);
+	struct address_space *mapping = folio_mapping(file);
+	VM_BUG_ON_FOLIO(!mapping, file);
+	VM_BUG_ON_FOLIO(mapping_unmovable(mapping), file);
+	int expected_refs = folio_expected_refs(mapping, file);
+	long nr = folio_nr_pages(file);
+	VM_BUG_ON_FOLIO(nr != folio_nr_pages(anon), anon);
+
+	XA_STATE(xas, &mapping->i_pages, folio_index(file));
+	guard(irqsave)();
+	scoped_guard(xas_lock, &xas)
+	{
+		// We should always success because of we are seralized by the
+		// folio lock
+		VM_BUG_ON_FOLIO(!folio_ref_freeze(file, expected_refs), file);
+		folio_ref_add(anon, nr);
+		swap(file->index, anon->index);
+		swap(file->mapping, anon->mapping);
+		folio_swap_swapbacked(file, anon);
+		folio_swap_dirty(file, anon);
+
+		// Notice that we have changed the swapbacked bit
+		long entries = folio_test_swapbacked(anon) ? nr : 1;
+		for (long i = 0; i < entries; ++i) {
+			xas_store(&xas, anon);
+			xas_next(&xas);
+		}
+
+		folio_ref_unfreeze(file, expected_refs - nr);
+	}
+
+	// pr_info("%s: old=%p new=%p  mapping=%p<->%p index=0x%lx<->0x%lx  success",
+	// 	__func__, file, anon, anon->mapping, file->mapping, anon->index,
+	// 	file->index);
+
+	folio_exchange_mapping_update_stats(file, anon);
+}
+
+void folio_exchange_mapping_file_file(struct folio *old, struct folio *new)
+{
+	// pr_info("%s: old=%p new=%p", __func__, old, new);
+	struct address_space *old_mapping = folio_mapping(old);
+	VM_BUG_ON_FOLIO(!old_mapping, old);
+	VM_BUG_ON_FOLIO(mapping_unmovable(old_mapping), old);
+	int old_expected_refs = folio_expected_refs(old_mapping, old);
+	struct address_space *new_mapping = folio_mapping(new);
+	VM_BUG_ON_FOLIO(!new_mapping, new);
+	VM_BUG_ON_FOLIO(mapping_unmovable(new_mapping), new);
+	int new_expected_refs = folio_expected_refs(new_mapping, new);
+	long nr = folio_nr_pages(old);
+	VM_BUG_ON_FOLIO(nr != folio_nr_pages(new), new);
+
+	XA_STATE(old_xas, &old_mapping->i_pages, folio_index(old));
+	XA_STATE(new_xas, &new_mapping->i_pages, folio_index(new));
+	guard(irqsave)();
+	scoped_guard(xas_lock, &old_xas) scoped_guard(xas_lock, &new_xas)
+	{
+		VM_BUG_ON_FOLIO(!folio_ref_freeze(old, old_expected_refs), old);
+		VM_BUG_ON_FOLIO(!folio_ref_freeze(new, old_expected_refs), new);
+		// The ref count should remain the same
+		swap(old->index, new->index);
+		swap(old->mapping, new->mapping);
+		folio_swap_swapbacked(old, new);
+		folio_swap_dirty(old, new);
+
+		// Notice that we have changed the swapbacked bit
+		long entries = folio_test_swapbacked(new) ? nr : 1;
+		for (long i = 0; i < entries; ++i) {
+			xas_store(&old_xas, new);
+			xas_next(&old_xas);
+		}
+		entries = folio_test_swapbacked(old) ? nr : 1;
+		for (long i = 0; i < entries; ++i) {
+			xas_store(&new_xas, old);
+			xas_next(&new_xas);
+		}
+
+		folio_ref_unfreeze(new, new_expected_refs);
+		folio_ref_unfreeze(old, old_expected_refs);
+	}
+
+	// pr_info("%s: old=%p new=%p  mapping=%p<->%p index=0x%lx<->0x%lx  success",
+	// 	__func__, old, new, new->mapping, old->mapping, new->index,
+	// 	old->index);
+
+	folio_exchange_mapping_update_stats(old, new);
+	folio_exchange_mapping_update_stats(new, old);
+}
+
+void folio_exchange_mapping(struct folio *old, struct folio *new)
+{
+	struct address_space *old_mapping = folio_mapping(old);
+	struct address_space *new_mapping = folio_mapping(new);
+	if (old_mapping) {
+		if (new_mapping)
+			return folio_exchange_mapping_file_file(old, new);
+		else
+			return folio_exchange_mapping_file_anon(old, new);
+	} else {
+		if (new_mapping)
+			return folio_exchange_mapping_file_anon(new, old);
+		else
+			return folio_exchange_mapping_anon_anon(old, new);
+	}
+}
+
+void folio_exchange_fs_private(struct folio *old, struct folio *new,
+			       enum migrate_mode mode)
+{
+	// The mapping has already been exchanged, so new folio has the old mapping
+	struct address_space *old_mapping = folio_mapping(old);
+	struct address_space *new_mapping = folio_mapping(new);
+	// But the private bit and private field have not been exchanged
+	void *old_private = folio_detach_private(old);
+	void *new_private = folio_detach_private(new);
+	// pr_info("%s: old=%p new=%p  mapping=%p<->%p private=%p<->%p", __func__,
+	// 	old, new, new_mapping, old_mapping, old_private, new_private);
+	if (old_private) {
+		VM_BUG_ON_FOLIO(!new_mapping, new);
+		folio_attach_private(new, old_private);
+		DEFINE_COPY(fn, new_mapping->a_ops->migrate_folio);
+		if (!fn) {
+		} else if (fn == buffer_migrate_folio) {
+			struct buffer_head *head = old_private, *bh = head;
+			do {
+				folio_set_bh(head, new, bh_offset(head));
+				bh = bh->b_this_page;
+			} while (bh != head);
+		} else if (fn == filemap_migrate_folio) {
+		} else {
+			pr_err("%s: mapping not supported %p\n", __func__,
+			       new_mapping);
+			dump_page(folio_page(new, 0), NULL);
+			BUG();
+		}
+	}
+	if (new_private) {
+		VM_BUG_ON_FOLIO(!old_mapping, old);
+		folio_attach_private(old, new_private);
+		DEFINE_COPY(fn, old_mapping->a_ops->migrate_folio);
+		if (!fn) {
+		} else if (fn == buffer_migrate_folio) {
+			struct buffer_head *head = new_private, *bh = head;
+			do {
+				folio_set_bh(head, old, bh_offset(head));
+				bh = bh->b_this_page;
+			} while (bh != head);
+		} else if (fn == filemap_migrate_folio) {
+		} else {
+			pr_err("%s: mapping not supported %p\n", __func__,
+			       old_mapping);
+			dump_page(folio_page(old, 0), NULL);
+			BUG();
+		}
+	}
+}
+
+enum parallel_mode {
+	PARALLEL_SINGLE,
+	PARALLEL_2THREAD,
+	PARALLEL_4THREAD,
+	PARALLEL_8THREAD,
+	// PARALLEL_DMA,
+};
+enum {
+	NUM_WORKERS_MAX = 8,
+};
+struct exchange_data_parallel_work {
+	struct work_struct work;
+	// struct completion done;
+	void *src, *dst;
+	size_t size;
+};
+void exchange_data_parallel_work_fn(struct work_struct *work)
+{
+	struct exchange_data_parallel_work *w =
+		container_of(work, struct exchange_data_parallel_work, work);
+	unsigned long *src = w->src, *dst = w->dst;
+	for (size_t i = 0, size = w->size / sizeof(unsigned long); i < size;
+	     ++i) {
+		swap(src[i], dst[i]);
+	}
+	// completion_done(&w->done);
+}
+void exchange_data_parallel_work_init(struct exchange_data_parallel_work *work,
+				      void *src, void *dst, size_t size)
+{
+	INIT_WORK(&work->work, exchange_data_parallel_work_fn);
+	// init_completion(&work->done);
+	work->src = src;
+	work->dst = dst;
+	work->size = size;
+}
+void exchange_data_parallel(struct page *old, struct page *new, int nworkers)
+{
+	CLASS(kmap, vold)(old);
+	CLASS(kmap, vnew)(new);
+	struct exchange_data_parallel_work works[NUM_WORKERS_MAX] = {};
+	for (size_t i = 0, chunk_size = PAGE_SIZE / nworkers; i < nworkers;
+	     ++i) {
+		exchange_data_parallel_work_init(&works[i],
+						 vold + i * chunk_size,
+						 vnew + i * chunk_size,
+						 chunk_size);
+		queue_work(exchange_wq, &works[i].work);
+	}
+	flush_workqueue(exchange_wq);
+	// __flush_workqueue(system_highpri_wq);
+	// for (size_t i = 0; i < PAGE_SIZE / NUM_WORKERS; i++) {
+	// 	wait_for_completion(&works[i].done);
+	// }
+}
+
+void exchange_data_single(struct page *old, struct page *new)
+{
+	CLASS(kmap, vold)(old);
+	CLASS(kmap, vnew)(new);
+	// TODO: choose a faster exchanging algorithm
+	for (int i = 0; i < PAGE_SIZE / sizeof(unsigned long); i++) {
+		swap(((unsigned long *)vold)[i], ((unsigned long *)vnew)[i]);
+	}
+}
+
+void folio_exchange_data(struct folio *old, struct folio *new,
+			 enum migrate_mode mode, enum parallel_mode par)
+{
+	VM_BUG_ON_FOLIO(folio_nr_pages(old) != folio_nr_pages(new), new);
+	for (long i = 0, nr = folio_nr_pages(old); i < nr; ++i) {
+		switch (par) {
+		case PARALLEL_SINGLE:
+			exchange_data_single(folio_page(old, i),
+					     folio_page(new, i));
+			break;
+		case PARALLEL_2THREAD:
+			exchange_data_parallel(folio_page(old, i),
+					       folio_page(new, i), 2);
+			break;
+		case PARALLEL_4THREAD:
+			exchange_data_parallel(folio_page(old, i),
+					       folio_page(new, i), 4);
+			break;
+		case PARALLEL_8THREAD:
+			exchange_data_parallel(folio_page(old, i),
+					       folio_page(new, i), 8);
+			break;
+		default:
+			BUG();
+		}
+		cond_resched();
+	}
+}
+
+void folio_exchange_flags(struct folio *old, struct folio *new,
+			  enum migrate_mode mode)
+{
+	// if (folio_test_waiters(old)) {
+	// 	dump_page(folio_page(old, 0), "found waiters on folio");
+	// }
+	// if (folio_test_waiters(new)) {
+	// 	dump_page(folio_page(new, 0), "found waiters on folio");
+	// }
+	folio_swap_error(old, new);
+	folio_swap_referenced(old, new);
+	folio_swap_uptodate(old, new);
+	folio_swap_active(old, new);
+	folio_swap_unevictable(old, new);
+	folio_swap_workingset(old, new);
+	folio_swap_checked(old, new);
+	folio_swap_mappedtodisk(old, new);
+	// already done in folio_exchange_mapping
+	// folio_swap_dirty(old, new);
+
+#ifdef CONFIG_PAGE_IDLE_FLAG
+	folio_swap_young(old, new);
+	folio_swap_idle(old, new);
+#endif
+
+	/*
+	 * For memory tiering mode, when migrate between slow and fast
+	 * memory node, reset cpupid, because that is used to record
+	 * page access time in slow memory node.
+	 */
+	folio_xchg_last_cpupid(old, -1);
+	folio_xchg_last_cpupid(new, -1);
+
+	// will not supported for now: ksm is aleady excluded in folio_exchange_mapping
+	// folio_exchange_ksm(old, new);
+
+	// already done in folio_exchange_mapping
+	// folio_swap_swapcache(old, new);
+	// already done in folio_exchange_fs_private
+	// folio_swap_private(old, new);
+
+	folio_swap_writeback(old, new);
+	// if (folio_test_writeback(old)) {
+	// 	folio_end_writeback(old);
+	// }
+	// if (folio_test_writeback(new)) {
+	// 	folio_end_writeback(new);
+	// }
+	folio_swap_readahead(old, new);
+
+	// will not support for now
+	// folio_exchange_owner(old, new);
+	BUILD_BUG_ON(IS_ENABLED(CONFIG_PAGE_OWNER));
+
+	// Make sure we wake up those blocked on the migration entry
+	folio_set_waiters(old);
+	folio_set_waiters(new);
+
+	// mem_cgroup_exchange(old, new);
+	swap(old->memcg_data, new->memcg_data);
+	if (folio_test_large(old) && folio_test_large_rmappable(old))
+		folio_undo_large_rmappable(old);
+	if (folio_test_large(new) && folio_test_large_rmappable(new))
+		folio_undo_large_rmappable(new);
+}
+
+struct folio *folio_exchange_lock(struct folio *folio, enum migrate_mode mode)
+{
+	// VM_BUG_ON_FOLIO(folio_test_lru(folio), folio);
+
+	if (!folio_trylock(folio)) {
+		if (mode == MIGRATE_ASYNC)
+			return ERR_PTR(-EAGAIN);
+		if (current->flags & PF_MEMALLOC)
+			return ERR_PTR(-EPERM);
+		if (mode == MIGRATE_SYNC_LIGHT && !folio_test_uptodate(folio))
+			return ERR_PTR(-EAGAIN);
+		folio_lock(folio);
+	}
+	// pr_info("%s: folio locked folio=%p", __func__, folio);
+	return folio;
+}
+DEFINE_CLASS(folio_exchange_lock, struct folio *, ({
+		     if (IS_ERR_OR_NULL(_T))
+			     return;
+		     folio_unlock(_T);
+	     }),
+	     folio_exchange_lock(folio, mode), struct folio *folio,
+	     enum migrate_mode mode);
+
+bool folio_exchange_supported(struct folio *folio, enum migrate_mode mode)
+{
+	if (folio_ref_count(folio) == 1) {
+		// /* Folio was freed from under us. So we are done. */
+		// folio_clear_active(folio);
+		// folio_clear_unevictable(folio);
+		// /* free_pages_prepare() will clear PG_isolated. */
+		// list_del(&folio->lru);
+		// // migrate_folio_done(folio, MR_NUMA_MISPLACED);
+		pr_warn_ratelimited(
+			"%s: folio=%p freed from under us ref_count=%d mapcount=%d",
+			__func__, folio, folio_ref_count(folio),
+			folio_mapcount(folio));
+		return false;
+	}
+	if (folio_test_writeback(folio)) {
+		switch (mode) {
+		case MIGRATE_SYNC:
+		case MIGRATE_SYNC_NO_COPY:
+			break;
+		default:
+			return false;
+		}
+		pr_warn_ratelimited("%s: folio=%p waiting for writeback",
+				    __func__, folio);
+		folio_wait_writeback(folio);
+		pr_warn_ratelimited("%s: folio=%p waiting for writeback done",
+				    __func__, folio);
+	}
+	/*
+	 * Corner case handling:
+	 * 1. ...
+	 * 2. An orphaned page (see truncate_cleanup_page) might have
+	 * fs-private metadata. The page can be picked up due to memory
+	 * offlining.  Everywhere else except page reclaim, the page is
+	 * invisible to the vm, so the page can not be migrated.  So try to
+	 * free the metadata, so the page can be freed.
+	 */
+	if (!folio->mapping && folio_test_private(folio)) {
+		try_to_free_buffers(folio);
+		pr_warn_ratelimited("%s: folio=%p orphaned page", __func__,
+				    folio);
+		return false;
+	}
+	if (folio_test_mlocked(folio)) {
+		pr_warn_ratelimited("%s: folio=%p mlocked", __func__, folio);
+		return false;
+	}
+
+	struct address_space *mapping = folio_mapping(folio);
+	if (mapping && mapping_unmovable(mapping))
+		return false;
+	int expected_refs = folio_expected_refs(mapping, folio);
+	int ref_count = folio_ref_count(folio);
+	int mapcount = folio_mapcount(folio);
+	if (expected_refs + mapcount != ref_count) {
+		pr_warn_ratelimited(
+			"%s: folio=%p wrong reference count ref_count=%d expected=%d mapcount=%d",
+			__func__, folio, ref_count, expected_refs, mapcount);
+		return false;
+	}
+	return true;
+}
+EXPORT_SYMBOL(folio_exchange_supported);
+
+struct folio_isolated {
+	struct folio *folio;
+};
+void folio_exchange_putback(struct folio_isolated *isolated)
+{
+	struct folio *folio = isolated->folio;
+	if (!folio)
+		return;
+	node_stat_mod_folio(folio, NR_ISOLATED_ANON + folio_is_file_lru(folio),
+			    -folio_nr_pages(folio));
+	folio_add_lru(folio);
+	// We use this because folio_add_lru() (lru, the new owner of this folio)
+	// increased a reference count, we need another folio_put()
+	// folio_putback_lru(folio);
+}
+struct folio_isolated folio_exchange_isolate(struct folio *folio,
+					     enum migrate_mode mode)
+{
+	struct folio_isolated r = {};
+	if (!folio_isolate_lru(folio)) {
+		pr_err_ratelimited("%s: folio=%p failed to isolate", __func__,
+				   folio);
+		return r;
+	}
+	r.folio = folio;
+	node_stat_mod_folio(folio, NR_ISOLATED_ANON + folio_is_file_lru(folio),
+			    folio_nr_pages(folio));
+	/*
+	 * Isolating the folio has taken another reference, so the
+	 * caller's reference can be safely dropped without the folio
+	 * disappearing underneath us during migration.
+	 */
+	folio_put(folio);
+	return r;
+}
+DEFINE_CLASS(folio_exchange_isolate, struct folio_isolated,
+	     folio_exchange_putback(&_T), folio_exchange_isolate(folio, mode),
+	     struct folio *folio, enum migrate_mode mode)
+
+struct folio_unmapped {
+	struct folio *src;
+	struct folio *dst;
+	struct anon_vma *anon_vma;
+};
+void folio_exchange_remap(struct folio_unmapped *u)
+{
+	if (u->src)
+		remove_migration_ptes(u->src, u->dst ?: u->src, false);
+	if (u->anon_vma)
+		put_anon_vma(u->anon_vma);
+}
+struct folio_unmapped folio_exchange_unmap(struct folio *folio,
+					   enum migrate_mode mode)
+{
+	struct folio_unmapped u = {};
+
+	/*
+	 * By try_to_migrate(), src->mapcount goes down to 0 here. In this case,
+	 * we cannot notice that anon_vma is freed while we migrate a page.
+	 * This get_anon_vma() delays freeing anon_vma pointer until the end
+	 * of migration. File cache pages are no problem because of page_lock()
+	 * File Caches may use write_page() or lock_page() in migration, then,
+	 * just care Anon page here.
+	 *
+	 * Only folio_get_anon_vma() understands the subtleties of
+	 * getting a hold on an anon_vma from outside one of its mms.
+	 * But if we cannot get anon_vma, then we won't need it anyway,
+	 * because that implies that the anon page is no longer mapped
+	 * (and cannot be remapped so long as we hold the page lock).
+	 */
+	if (folio_test_anon(folio) && !folio_test_ksm(folio))
+		u.anon_vma = folio_get_anon_vma(folio);
+
+	/*
+	 * Corner case handling:
+	 * 1. When a new swap-cache page is read into, it is added to the LRU
+	 * and treated as swapcache but it has no rmap yet.
+	 * Calling try_to_migrate() against a folio->mapping==NULL folio will
+	 * trigger a BUG.  So handle it here.
+	 * 2. ...
+	 */
+	if (folio->mapping && folio_mapped(folio)) {
+		/* Establish migration ptes */
+		VM_BUG_ON_FOLIO(folio_test_anon(folio) &&
+					!folio_test_ksm(folio) && !u.anon_vma,
+				folio);
+		VM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);
+		try_to_migrate(folio,
+			       mode == MIGRATE_ASYNC ? TTU_BATCH_FLUSH : 0);
+		u.src = folio;
+		// pr_info("%s: migration entry installed folio=%p", __func__,
+		// 	folio);
+	}
+
+	return u;
+}
+DEFINE_CLASS(folio_exchange_unmap, struct folio_unmapped,
+	     folio_exchange_remap(&_T), folio_exchange_unmap(folio, mode),
+	     struct folio *folio, enum migrate_mode mode);
+
+int folio_exchange_move(struct folio *old, struct folio *new,
+			enum migrate_mode mode, enum parallel_mode par)
+{
+	// pr_info("%s: old=%p new=%p mode=%d", __func__, old, new, mode);
+	VM_BUG_ON_FOLIO(!folio_test_locked(old), old);
+	VM_BUG_ON_FOLIO(!folio_test_locked(new), new);
+	if (mode != MIGRATE_SYNC && mode != MIGRATE_ASYNC)
+		return -EINVAL;
+	bool old_buffer_locked = false, new_buffer_locked = false;
+
+	int rc = 0;
+	rc = folio_exchange_fs_prepare(old, mode, &old_buffer_locked);
+	if (rc != MIGRATEPAGE_SUCCESS)
+		goto out_old;
+	rc = folio_exchange_fs_prepare(new, mode, &new_buffer_locked);
+	if (rc != MIGRATEPAGE_SUCCESS)
+		goto out;
+
+	folio_exchange_mapping(old, new);
+	folio_exchange_fs_private(old, new, mode);
+	// Notice: here the lock on the fs private data should be swapped along
+	// with the exchange of fs private data
+	swap(old_buffer_locked, new_buffer_locked);
+	folio_exchange_data(old, new, mode, par);
+	folio_exchange_flags(old, new, mode);
+
+out:
+	folio_exchange_fs_finish(new, mode, new_buffer_locked);
+out_old:
+	folio_exchange_fs_finish(old, mode, old_buffer_locked);
+	return rc;
+}
+
+int folio_exchange_parallel_isolated(struct folio *old, struct folio *new,
+				     enum migrate_mode mode,
+				     enum parallel_mode par)
+{
+	CLASS(folio_exchange_lock, old_locked)(old, mode);
+	if (IS_ERR(old_locked)) {
+		count_vm_event(FOLIO_EXCHANGE_FAILED_LOCK);
+		return PTR_ERR(old_locked);
+	}
+	CLASS(folio_exchange_lock, new_locked)(new, mode);
+	if (IS_ERR(new_locked)) {
+		count_vm_event(FOLIO_EXCHANGE_FAILED_LOCK);
+		return PTR_ERR(old_locked);
+	}
+	if (!folio_exchange_supported(old, mode)) {
+		count_vm_event(FOLIO_EXCHANGE_FAILED_SUPPORT);
+		return -ENOTSUPP;
+	}
+
+	if (!folio_exchange_supported(new, mode)) {
+		count_vm_event(FOLIO_EXCHANGE_FAILED_SUPPORT);
+		return -ENOTSUPP + 1;
+	}
+
+	CLASS(folio_exchange_unmap, old_unmapped)(old, mode);
+	CLASS(folio_exchange_unmap, new_unmapped)(new, mode);
+
+	// TODO: improve TLB flushing via batching
+	try_to_unmap_flush();
+
+	int err = folio_exchange_move(old, new, mode, par);
+	if (err) {
+		count_vm_event(FOLIO_EXCHANGE_FAILED_MOVE);
+	}
+	old_unmapped.dst = new;
+	new_unmapped.dst = old;
+
+	return err;
+}
+EXPORT_SYMBOL(folio_exchange_parallel_isolated);
+
+int folio_exchange_isolated(struct folio *old, struct folio *new,
+			    enum migrate_mode mode)
+{
+	count_vm_event(FOLIO_EXCHANGE);
+	int err = folio_exchange_parallel_isolated(old, new, mode,
+						   PARALLEL_SINGLE);
+	count_vm_event(err ? FOLIO_EXCHANGE_FAILED : FOLIO_EXCHANGE_SUCCESS);
+	return err;
+}
+EXPORT_SYMBOL(folio_exchange_isolated);
+
+int folio_exchange_parallel(struct folio *old, struct folio *new,
+			    enum migrate_mode mode, enum parallel_mode par)
+{
+	bool old_lru = !__folio_test_movable(old);
+	bool new_lru = !__folio_test_movable(new);
+	VM_BUG_ON_FOLIO(!old_lru, old);
+	VM_BUG_ON_FOLIO(!new_lru, new);
+
+	// Reorganize the code logic:
+	// 1. isolate folio
+	// 2. lock folio
+	// 3. check for support
+	// 4. unmap folio
+
+	CLASS(folio_exchange_isolate, old_isolated)(old, mode);
+	if (IS_ERR_OR_NULL(old_isolated.folio)) {
+		count_vm_event(FOLIO_EXCHANGE_FAILED_ISOLATE);
+		return -ENOENT;
+	}
+	CLASS(folio_exchange_isolate, new_isolated)(new, mode);
+	if (IS_ERR_OR_NULL(old_isolated.folio)) {
+		count_vm_event(FOLIO_EXCHANGE_FAILED_ISOLATE);
+		return -ENOENT;
+	}
+
+	CLASS(folio_exchange_lock, old_locked)(old, mode);
+	if (IS_ERR(old_locked)) {
+		count_vm_event(FOLIO_EXCHANGE_FAILED_LOCK);
+		return PTR_ERR(old_locked);
+	}
+	CLASS(folio_exchange_lock, new_locked)(new, mode);
+	if (IS_ERR(new_locked)) {
+		count_vm_event(FOLIO_EXCHANGE_FAILED_LOCK);
+		return PTR_ERR(old_locked);
+	}
+	if (!folio_exchange_supported(old, mode)) {
+		count_vm_event(FOLIO_EXCHANGE_FAILED_SUPPORT);
+		return PTR_ERR(old_locked);
+	}
+
+	if (!folio_exchange_supported(new, mode)) {
+		count_vm_event(FOLIO_EXCHANGE_FAILED_SUPPORT);
+		return PTR_ERR(old_locked);
+	}
+
+	CLASS(folio_exchange_unmap, old_unmapped)(old, mode);
+	CLASS(folio_exchange_unmap, new_unmapped)(new, mode);
+
+	// TODO: improve TLB flushing via batching
+	try_to_unmap_flush();
+
+	int err = folio_exchange_move(old, new, mode, par);
+	if (err) {
+		count_vm_event(FOLIO_EXCHANGE_FAILED_MOVE);
+	}
+	old_unmapped.dst = new;
+	new_unmapped.dst = old;
+
+	return err;
+}
+EXPORT_SYMBOL(folio_exchange_parallel);
+
+int folio_exchange(struct folio *old, struct folio *new, enum migrate_mode mode)
+{
+	count_vm_event(FOLIO_EXCHANGE);
+	int err = folio_exchange_parallel(old, new, mode, PARALLEL_SINGLE);
+	count_vm_event(err ? FOLIO_EXCHANGE_FAILED : FOLIO_EXCHANGE_SUCCESS);
+	return err;
+}
+EXPORT_SYMBOL(folio_exchange);
+
+// ============================================================================
+// ======== Below is the syscall implementation of exchange folios ============
+// ============================================================================
+
+struct mm_struct *find_mm_struct(pid_t pid, nodemask_t *mem_nodes);
+
+// Resolves the given address to a struct folio. The reference count is not
+// changed after calling this function.
+struct folio *resolve_folio(struct mm_struct *mm, const void __user *p)
+{
+	guard(mmap_read_lock)(mm);
+
+	unsigned long addr = (unsigned long)untagged_addr_remote(mm, p);
+
+	struct vm_area_struct *vma = vma_lookup(mm, addr);
+	if (!vma || !vma_migratable(vma))
+		return ERR_PTR(-EFAULT);
+
+	/* FOLL_DUMP to ignore special (like zero) pages */
+	struct page *page = follow_page(vma, addr, FOLL_GET | FOLL_DUMP);
+	if (IS_ERR(page))
+		return (struct folio *)page;
+
+	if (!page)
+		return ERR_PTR(-ENOENT);
+
+	struct folio *folio = page_folio(page);
+	if (folio_is_zone_device(folio) || folio_test_hugetlb(folio)) {
+		folio_put(folio);
+		return ERR_PTR(-EINVAL);
+	}
+	// folio_put(folio);
+	return folio;
+}
+void folio_dump_short(struct folio *folio, char const *caller)
+{
+	extern void dump_mapping(const struct address_space *mapping);
+	struct address_space *mapping = folio_mapping(folio);
+	pr_warn("%s: folio=%p refcount=%d mapcount=%d mapping=%p index=%#lx pfn=%#lx %s\n",
+		caller, folio, folio_ref_count(folio), folio_mapcount(folio),
+		mapping, folio_index(folio), folio_pfn(folio),
+		folio_test_anon(folio) ? "anon" : "");
+	if (mapping)
+		dump_mapping(mapping);
+}
+EXPORT_SYMBOL(folio_dump_short);
+void resolve_folio_cleanup(struct folio **foliop)
+{
+	struct folio *folio = *foliop;
+	if (IS_ERR_OR_NULL(folio))
+		return;
+	// folio_dump_short(folio, __func__);
+	folio_put(folio);
+}
+EXPORT_SYMBOL(resolve_folio_cleanup);
+
+void folio_isolate_lru_cleanup(struct folio **foliop)
+{
+	struct folio *folio = *foliop;
+	if (!folio || IS_ERR(folio))
+		return;
+	node_stat_mod_folio(folio, NR_ISOLATED_ANON + folio_is_file_lru(folio),
+			    folio_nr_pages(folio));
+}
+
+static int kernel_exchange_folios(struct mm_struct *mm, unsigned long nr_pages,
+				  const void __user *__user *src,
+				  const void __user *__user *dst,
+				  int __user *err)
+{
+	// Newly allocated folios might still be waiting in the fbatch, make
+	// sure they are properly inserted to the LRU before the exchange.
+	// These folios will ususally have an extra reference count and have the
+	// PG_lru bit cleared.
+	lru_add_drain_all();
+	enum migrate_mode mode = MIGRATE_SYNC;
+	DEFINE_XARRAY_ALLOC(xa);
+	for (unsigned long i = 0; i < nr_pages; i++) {
+		void const __user *src_addr, __user *dst_addr;
+		get_user(src_addr, src + i);
+		get_user(dst_addr, dst + i);
+
+		// pr_info("%s: ===== EXCHANGE i=%ld FOLIO ====", __func__, i);
+		// pr_info("%s: vaddr=%p<->%p ", __func__, src_addr, dst_addr);
+		struct folio *src __cleanup(resolve_folio_cleanup) =
+			resolve_folio(mm, src_addr);
+		if (IS_ERR(src)) {
+			xa_store(&xa, i, src, GFP_KERNEL);
+			pr_err("%s: src_addr=%p resolve_folio()=%pe failed ",
+			       __func__, src_addr, src);
+			continue;
+		}
+		// pr_info("%s: ===== SRC FOLIO BEFORE EXCHANGE ====", __func__);
+		// dump_page(folio_page(src, 0), NULL);
+		// pr_info("%s: lruvec=%p", __func__, folio_lruvec(src));
+
+		struct folio *dst __cleanup(resolve_folio_cleanup) =
+			resolve_folio(mm, dst_addr);
+		if (IS_ERR(dst)) {
+			xa_store(&xa, i, dst, GFP_KERNEL);
+			pr_err("%s: dst_addr=%p resolve_folio()=%pe failed ",
+			       __func__, dst_addr, (void *)dst);
+			continue;
+		}
+		// pr_info("%s: ===== DST FOLIO BEFORE EXCHANGE ====", __func__);
+		// dump_page(folio_page(dst, 0), NULL);
+		// pr_info("%s: lruvec=%p", __func__, folio_lruvec(src));
+
+		// pr_info("%s: ===== EXCHANGE EXECUTING ====", __func__);
+		for (long j = 0, err = -EAGAIN; j < 3 && err == -EAGAIN; ++j) {
+			err = folio_exchange(src, dst, mode);
+			xa_store(&xa, i, ERR_PTR(err), GFP_KERNEL);
+			pr_err("%s: exchange_folio(src=%p, dst=%p, mode=%d)=%pe trial=%ld",
+			       __func__, src, dst, mode, ERR_PTR(err), j);
+		}
+		// pr_info("%s: ===== EXCHANGE RETURNED ====", __func__);
+
+		// pr_info("%s: ===== SRC FOLIO AFTER EXCHANGE ====", __func__);
+		// dump_page(folio_page(src, 0), NULL);
+		// pr_info("%s: lruvec=%p", __func__, folio_lruvec(src));
+		// pr_info("%s: ===== DST FOLIO AFTER EXCHANGE ====", __func__);
+		// dump_page(folio_page(dst, 0), NULL);
+		// pr_info("%s: lruvec=%p", __func__, folio_lruvec(dst));
+		// pr_info("%s: ===== EXCHANGE i=%ld FOLIO ====", __func__, i);
+	}
+
+	// lru_add_drain_all();
+	return 0;
+}
+
+static int __init exchange_init(void)
+{
+	exchange_wq = alloc_workqueue("exchange_wq", WQ_HIGHPRI, 0);
+	return 0;
+}
+late_initcall(exchange_init);
+
+/*
+ * Move a list of pages in the address space of the currently executing
+ * process.
+ */
+SYSCALL_DEFINE5(exchange_folios, pid_t, pid, unsigned long, nr_pages,
+		const void __user *__user *, src, const void __user *__user *,
+		dst, int __user *, err)
+{
+	pr_info("%s: testing incomplete implementation pid=%d nr_pages=%lu",
+		__func__, pid, nr_pages);
+	nodemask_t task_nodes;
+	struct mm_struct *mm = find_mm_struct(pid, &task_nodes);
+	if (IS_ERR(mm))
+		return PTR_ERR(mm);
+	int ret = kernel_exchange_folios(mm, nr_pages, src, dst, err);
+	mmput(mm);
+	return ret;
+}
+
+int migrate_folio_to_node(struct folio *folio, int node, enum migrate_mode mode)
+{
+	LIST_HEAD(pagelist);
+	struct migration_target_control mtc = {
+		.nid = node,
+		.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,
+	};
+
+	if (!folio_isolate_lru(folio)) {
+		return -EBUSY;
+	}
+	folio_put(folio);
+
+	list_add_tail(&folio->lru, &pagelist);
+	node_stat_mod_folio(folio, NR_ISOLATED_ANON + folio_is_file_lru(folio),
+			    folio_nr_pages(folio));
+	int err = migrate_pages(&pagelist, alloc_migration_target, NULL,
+				(unsigned long)&mtc, mode, MR_SYSCALL, NULL);
+	if (err > 0) {
+		putback_movable_pages(&pagelist);
+	}
+	return err;
+}
+EXPORT_SYMBOL(migrate_folio_to_node);
+
+int folio_bimigrate(struct folio *old, struct folio *new,
+		    enum migrate_mode mode)
+{
+	int old_nid = folio_nid(old), new_nid = folio_nid(new);
+	int err = migrate_folio_to_node(old, new_nid, mode);
+	if (err)
+		return err;
+	err = migrate_folio_to_node(new, old_nid, mode);
+	return err;
+}
+EXPORT_SYMBOL(folio_bimigrate);
+
+// Count how many folios in the given virtual address range are on the given node.
+int kernel_count_node_folios(struct mm_struct *mm, int nid, u64 va_start,
+			     u64 va_end, u64 *count, u64 *total)
+{
+	*count = 0, *total = 0;
+	guard(mmap_read_lock)(mm);
+	struct vm_area_struct *vma;
+	for (u64 cur = va_start; cur < va_end;) {
+		vma = vma_lookup(mm, cur);
+		if (!vma) {
+			cur += PAGE_SIZE;
+			continue;
+		}
+		struct folio *folio, *last = NULL;
+		for (; cur < vma->vm_end && cur < va_end; cur += PAGE_SIZE) {
+			struct page *page =
+				follow_page(vma, cur, FOLL_GET | FOLL_DUMP);
+			if (IS_ERR_OR_NULL(page))
+				continue;
+			folio = page_folio(page);
+			if (folio != last) {
+				last = folio;
+				if (folio_nid(folio) == nid)
+					(*count)++;
+				(*total)++;
+			}
+			put_page(page);
+		}
+	}
+	return 0;
+}
+EXPORT_SYMBOL(kernel_count_node_folios);
+
+SYSCALL_DEFINE6(count_node_folios, pid_t, pid, int, nid, u64, va_start, u64,
+		va_end, u64 __user *, ucount, u64 __user *, utotal)
+{
+	pr_info("%s: pid=%d nid=%d va_start=%llu va_end=%llu", __func__, pid,
+		nid, va_start, va_end);
+	nodemask_t task_nodes;
+	struct mm_struct *mm = find_mm_struct(pid, &task_nodes);
+	if (IS_ERR_OR_NULL(mm))
+		return PTR_ERR(mm);
+	u64 count = 0, total = 0;
+	int ret = kernel_count_node_folios(mm, nid, va_start, va_end, &count,
+					   &total);
+	mmput(mm);
+	if (copy_to_user(ucount, &count, sizeof(count)))
+		return -EINVAL;
+	if (copy_to_user(utotal, &total, sizeof(total)))
+		return -EINVAL;
+	return ret;
+}
diff --git a/mm/exchange_test.c b/mm/exchange_test.c
new file mode 100644
index 000000000000..7dea56a47311
--- /dev/null
+++ b/mm/exchange_test.c
@@ -0,0 +1,564 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Folio exchange testcases - linux/mm/exchange_test.c
+ *
+ * Copyright (C) 2021-2024 Junliang Hu
+ *
+ * Author: Junliang Hu <jlhu@cse.cuhk.edu.hk>
+ *
+ */
+
+#include <linux/mm.h>
+#include <linux/swap.h>
+#include <linux/mempolicy.h>
+#include <linux/cleanup.h>
+#include <linux/sched/clock.h>
+#include <linux/umh.h>
+
+#include <kunit/test.h>
+
+#include <internal.h>
+
+// #include <linux/xxhash.h>
+// #define xxhash_ulong(value)                               \
+// 	({                                                \
+// 		ulong __tmp = (value);                    \
+// 		xxh64(&__tmp, sizeof(__tmp), 0x6A09E667); \
+// 	})
+
+extern int folio_exchange(struct folio *old, struct folio *new,
+			  enum migrate_mode mode);
+extern bool folio_exchange_supported(struct folio *folio,
+				     enum migrate_mode mode);
+
+extern int folio_bimigrate(struct folio *old, struct folio *new,
+			   enum migrate_mode mode);
+extern int migrate_folio_to_node(struct folio *folio, int node,
+				 enum migrate_mode mode);
+extern void resolve_folio_cleanup(struct folio **foliop);
+
+static void follow_page_cleanup(struct page **pagep)
+{
+	struct page *page = *pagep;
+	if (!page || !IS_ERR(page))
+		return;
+	put_page(page);
+}
+
+#define DRAM_NODE first_node(node_states[N_MEMORY])
+#define PMEM_NODE next_node(DRAM_NODE, node_states[N_MEMORY])
+
+enum {
+	PMEM_FILE_REGION,
+	DRAM_FILE_REGION,
+	PMEM_ANON_REGION,
+	DRAM_ANON_REGION,
+	EXPECTED_REGIONS = 4,
+
+	REGION_SIZE = 1ul << 30,
+};
+struct usermode_helper {
+	char dram_file[64], pmem_file[64];
+	int dram_node, pmem_node;
+	size_t size;
+	// If the task is NULL, the helper is not valid
+	struct task_struct *task;
+};
+static void usermode_helper_drop(struct usermode_helper *h)
+{
+	pr_info("%s", __func__);
+	if (IS_ERR_OR_NULL(h) || IS_ERR_OR_NULL(h->task))
+		return;
+	pr_info("%s: task=%p pid=%d tgid=%d", __func__, h->task, h->task->pid,
+		h->task->tgid);
+	send_sig(SIGKILL, h->task, 0);
+	// We called get_task_struct() in usermodehelper_save_task_struct()
+	put_task_struct(h->task);
+}
+static int usermodehelper_save_task_struct(struct subprocess_info *info,
+					   struct cred *new)
+{
+	// Make sure it does not go away during testing
+	get_task_struct(current);
+	*(struct task_struct **)info->data = current;
+	return 0;
+}
+static struct usermode_helper usermode_helper_new(void)
+{
+	pr_info("%s", __func__);
+	struct usermode_helper h = {
+		.dram_node = DRAM_NODE,
+		.pmem_node = PMEM_NODE,
+		.size = REGION_SIZE,
+	};
+	char size[32] = {}, dram_node[32] = {}, pmem_node[32] = {};
+	snprintf(size, sizeof(size), "%lu", h.size);
+	snprintf(dram_node, sizeof(dram_node), "%d", h.dram_node);
+	snprintf(pmem_node, sizeof(pmem_node), "%d", h.pmem_node);
+	snprintf(h.dram_file, sizeof(h.dram_file), "/dram-%llu", local_clock());
+	snprintf(h.pmem_file, sizeof(h.pmem_file), "/pmem-%llu", local_clock());
+	// The newly created mmap region should be located at lower address
+	// clang-format off
+	char *argv[] = {
+		"/data/mmap-helper", // Helper binary
+		"anon", size, dram_node, // anonymous DRAM region
+		"anon", size, pmem_node, // anonymous PMEM region
+		h.dram_file, size, dram_node, // file PMEM region
+		h.pmem_file, size, pmem_node, // file PMEM region
+		NULL,
+	};
+	// clang-format on
+	static char *envp[] = { "HOME=/", "PATH=/sbin:/bin:/usr/sbin:/usr/bin",
+				NULL };
+
+	// We need a custom init function to save the task_struct of the newly
+	// launched helper, see: call_usermodehelper()
+	struct subprocess_info *info = call_usermodehelper_setup(
+		argv[0], argv, envp, GFP_KERNEL,
+		usermodehelper_save_task_struct, NULL, &h.task);
+	BUG_ON(IS_ERR_OR_NULL(info));
+	long err = call_usermodehelper_exec(info, UMH_WAIT_EXEC);
+	BUG_ON(IS_ERR_OR_NULL(h.task));
+	pr_info("%s: call_usermodehelper_exec()=%pe task=%p pid=%d tgid=%d",
+		__func__, (void *)err, h.task, h.task->pid, h.task->tgid);
+
+	return h;
+}
+// Return the regions found in the vmas array, the size should be not less than
+// EXPECTED_REGIONS, which is the number of regions we created.
+// Also return the number of regions found via return value.
+// The caller should hold the mmap read lock, because they need the returned vma
+// to be valid.
+static int usermode_helper_find_regions(struct usermode_helper *h,
+					struct mm_struct *mm,
+					struct vm_area_struct **vmas)
+{
+	mmap_assert_locked(mm);
+	struct vm_area_struct *vma;
+	VMA_ITERATOR(vmi, mm, 0);
+	int found = 0;
+	for_each_vma(vmi, vma) {
+		// We created the regions with the given size and
+		// MPOL_PREFERRED policy
+		if (vma->vm_end - vma->vm_start != h->size ||
+		    MPOL_PREFERRED != vma->vm_policy->mode)
+			continue;
+		vmas[found++] = vma;
+		pr_info("%s: found vma=%p flags=%pGv start=0x%lx end=0x%lx size=0x%lx mpol=%u nodes=%*pbl ops=%pS",
+			__func__, vma, &vma->vm_flags, vma->vm_start,
+			vma->vm_end, vma->vm_end - vma->vm_start,
+			vma->vm_policy->mode,
+			nodemask_pr_args(&vma->vm_policy->nodes), vma->vm_ops);
+	}
+
+	return found;
+};
+
+DEFINE_CLASS(usermode_helper, struct usermode_helper, usermode_helper_drop(&_T),
+	     usermode_helper_new(), void);
+
+static void usermode_helper_start(struct kunit *test)
+{
+	CLASS(usermode_helper, h)();
+	// This should always be true, because otherwise we would have bugged in
+	// usermode_helper_new()
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, h.task);
+	pr_info("%s: task=%p state=0x%x flags=0x%x mm=%p", __func__, h.task,
+		h.task->__state, h.task->flags, h.task->mm);
+
+	// call_usermodehelper() creates a user thread using a workqueue worker
+	// as the parent, make sure the task is indeed a child
+	KUNIT_EXPECT_NE(test, h.task->flags & PF_WQ_WORKER, PF_WQ_WORKER);
+
+	// We should also not being a kernel thread
+	KUNIT_EXPECT_NE(test, h.task->flags & PF_KTHREAD, PF_KTHREAD);
+}
+
+// clang-format off
+DEFINE_LOCK_GUARD_1(mmap_read_lock, struct mm_struct, mmap_read_lock(_T->lock), mmap_read_unlock(_T->lock));
+DEFINE_CLASS(mm_struct, struct mm_struct *, mmput(_T), get_task_mm(task), struct task_struct *task);
+DEFINE_CLASS(folio_get, struct folio *, folio_put(_T), ({ folio_get(folio); folio; }), struct folio *folio);
+DEFINE_CLASS(kmap, void *, kunmap(_T), kmap(page), struct page *page);
+// clang-format on
+
+static void usermode_helper_check_regions(struct kunit *test)
+{
+	CLASS(usermode_helper, h)();
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, h.task);
+	pr_info("%s: task=%p state=0x%x flags=0x%x mm=%p", __func__, h.task,
+		h.task->__state, h.task->flags, h.task->mm);
+
+	CLASS(mm_struct, mm)(h.task);
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, mm);
+
+	struct vm_area_struct *vmas[EXPECTED_REGIONS];
+	int found = 0;
+	// The creation of file backed region needs IO, so this may take a while.
+	for (int trial = 0; trial < 10; trial++,
+		 schedule_timeout_uninterruptible(msecs_to_jiffies(1000))) {
+		guard(mmap_read_lock)(mm);
+
+		found = usermode_helper_find_regions(&h, mm, vmas);
+		if (found == EXPECTED_REGIONS) {
+			// On success, we should hold the lock so that vmas is
+			// valid until we are done with them.
+			// We have to manually handle the un/locking here.
+			mmap_read_lock(mm);
+			break;
+		}
+	}
+	KUNIT_EXPECT_EQ(test, found, EXPECTED_REGIONS);
+
+	{
+		struct vm_area_struct *vma = vmas[DRAM_ANON_REGION];
+		KUNIT_EXPECT_NOT_ERR_OR_NULL(test, vma);
+		KUNIT_EXPECT_NULL(test, vma->vm_file);
+		KUNIT_EXPECT_TRUE(test, vma_is_anonymous(vma));
+		KUNIT_EXPECT_NOT_ERR_OR_NULL(test, vma->anon_vma);
+	}
+
+	{
+		struct vm_area_struct *vma = vmas[PMEM_ANON_REGION];
+		KUNIT_EXPECT_NOT_ERR_OR_NULL(test, vma);
+		KUNIT_EXPECT_NULL(test, vma->vm_file);
+		KUNIT_EXPECT_TRUE(test, vma_is_anonymous(vma));
+		KUNIT_EXPECT_NOT_ERR_OR_NULL(test, vma->anon_vma);
+	}
+
+	{
+		struct vm_area_struct *vma = vmas[DRAM_FILE_REGION];
+		KUNIT_EXPECT_NOT_ERR_OR_NULL(test, vma);
+		KUNIT_EXPECT_NOT_ERR_OR_NULL(test, vma->vm_file);
+		KUNIT_EXPECT_NULL(test, vma->anon_vma);
+		KUNIT_EXPECT_FALSE(test, vma_is_anonymous(vma));
+		KUNIT_EXPECT_FALSE(test, vma_is_anon_shmem(vma));
+	}
+
+	{
+		struct vm_area_struct *vma = vmas[PMEM_FILE_REGION];
+		KUNIT_EXPECT_NOT_ERR_OR_NULL(test, vma);
+		KUNIT_EXPECT_NOT_ERR_OR_NULL(test, vma->vm_file);
+		KUNIT_EXPECT_NULL(test, vma->anon_vma);
+		KUNIT_EXPECT_FALSE(test, vma_is_anonymous(vma));
+		KUNIT_EXPECT_FALSE(test, vma_is_anon_shmem(vma));
+	}
+
+	if (found == EXPECTED_REGIONS)
+		mmap_read_unlock(mm);
+}
+
+static void bench_follow_page(struct kunit *test)
+{
+	CLASS(usermode_helper, h)();
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, h.task);
+	CLASS(mm_struct, mm)(h.task);
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, mm);
+	schedule_timeout_uninterruptible(msecs_to_jiffies(6000));
+	guard(mmap_read_lock)(mm);
+	struct vm_area_struct *vmas[EXPECTED_REGIONS];
+	int found = usermode_helper_find_regions(&h, mm, vmas);
+	KUNIT_EXPECT_EQ(test, found, EXPECTED_REGIONS);
+
+	struct vm_area_struct *vma = vmas[DRAM_ANON_REGION];
+	KUNIT_EXPECT_TRUE(test, vma_is_anonymous(vma));
+
+	unsigned long begin = local_clock();
+	for (unsigned long addr = vma->vm_start; addr < vma->vm_end;
+	     addr += PAGE_SIZE) {
+		struct page *page __cleanup(follow_page_cleanup) =
+			follow_page(vma, addr, FOLL_GET | FOLL_DUMP);
+		KUNIT_EXPECT_NOT_ERR_OR_NULL(test, page);
+	}
+	unsigned long elapsed = local_clock() - begin,
+		      npages = (vma->vm_end - vma->vm_start) / PAGE_SIZE;
+	pr_info("%s: follow_page() speed test: npages=%lu elapsed=%lu avgtime=%lu",
+		__func__, npages, elapsed, elapsed / npages);
+	// struct folio *folio = page_folio(
+	// 	follow_page(vma, vma->vm_start, FOLL_GET | FOLL_DUMP));
+}
+
+static void exchange_test_folio_exchange(struct kunit *test, int src, int dst)
+{
+	CLASS(usermode_helper, h)();
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, h.task);
+	CLASS(mm_struct, mm)(h.task);
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, mm);
+	schedule_timeout_uninterruptible(msecs_to_jiffies(6000));
+	guard(mmap_read_lock)(mm);
+	struct vm_area_struct *vmas[EXPECTED_REGIONS];
+	int found = usermode_helper_find_regions(&h, mm, vmas);
+	KUNIT_EXPECT_EQ(test, found, EXPECTED_REGIONS);
+
+	struct vm_area_struct *src_vma = vmas[src];
+	struct vm_area_struct *dst_vma = vmas[dst];
+
+	struct folio *src_folio __cleanup(resolve_folio_cleanup) = page_folio(
+		follow_page(src_vma, src_vma->vm_start, FOLL_GET | FOLL_DUMP));
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, src_folio);
+	struct folio *dst_folio __cleanup(resolve_folio_cleanup) = page_folio(
+		follow_page(dst_vma, dst_vma->vm_start, FOLL_GET | FOLL_DUMP));
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, dst_folio);
+
+	CLASS(kmap, src_addr)(folio_page(src_folio, 0));
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, src_addr);
+	memset(src_addr, 'a', PAGE_SIZE);
+
+	CLASS(kmap, dst_addr)(folio_page(dst_folio, 0));
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, dst_addr);
+	memset(dst_addr, 'b', PAGE_SIZE);
+
+	// Try the exchange
+	int ret = folio_exchange(src_folio, dst_folio, MIGRATE_SYNC);
+	KUNIT_EXPECT_EQ(test, ret, 0);
+
+	void *buf = kvzalloc(PAGE_SIZE, GFP_KERNEL);
+	memset(buf, 'a', PAGE_SIZE);
+	KUNIT_EXPECT_EQ(test, memcmp(dst_addr, buf, PAGE_SIZE), 0);
+
+	memset(buf, 'b', PAGE_SIZE);
+	KUNIT_EXPECT_EQ(test, memcmp(src_addr, buf, PAGE_SIZE), 0);
+
+	kvfree(buf);
+}
+
+static void exchange_anon_anon(struct kunit *test)
+{
+	exchange_test_folio_exchange(test, DRAM_ANON_REGION, PMEM_ANON_REGION);
+}
+static void exchange_file_file(struct kunit *test)
+{
+	exchange_test_folio_exchange(test, DRAM_FILE_REGION, PMEM_FILE_REGION);
+}
+static void exchange_anon_file(struct kunit *test)
+{
+	exchange_test_folio_exchange(test, DRAM_ANON_REGION, PMEM_FILE_REGION);
+}
+static void exchange_file_anon(struct kunit *test)
+{
+	exchange_test_folio_exchange(test, DRAM_FILE_REGION, PMEM_ANON_REGION);
+}
+
+enum parallel_mode {
+	PARALLEL_SINGLE,
+	PARALLEL_2THREAD,
+	PARALLEL_4THREAD,
+	PARALLEL_8THREAD,
+	// PARALLEL_DMA,
+};
+extern int folio_exchange_parallel(struct folio *old, struct folio *new,
+				   enum migrate_mode mode,
+				   enum parallel_mode par);
+static void bench_folio_exchange_parallel(struct kunit *test,
+					  enum parallel_mode par)
+{
+	CLASS(usermode_helper, h)();
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, h.task);
+	CLASS(mm_struct, mm)(h.task);
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, mm);
+	schedule_timeout_uninterruptible(msecs_to_jiffies(6000));
+	guard(mmap_read_lock)(mm);
+	struct vm_area_struct *vmas[EXPECTED_REGIONS];
+	int found = usermode_helper_find_regions(&h, mm, vmas);
+	KUNIT_EXPECT_EQ(test, found, EXPECTED_REGIONS);
+
+	struct vm_area_struct *src_vma = vmas[DRAM_ANON_REGION];
+	struct vm_area_struct *dst_vma = vmas[PMEM_ANON_REGION];
+	KUNIT_EXPECT_EQ(test, src_vma->vm_end - src_vma->vm_start, REGION_SIZE);
+	KUNIT_EXPECT_EQ(test, src_vma->vm_end - src_vma->vm_start, REGION_SIZE);
+
+	lru_add_drain_all();
+
+	unsigned long begin = local_clock();
+	for (unsigned long offset = 0; offset < REGION_SIZE;
+	     offset += PAGE_SIZE) {
+		struct folio *src_folio __cleanup(resolve_folio_cleanup) =
+			page_folio(follow_page(src_vma,
+					       src_vma->vm_start + offset,
+					       FOLL_GET | FOLL_DUMP));
+		KUNIT_EXPECT_NOT_ERR_OR_NULL(test, src_folio);
+		struct folio *dst_folio __cleanup(resolve_folio_cleanup) =
+			page_folio(follow_page(dst_vma,
+					       dst_vma->vm_start + offset,
+					       FOLL_GET | FOLL_DUMP));
+		KUNIT_EXPECT_NOT_ERR_OR_NULL(test, dst_folio);
+
+		int ret = folio_exchange_parallel(src_folio, dst_folio,
+						  MIGRATE_SYNC, par);
+		KUNIT_EXPECT_EQ(test, ret, 0);
+	}
+	unsigned long elapsed = local_clock() - begin;
+	unsigned long npages = REGION_SIZE / PAGE_SIZE;
+	pr_info("%s: folio_exchange() speed test: npages=%lu elapsed=%lu avgtime=%lu",
+		__func__, npages, elapsed, elapsed / npages);
+}
+static void bench_folio_exchange_single(struct kunit *test)
+{
+	bench_folio_exchange_parallel(test, PARALLEL_SINGLE);
+}
+static void bench_folio_exchange_2thread(struct kunit *test)
+{
+	bench_folio_exchange_parallel(test, PARALLEL_2THREAD);
+}
+static void bench_folio_exchange_4thread(struct kunit *test)
+{
+	bench_folio_exchange_parallel(test, PARALLEL_4THREAD);
+}
+static void bench_folio_exchange_8thread(struct kunit *test)
+{
+	bench_folio_exchange_parallel(test, PARALLEL_8THREAD);
+}
+
+static void exchange_test_folio_migrate(struct kunit *test, int src, int dst)
+{
+	CLASS(usermode_helper, h)();
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, h.task);
+	CLASS(mm_struct, mm)(h.task);
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, mm);
+	schedule_timeout_uninterruptible(msecs_to_jiffies(6000));
+	guard(mmap_read_lock)(mm);
+	struct vm_area_struct *vmas[EXPECTED_REGIONS];
+	int found = usermode_helper_find_regions(&h, mm, vmas);
+	KUNIT_EXPECT_EQ(test, found, EXPECTED_REGIONS);
+
+	struct vm_area_struct *src_vma = vmas[src];
+	struct vm_area_struct *dst_vma = vmas[dst];
+
+	struct folio *src_folio __cleanup(resolve_folio_cleanup) = page_folio(
+		follow_page(src_vma, src_vma->vm_start, FOLL_GET | FOLL_DUMP));
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, src_folio);
+	struct folio *dst_folio __cleanup(resolve_folio_cleanup) = page_folio(
+		follow_page(dst_vma, dst_vma->vm_start, FOLL_GET | FOLL_DUMP));
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, dst_folio);
+
+	CLASS(kmap, src_addr)(folio_page(src_folio, 0));
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, src_addr);
+	memset(src_addr, 'a', PAGE_SIZE);
+
+	CLASS(kmap, dst_addr)(folio_page(dst_folio, 0));
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, dst_addr);
+	memset(dst_addr, 'b', PAGE_SIZE);
+
+	// Try the migrate
+	int src_nid = folio_nid(src_folio), dst_nid = folio_nid(dst_folio);
+	{
+		// dump_page(folio_page(src_folio, 0), NULL);
+		KUNIT_EXPECT_TRUE(test, folio_exchange_supported(src_folio,
+								 MIGRATE_SYNC));
+		int ret =
+			migrate_folio_to_node(src_folio, dst_nid, MIGRATE_SYNC);
+		KUNIT_EXPECT_EQ(test, ret, 0);
+	}
+	{
+		// dump_page(folio_page(dst_folio, 0), NULL);
+		KUNIT_EXPECT_TRUE(test, folio_exchange_supported(dst_folio,
+								 MIGRATE_SYNC));
+		int ret =
+			migrate_folio_to_node(dst_folio, src_nid, MIGRATE_SYNC);
+		KUNIT_EXPECT_EQ(test, ret, 0);
+	}
+}
+static void bimigrate_anon_anon(struct kunit *test)
+{
+	exchange_test_folio_migrate(test, DRAM_ANON_REGION, PMEM_ANON_REGION);
+}
+static void bimigrate_file_file(struct kunit *test)
+{
+	exchange_test_folio_migrate(test, DRAM_FILE_REGION, PMEM_FILE_REGION);
+}
+static void bimigrate_anon_file(struct kunit *test)
+{
+	exchange_test_folio_migrate(test, DRAM_ANON_REGION, PMEM_FILE_REGION);
+}
+static void bimigrate_file_anon(struct kunit *test)
+{
+	exchange_test_folio_migrate(test, DRAM_FILE_REGION, PMEM_ANON_REGION);
+}
+
+static void bench_folio_bimigrate(struct kunit *test)
+{
+	CLASS(usermode_helper, h)();
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, h.task);
+	CLASS(mm_struct, mm)(h.task);
+	KUNIT_EXPECT_NOT_ERR_OR_NULL(test, mm);
+	schedule_timeout_uninterruptible(msecs_to_jiffies(6000));
+	guard(mmap_read_lock)(mm);
+	struct vm_area_struct *vmas[EXPECTED_REGIONS];
+	int found = usermode_helper_find_regions(&h, mm, vmas);
+	KUNIT_EXPECT_EQ(test, found, EXPECTED_REGIONS);
+
+	struct vm_area_struct *src_vma = vmas[DRAM_ANON_REGION];
+	struct vm_area_struct *dst_vma = vmas[PMEM_ANON_REGION];
+	KUNIT_EXPECT_EQ(test, src_vma->vm_end - src_vma->vm_start, REGION_SIZE);
+	KUNIT_EXPECT_EQ(test, src_vma->vm_end - src_vma->vm_start, REGION_SIZE);
+
+	lru_add_drain_all();
+
+	unsigned long begin = local_clock();
+	for (unsigned long offset = 0; offset < REGION_SIZE;
+	     offset += PAGE_SIZE) {
+		struct folio *src_folio __cleanup(resolve_folio_cleanup) =
+			page_folio(follow_page(src_vma,
+					       src_vma->vm_start + offset,
+					       FOLL_GET | FOLL_DUMP));
+		KUNIT_EXPECT_NOT_ERR_OR_NULL(test, src_folio);
+		struct folio *dst_folio __cleanup(resolve_folio_cleanup) =
+			page_folio(follow_page(dst_vma,
+					       dst_vma->vm_start + offset,
+					       FOLL_GET | FOLL_DUMP));
+		KUNIT_EXPECT_NOT_ERR_OR_NULL(test, dst_folio);
+
+		int ret = folio_bimigrate(src_folio, dst_folio, MIGRATE_SYNC);
+		KUNIT_EXPECT_EQ(test, ret, 0);
+	}
+	unsigned long elapsed = local_clock() - begin;
+	unsigned long npages = REGION_SIZE / PAGE_SIZE;
+	pr_info("%s: folio_bimigrate() speed test: npages=%lu elapsed=%lu avgtime=%lu",
+		__func__, npages, elapsed, elapsed / npages);
+}
+
+static struct kunit_case exchange_test_cases[] = {
+	KUNIT_CASE(usermode_helper_start),
+	KUNIT_CASE_SLOW(usermode_helper_check_regions),
+	KUNIT_CASE_SLOW(exchange_anon_anon),
+	KUNIT_CASE_SLOW(exchange_file_file),
+	KUNIT_CASE_SLOW(exchange_file_anon),
+	KUNIT_CASE_SLOW(exchange_anon_file),
+	KUNIT_CASE_SLOW(bimigrate_anon_anon),
+	KUNIT_CASE_SLOW(bimigrate_file_file),
+	KUNIT_CASE_SLOW(bimigrate_file_anon),
+	KUNIT_CASE_SLOW(bimigrate_anon_file),
+	{},
+};
+
+static struct kunit_case exchange_bench_cases[] = {
+	KUNIT_CASE_SLOW(bench_follow_page),
+	KUNIT_CASE_SLOW(bench_folio_bimigrate),
+	KUNIT_CASE_SLOW(bench_folio_exchange_single),
+	KUNIT_CASE_SLOW(bench_folio_exchange_2thread),
+	KUNIT_CASE_SLOW(bench_folio_exchange_4thread),
+	KUNIT_CASE_SLOW(bench_folio_exchange_8thread),
+	{},
+};
+
+static struct kunit_suite exchange_test_suite = {
+	.name = "exchange",
+	.test_cases = exchange_test_cases,
+};
+// We have to build this as a module, otherwise we cannot access the
+// usermodehelper at the default kunit run time, which is at system startup and
+// is well before we have the root filesystem ready.
+kunit_test_suite(exchange_test_suite);
+
+static struct kunit_suite exchange_bench_suite = {
+	.name = "exchange_bench",
+	.test_cases = exchange_bench_cases,
+};
+kunit_test_suite(exchange_bench_suite);
+
+// static int __init init(void) { return 0; }
+// static void __exit exit(void) { }
+// module_init(init);
+// module_exit(exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Junliang Hu <jlhu@cse.cuhk.edu.hk>");
diff --git a/mm/gup.c b/mm/gup.c
index f1d6bc06eb52..eb11b64c1c86 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -1202,6 +1202,7 @@ struct page *follow_page(struct vm_area_struct *vma, unsigned long address,
 		put_dev_pagemap(ctx.pgmap);
 	return page;
 }
+EXPORT_SYMBOL(follow_page);
 
 static int get_gate_page(struct mm_struct *mm, unsigned long address,
 		unsigned int gup_flags, struct vm_area_struct **vma,
diff --git a/mm/demeter/Kconfig b/mm/demeter/Kconfig
new file mode 100644
index 000000000000..0dbe8d4cef21
--- /dev/null
+++ b/mm/demeter/Kconfig
@@ -0,0 +1,21 @@
+config DEMETER
+        tristate "Heterogeneous memory agent"
+        default m
+        select PRIME_NUMBERS
+        help
+          Enable heterogeneous memory guest agent to rebalance memory across different memory media.
+
+config DEMETER_TEST
+	tristate "Hetagent test" if !KUNIT_ALL_TESTS
+	default KUNIT_ALL_TESTS
+
+config EXCHANGE
+	bool "Page exchange"
+	default y
+	depends on MIGRATION
+
+config EXCHANGE_TEST
+	tristate "Page exchange test" if !KUNIT_ALL_TESTS
+	depends on EXCHANGE && KUNIT=y
+	default KUNIT_ALL_TESTS
+
diff --git a/mm/demeter/Makefile b/mm/demeter/Makefile
new file mode 100644
index 000000000000..2163b79d797f
--- /dev/null
+++ b/mm/demeter/Makefile
@@ -0,0 +1,14 @@
+ccflags-y += -DDEFAULT_SYMBOL_NAMESPACE=DEMETER
+
+obj-$(CONFIG_DEMETER) += demeter_placement.o
+
+demeter_placement-objs += module.o sysfs.o core.o vector.o
+
+# obj-$(CONFIG_DEMETER_TEST) += demeter_test.o
+
+# demeter_test-objs +=
+
+obj-$(CONFIG_DEMETER) += demeter_balloon.o
+
+demeter_balloon-objs += balloon.o
+
diff --git a/mm/demeter/balloon.c b/mm/demeter/balloon.c
new file mode 100644
index 000000000000..9c6d0c1bcf70
--- /dev/null
+++ b/mm/demeter/balloon.c
@@ -0,0 +1,839 @@
+#include <linux/virtio.h>
+#include <linux/virtio_balloon.h>
+#include <linux/swap.h>
+#include <linux/workqueue.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/balloon_compaction.h>
+#include <linux/oom.h>
+#include <linux/wait.h>
+#include <linux/mm.h>
+#include <linux/page_reporting.h>
+#include <linux/sched/clock.h>
+
+#define TRY(exp)                                                            \
+	({                                                                  \
+		__typeof__((exp)) __err = (exp);                            \
+		if ((u64)(__err) >= (u64)(-MAX_ERRNO)) {                    \
+			pr_err("%s:%d failed with error %lld:\n", __FILE__, \
+			       __LINE__, (s64)__err);                       \
+			dump_stack();                                       \
+			return (s64)(__err);                                \
+		}                                                           \
+		__err;                                                      \
+	})
+
+struct virtio_balloon_config_extended {
+	struct virtio_balloon_config base;
+	/* Number of heterogeneous pages host wants Guest to give up. */
+	__le32 num_hetero_pages;
+	/* Number of pages we've actually got in balloon. */
+	__le32 actual_hetero;
+};
+
+/*
+ * Balloon device works in 4K page units.  So each page is pointed to by
+ * multiple balloon pages.  All memory counters in this driver are in balloon
+ * page units.
+ */
+#define VIRTIO_BALLOON_PAGES_PER_PAGE \
+	(unsigned int)(PAGE_SIZE >> VIRTIO_BALLOON_PFN_SHIFT)
+#define VIRTIO_BALLOON_ARRAY_PFNS_MAX 256
+/* Maximum number of (4k) pages to deflate on OOM notifications. */
+#define VIRTIO_BALLOON_OOM_NR_PAGES 256
+#define VIRTIO_BALLOON_OOM_NOTIFY_PRIORITY 80
+
+static const struct virtio_device_id id_table[] = {
+	{ VIRTIO_ID_BALLOON, VIRTIO_DEV_ANY_ID },
+	{ 0 },
+};
+
+enum virtio_balloon_feature {
+	F_TELL_HOST = VIRTIO_BALLOON_F_MUST_TELL_HOST,
+	F_STATS = VIRTIO_BALLOON_F_STATS_VQ,
+	F_OOM = VIRTIO_BALLOON_F_DEFLATE_ON_OOM,
+	F_REPORT = VIRTIO_BALLOON_F_REPORTING,
+#define VIRTIO_BALLOON_F_HETERO_MEM \
+	6 /* Additional inflate/deflate queue for heterogeneous memory*/
+	F_HETERO = VIRTIO_BALLOON_F_HETERO_MEM,
+};
+
+// clang-format off
+static unsigned int features[] = {
+	F_TELL_HOST,
+	F_STATS,
+	F_OOM,
+	F_REPORT,
+	F_HETERO,
+};
+// clang-format on
+
+enum virtio_balloon_vq {
+	Q_INFLATE,
+	Q_DEFLATE,
+	Q_STATS,
+	// Q_FREE_PAGE,
+	Q_REPORTING,
+	Q_HETERO_INFLATE,
+	Q_HETERO_DEFLATE,
+	Q_MAX
+};
+
+enum virtio_balloon_inner_idx {
+	I_NORMAL,
+	I_HETERO,
+	I_MAX,
+};
+
+enum virtio_balloon_stats_tag {
+	T_SWAP_IN = VIRTIO_BALLOON_S_SWAP_IN,
+	T_SWAP_OUT = VIRTIO_BALLOON_S_SWAP_OUT,
+	T_MAJFLT = VIRTIO_BALLOON_S_MAJFLT,
+	T_MINFLT = VIRTIO_BALLOON_S_MINFLT,
+	T_MEMFREE = VIRTIO_BALLOON_S_MEMFREE,
+	T_MEMTOT = VIRTIO_BALLOON_S_MEMTOT,
+	T_AVAIL = VIRTIO_BALLOON_S_AVAIL,
+	T_CACHES = VIRTIO_BALLOON_S_CACHES,
+	T_HTLB_PGALLOC = VIRTIO_BALLOON_S_HTLB_PGALLOC,
+	T_HTLB_PGFAIL = VIRTIO_BALLOON_S_HTLB_PGFAIL,
+	T_NORMAL_ACCESS,
+	T_NORMAL_FREE,
+	T_NORMAL_TOTAL,
+	T_HETERO_ACCESS,
+	T_HETERO_FREE,
+	T_HETERO_TOTAL,
+	T_MAX,
+};
+
+typedef struct balloon_dev_info page_tracker_t;
+static void (*page_tracker_track)(page_tracker_t *tracker,
+				  struct page *page) = balloon_page_enqueue;
+static struct page *(*page_tracker_untrack)(page_tracker_t *tracker) =
+	balloon_page_dequeue;
+static inline void page_tracker_init(page_tracker_t *tracker)
+{
+	balloon_devinfo_init(tracker);
+}
+
+struct virtio_balloon {
+	struct virtio_device *vdev;
+	struct virtqueue *vqs[Q_MAX];
+	struct work_struct work[Q_MAX];
+	struct virtio_balloon_tracepoints {
+		u64 total_elapsed, work_elapsed;
+	} tracepoints[Q_MAX];
+	// make sure no new work are queued when stopping the device
+	spinlock_t queue_work;
+	atomic_t should_exit;
+	struct notifier_block oom_notification;
+	wait_queue_head_t ack;
+	struct virtio_balloon_inner {
+		struct mutex lock;
+		// The actual size of pages in the balloon
+		u32 len;
+		// All the pages we have returned to the host
+		page_tracker_t tracking;
+		// Temporary storage for communicating with the host
+		u32 pfns[VIRTIO_BALLOON_ARRAY_PFNS_MAX];
+	} inner[I_MAX];
+	struct virtio_balloon_stat_vec {
+		u32 len;
+		struct virtio_balloon_stat items[T_MAX];
+	} stats;
+};
+
+static u32 vb_config_read_target(struct virtio_balloon *vb, u32 idx)
+{
+	u32 target = 0;
+	switch (idx) {
+	case I_NORMAL:
+		virtio_cread_le(vb->vdev, struct virtio_balloon_config_extended,
+				base.num_pages, &target);
+		break;
+	case I_HETERO:
+		virtio_cread_le(vb->vdev, struct virtio_balloon_config_extended,
+				num_hetero_pages, &target);
+		break;
+	default:
+		dev_err(&vb->vdev->dev,
+			"%s failure: requested sub-ballon does not exit\n",
+			__func__);
+		BUG();
+	}
+	return target;
+}
+
+static void vb_config_write_actual(struct virtio_balloon *vb, u32 idx,
+				   u32 actual)
+{
+	switch (idx) {
+	case I_NORMAL:
+		virtio_cwrite_le(vb->vdev,
+				 struct virtio_balloon_config_extended,
+				 base.actual, &actual);
+		break;
+	case I_HETERO:
+		virtio_cwrite_le(vb->vdev,
+				 struct virtio_balloon_config_extended,
+				 actual_hetero, &actual);
+		break;
+	default:
+		dev_err(&vb->vdev->dev,
+			"%s failure: requested sub-ballon does not exit\n",
+			__func__);
+		BUG();
+	}
+}
+
+static void vb_callback_ack(struct virtqueue *vq)
+{
+	struct virtio_balloon *vb = vq->vdev->priv;
+	wake_up(&vb->ack);
+}
+
+static void vb_callback_stats_request(struct virtqueue *vq)
+{
+	struct virtio_balloon *vb = vq->vdev->priv;
+
+	scoped_guard(spinlock_irqsave, &vb->queue_work)
+	{
+		if (atomic_read(&vb->should_exit)) {
+			return;
+		}
+		queue_work(system_freezable_wq, &vb->work[Q_STATS]);
+	}
+}
+
+static bool vb_acked(struct virtio_balloon *vb, u32 feature)
+{
+	return virtio_has_feature(vb->vdev, feature);
+}
+
+static int vb_send_buf(struct virtio_balloon *vb, u32 qidx, void *buf, u32 len)
+{
+	struct virtqueue *vq = vb->vqs[qidx];
+	struct scatterlist sg;
+	sg_init_one(&sg, buf, len);
+	TRY(virtqueue_add_outbuf(vq, &sg, 1, vb, GFP_KERNEL));
+	virtqueue_kick(vq);
+	return 0;
+}
+
+static void *vb_recv_buf(struct virtio_balloon *vb, u32 qidx, u32 *len)
+{
+	struct virtqueue *vq = vb->vqs[qidx];
+	u32 _len;
+	// no data should be associated with used buffer for all balloon vq
+	return virtqueue_get_buf(vq, len ? len : &_len);
+}
+
+static s64 vb_inner_diff_from_target(struct virtio_balloon *vb, u32 idx)
+{
+	BUG_ON(idx != I_NORMAL && idx != I_HETERO);
+	struct virtio_balloon_inner *inner = &vb->inner[idx];
+	mutex_lock(&inner->lock);
+	s64 target = vb_config_read_target(vb, idx);
+	s64 diff = target - inner->len;
+	mutex_unlock(&inner->lock);
+	// dev_info(&vb->vdev->dev, "%s: idx=%u, target=%lld, has=%u, diff=%lld\n",
+	// 	 __func__, idx, target, inner->len, diff);
+	return diff;
+}
+
+static struct page *vb_inner_page_alloc(struct virtio_balloon *vb, u64 idx)
+{
+	int nid = NUMA_NO_NODE;
+	switch (idx) {
+	case I_NORMAL:
+		nid = first_node(node_states[N_MEMORY]);
+		break;
+	case I_HETERO:
+		nid = last_node(node_states[N_MEMORY]);
+		break;
+	default:
+		dev_err(&vb->vdev->dev,
+			"%s failure: requested sub-ballon does not exit\n",
+			__func__);
+		BUG();
+	}
+	return alloc_pages_node(nid,
+				balloon_mapping_gfp_mask() | __GFP_NOMEMALLOC |
+					__GFP_NORETRY | __GFP_NOWARN,
+				0);
+}
+
+static u32 vb_inner_inflate(struct virtio_balloon *vb, u32 idx, u32 todo)
+{
+	BUG_ON(idx != I_NORMAL && idx != I_HETERO);
+	u32 qidx = idx == I_NORMAL ? Q_INFLATE : Q_HETERO_INFLATE;
+
+	struct virtio_balloon_inner *inner = &vb->inner[idx];
+	todo = min(todo, (u32)ARRAY_SIZE(vb->inner[idx].pfns));
+
+	// allocate pages without holding the lock
+	struct list_head pages = LIST_HEAD_INIT(pages);
+	while (todo-- > 0) {
+		struct page *page = vb_inner_page_alloc(vb, idx);
+		if (!page) {
+			dev_info_ratelimited(
+				&vb->vdev->dev,
+				"%s failure: Out of puff! Can't get pages\n",
+				__func__);
+			msleep(200);
+			break;
+		}
+		list_add(&page->lru, &pages);
+	}
+
+	mutex_lock(&inner->lock);
+
+	u32 done = 0;
+	struct page *page, *next;
+	list_for_each_entry_safe(page, next, &pages, lru) {
+		page_tracker_track(&inner->tracking, page);
+		inner->pfns[done++] = page_to_pfn(page);
+	}
+	vb_send_buf(vb, qidx, inner->pfns, sizeof(*inner->pfns) * done);
+	wait_event(vb->ack, vb_recv_buf(vb, qidx, NULL));
+	inner->len += done;
+	vb_config_write_actual(vb, idx, inner->len);
+
+	mutex_unlock(&inner->lock);
+	return done;
+}
+
+static u32 vb_inner_deflate(struct virtio_balloon *vb, u32 idx, u32 todo)
+{
+	BUG_ON(idx != I_NORMAL && idx != I_HETERO);
+	u32 qidx = idx == I_NORMAL ? Q_DEFLATE : Q_HETERO_DEFLATE;
+	struct virtio_balloon_inner *inner = &vb->inner[idx];
+	todo = min(todo, inner->len);
+	mutex_lock(&inner->lock);
+	u32 done = 0;
+	struct list_head pages = LIST_HEAD_INIT(pages);
+	while (done < todo) {
+		struct page *page = page_tracker_untrack(&inner->tracking);
+		if (!page)
+			break;
+		inner->pfns[done++] = page_to_pfn(page);
+		list_add(&page->lru, &pages);
+	}
+	vb_send_buf(vb, qidx, inner->pfns, sizeof(*inner->pfns) * done);
+	wait_event(vb->ack, vb_recv_buf(vb, qidx, NULL));
+	inner->len -= done;
+	vb_config_write_actual(vb, idx, inner->len);
+
+	struct page *page, *next;
+	list_for_each_entry_safe(page, next, &pages, lru) {
+		list_del(&page->lru);
+		put_page(page);
+	}
+
+	mutex_unlock(&inner->lock);
+	return done;
+}
+
+int vb_stat_push(struct virtio_balloon *vb, u16 tag, u64 val)
+{
+	struct virtio_balloon_stat_vec *vec = &vb->stats;
+	if (vec->len >= ARRAY_SIZE(vec->items))
+		return -EINVAL;
+	vec->items[vec->len++] =
+		(struct virtio_balloon_stat){ .tag = tag, .val = val };
+	return 0;
+}
+
+void vb_stat_clear(struct virtio_balloon *vb)
+{
+	struct virtio_balloon_stat_vec *vec = &vb->stats;
+	vec->len = 0;
+}
+
+static void vb_stats(struct virtio_balloon *vb)
+{
+	unsigned long events[NR_VM_EVENT_ITEMS] = {};
+	all_vm_events(events);
+	struct sysinfo global = {}, normal = {}, hetero = {};
+	si_meminfo(&global);
+	si_meminfo_node(&normal, first_node(node_states[N_MEMORY]));
+	si_meminfo_node(&hetero, last_node(node_states[N_MEMORY]));
+
+	// clang-format off
+	u64 items[T_MAX] = {
+		[T_SWAP_IN]       = events[PSWPIN],
+		[T_SWAP_OUT]      = events[PSWPOUT],
+		[T_MAJFLT]        = events[PGMAJFAULT],
+		[T_MINFLT]        = events[PGFAULT],
+		[T_MEMFREE]       = global.freeram * global.mem_unit,
+		[T_MEMTOT]        = global.totalram * global.mem_unit,
+		[T_AVAIL]         = si_mem_available() << PAGE_SHIFT,
+		[T_CACHES]        = global_node_page_state(NR_FILE_PAGES) << PAGE_SHIFT,
+		[T_HTLB_PGALLOC]  = events[HTLB_BUDDY_PGALLOC],
+		[T_HTLB_PGFAIL]   = events[HTLB_BUDDY_PGALLOC_FAIL],
+		// FIXME: DRAM access accounting
+		// [T_NORMAL_ACCESS] = events[DRAM_ACCESS],
+		[T_NORMAL_FREE]   = normal.freeram * normal.mem_unit,
+		[T_NORMAL_TOTAL]  = normal.totalram * normal.mem_unit,
+		// FIXME: PMEM access accounting
+		// [T_HETERO_ACCESS] = events[PMEM_ACCESS],
+		[T_HETERO_FREE]   = hetero.freeram * hetero.mem_unit,
+		[T_HETERO_TOTAL]  = hetero.totalram * hetero.mem_unit,
+	};
+	// clang-format on
+
+	vb_stat_clear(vb);
+	for (u64 i = 0; i < ARRAY_SIZE(items); ++i) {
+		vb_stat_push(vb, i, items[i]);
+	}
+
+	vb_send_buf(vb, Q_STATS, vb->stats.items,
+		    sizeof(*vb->stats.items) * ARRAY_SIZE(vb->stats.items));
+}
+
+static void vb_stats_initial(struct virtio_balloon *vb)
+{
+	if (!vb_acked(vb, F_STATS))
+		return;
+	vb_stats(vb);
+	dev_info(&vb->vdev->dev, "%s done\n", __func__);
+}
+
+static void vb_work_fn_inflate(struct work_struct *work)
+{
+	struct virtio_balloon *vb =
+		container_of(work, struct virtio_balloon, work[Q_INFLATE]);
+	struct virtio_balloon_tracepoints *t = &vb->tracepoints[Q_INFLATE];
+
+	u64 chunk_begin = local_clock(),
+	    *work_elapsed = &t->work_elapsed,
+	    *total_elapsed = &t->total_elapsed;
+
+	s64 todo = vb_inner_diff_from_target(vb, I_NORMAL);
+	if (todo <= 0)
+		return;
+	u32 done = vb_inner_inflate(vb, I_NORMAL, todo);
+
+	*work_elapsed += local_clock() - chunk_begin;
+
+	if (done < todo) {
+		queue_work(system_freezable_wq, &vb->work[Q_INFLATE]);
+	} else {
+		dev_info(&vb->vdev->dev, "%s: took %llu ms\n", __func__,
+			 *work_elapsed / 1000 / 1000);
+		*total_elapsed += *work_elapsed;
+		*work_elapsed = 0;
+	}
+}
+
+static void vb_work_fn_deflate(struct work_struct *work)
+{
+	struct virtio_balloon *vb =
+		container_of(work, struct virtio_balloon, work[Q_DEFLATE]);
+	struct virtio_balloon_tracepoints *t = &vb->tracepoints[Q_DEFLATE];
+
+	u64 chunk_begin = local_clock(),
+	    *work_elapsed = &t->work_elapsed,
+	    *total_elapsed = &t->total_elapsed;
+
+	s64 todo = -vb_inner_diff_from_target(vb, I_NORMAL);
+	if (todo <= 0)
+		return;
+	u32 done = vb_inner_deflate(vb, I_NORMAL, todo);
+
+	*work_elapsed += local_clock() - chunk_begin;
+
+	if (done < todo) {
+		queue_work(system_freezable_wq, &vb->work[Q_DEFLATE]);
+	} else {
+		dev_info(&vb->vdev->dev, "%s: took %llu ms\n", __func__,
+			 *work_elapsed / 1000 / 1000);
+		*total_elapsed += *work_elapsed;
+		*work_elapsed = 0;
+	}
+}
+
+static void vb_work_fn_stats(struct work_struct *work)
+{
+	struct virtio_balloon *vb =
+		container_of(work, struct virtio_balloon, work[Q_STATS]);
+
+	// We can only reach here by the user buffer notification callback.
+	// So we first need to remove that buffer
+	vb_recv_buf(vb, Q_STATS, NULL);
+	vb_stats(vb);
+}
+
+static void vb_work_fn_reporting(struct work_struct *work)
+{
+	struct virtio_balloon *vb =
+		container_of(work, struct virtio_balloon, work[Q_REPORTING]);
+
+	// TODO
+}
+
+static void vb_work_fn_hetero_inflate(struct work_struct *work)
+{
+	struct virtio_balloon *vb = container_of(work, struct virtio_balloon,
+						 work[Q_HETERO_INFLATE]);
+	struct virtio_balloon_tracepoints *t =
+		&vb->tracepoints[Q_HETERO_INFLATE];
+	u64 chunk_begin = local_clock(),
+	    *work_elapsed = &t->work_elapsed,
+	    *total_elapsed = &t->total_elapsed;
+	s64 todo = vb_inner_diff_from_target(vb, I_HETERO);
+	if (todo <= 0)
+		return;
+	u32 done = vb_inner_inflate(vb, I_HETERO, todo);
+	*work_elapsed += local_clock() - chunk_begin;
+	if (done < todo) {
+		queue_work(system_freezable_wq, &vb->work[Q_HETERO_INFLATE]);
+	} else {
+		dev_info(&vb->vdev->dev, "%s: took %llu ms\n", __func__,
+			 *work_elapsed / 1000 / 1000);
+		*total_elapsed += *work_elapsed;
+		*work_elapsed = 0;
+	}
+}
+
+static void vb_work_fn_hetero_deflate(struct work_struct *work)
+{
+	struct virtio_balloon *vb = container_of(work, struct virtio_balloon,
+						 work[Q_HETERO_DEFLATE]);
+	struct virtio_balloon_tracepoints *t =
+		&vb->tracepoints[Q_HETERO_DEFLATE];
+	u64 chunk_begin = local_clock(),
+	    *work_elapsed = &t->work_elapsed,
+	    *total_elapsed = &t->total_elapsed;
+	s64 todo = -vb_inner_diff_from_target(vb, I_HETERO);
+	if (todo <= 0)
+		return;
+	u32 done = vb_inner_deflate(vb, I_HETERO, todo);
+	*work_elapsed += local_clock() - chunk_begin;
+	if (done < todo) {
+		queue_work(system_freezable_wq, &vb->work[Q_HETERO_DEFLATE]);
+	} else {
+		dev_info(&vb->vdev->dev, "%s: took %llu ms\n", __func__,
+			 *work_elapsed / 1000 / 1000);
+		*total_elapsed += *work_elapsed;
+		*work_elapsed = 0;
+	}
+}
+
+static void vb_work_queue(struct virtio_balloon *vb)
+{
+	// clang-format off
+	struct work_struct *works[Q_MAX] = {
+		[Q_INFLATE] = &vb->work[Q_INFLATE],
+		[Q_DEFLATE] = &vb->work[Q_DEFLATE],
+		// CAVEAT: driven by the host's used buffer notification
+		// [Q_STATS] = vb_acked(vb, F_STATS) ? &vb->work[Q_STATS] : NULL,
+		[Q_HETERO_INFLATE] = vb_acked(vb, F_HETERO) ? &vb->work[Q_HETERO_INFLATE] : NULL,
+		[Q_HETERO_DEFLATE] = vb_acked(vb, F_HETERO) ? &vb->work[Q_HETERO_DEFLATE] : NULL,
+	};
+	// clang-format on
+	scoped_guard(spinlock_irqsave, &vb->queue_work)
+	{
+		if (atomic_read(&vb->should_exit)) {
+			return;
+		}
+		for (u64 i = 0; i < ARRAY_SIZE(works); ++i) {
+			if (!works[i] || !works[i]->func) {
+				continue;
+			}
+			queue_work(system_freezable_wq, works[i]);
+		}
+	}
+	dev_info(&vb->vdev->dev, "%s done\n", __func__);
+}
+
+static void vb_work_stop(struct virtio_balloon *vb)
+{
+	scoped_guard(spinlock_irqsave, &vb->queue_work)
+	{
+		atomic_set(&vb->should_exit, 1);
+		for (u64 i = 0; i < ARRAY_SIZE(vb->work); ++i) {
+			struct work_struct *work = &vb->work[i];
+			if (!work->func)
+				continue;
+			cancel_work_sync(work);
+		}
+	}
+}
+
+static int vb_work_init(struct virtio_balloon *vb)
+{
+	dev_info(&vb->vdev->dev, "%s started\n", __func__);
+
+	// clang-format off
+	struct work_struct *works[Q_MAX] = {
+		[Q_INFLATE]   = &vb->work[Q_INFLATE],
+		[Q_DEFLATE]   = &vb->work[Q_DEFLATE],
+		[Q_STATS]     = vb_acked(vb, F_STATS) ? &vb->work[Q_STATS] : NULL,
+		[Q_REPORTING] = vb_acked(vb, F_REPORT) ? &vb->work[Q_REPORTING] : NULL,
+		[Q_HETERO_INFLATE] = vb_acked(vb, F_HETERO) ? &vb->work[Q_HETERO_INFLATE] : NULL,
+		[Q_HETERO_DEFLATE] = vb_acked(vb, F_HETERO) ? &vb->work[Q_HETERO_DEFLATE] : NULL,
+	};
+	work_func_t fns[Q_MAX] = {
+		[Q_INFLATE]   = vb_work_fn_inflate,
+		[Q_DEFLATE]   = vb_work_fn_deflate,
+		[Q_STATS]     = vb_acked(vb, F_STATS) ? vb_work_fn_stats : NULL,
+		[Q_REPORTING] = vb_acked(vb, F_REPORT) ? vb_work_fn_reporting : NULL,
+		[Q_HETERO_INFLATE] = vb_acked(vb, F_HETERO) ? vb_work_fn_hetero_inflate : NULL,
+		[Q_HETERO_DEFLATE] = vb_acked(vb, F_HETERO) ? vb_work_fn_hetero_deflate : NULL,
+	};
+	// clang-format on
+	for (u64 i = 0; i < ARRAY_SIZE(works); ++i) {
+		if (!works[i] || !fns[i]) {
+			continue;
+		}
+		INIT_WORK(works[i], fns[i]);
+	}
+	spin_lock_init(&vb->queue_work);
+	dev_info(&vb->vdev->dev, "%s done\n", __func__);
+	return 0;
+}
+
+static int vb_vqs_init(struct virtio_balloon *vb)
+{
+	dev_info(&vb->vdev->dev, "%s started\n", __func__);
+
+	// clang-format off
+	vq_callback_t *callbacks[Q_MAX] = {
+		[Q_INFLATE]   = vb_callback_ack,
+		[Q_DEFLATE]   = vb_callback_ack,
+		[Q_STATS]     = vb_acked(vb, F_STATS) ? vb_callback_stats_request : NULL,
+		[Q_REPORTING] = vb_acked(vb, F_REPORT) ? vb_callback_ack : NULL,
+		[Q_HETERO_INFLATE] = vb_acked(vb, F_HETERO) ? vb_callback_ack : NULL,
+		[Q_HETERO_DEFLATE] = vb_acked(vb, F_HETERO) ? vb_callback_ack : NULL,
+	};
+	char const *names[Q_MAX] = {
+		[Q_INFLATE]   = "inflate",
+		[Q_DEFLATE]   = "deflate",
+		[Q_STATS]     = vb_acked(vb, F_STATS) ? "stats" : NULL,
+		[Q_REPORTING] = vb_acked(vb, F_REPORT) ? "reporting" : NULL,
+		[Q_HETERO_INFLATE] = vb_acked(vb, F_HETERO) ? "hetero-inflate" : NULL,
+		[Q_HETERO_DEFLATE] = vb_acked(vb, F_HETERO) ? "hetero-deflate" : NULL,
+	};
+	// clang-format on
+
+	TRY(virtio_find_vqs(vb->vdev, ARRAY_SIZE(vb->vqs), vb->vqs, callbacks,
+			    names, NULL));
+
+	dev_info(&vb->vdev->dev, "%s done\n", __func__);
+	return 0;
+}
+static void vb_vqs_drop(struct virtio_balloon *vb)
+{
+	vb->vdev->config->del_vqs(vb->vdev);
+}
+
+static int vb_oom(struct notifier_block *nb, unsigned long _0, void *freed)
+{
+	struct virtio_balloon *vb =
+		container_of(nb, struct virtio_balloon, oom_notification);
+
+	u32 idx = vb_acked(vb, F_HETERO) ? I_HETERO : I_NORMAL;
+	pr_warn_ratelimited("%s: deflate %d pages via %s vq\n", __func__,
+			    VIRTIO_BALLOON_OOM_NR_PAGES,
+			    idx == I_NORMAL ? "normal" : "hetero");
+	vb_inner_deflate(vb, idx, VIRTIO_BALLOON_OOM_NR_PAGES);
+	return NOTIFY_OK;
+}
+
+static int vb_init(struct virtio_balloon *vb, struct virtio_device *vdev)
+{
+	memset(vb, 0, sizeof(*vb));
+	vb->vdev = vdev;
+	vdev->priv = vb;
+	dev_info(&vdev->dev, "%s started: vb=0x%px\n", __func__, vb);
+
+	atomic_set(&vb->should_exit, 0);
+
+	TRY(vb_vqs_init(vb));
+	TRY(vb_work_init(vb));
+
+	int err = 0;
+	if (vb_acked(vb, F_OOM)) {
+		vb->oom_notification.notifier_call = vb_oom;
+		vb->oom_notification.priority =
+			VIRTIO_BALLOON_OOM_NOTIFY_PRIORITY;
+		err = register_oom_notifier(&vb->oom_notification);
+		if (err)
+			goto err_oom;
+	}
+	init_waitqueue_head(&vb->ack);
+	for (u64 i = 0; i < ARRAY_SIZE(vb->inner); ++i) {
+		struct virtio_balloon_inner *inner = &vb->inner[i];
+		mutex_init(&inner->lock);
+		page_tracker_init(&inner->tracking);
+	}
+	virtio_device_ready(vdev);
+	dev_info(&vdev->dev, "virtio-balloon device registered\n");
+	// stats queue require an initial stat item to kick-start
+	vb_stats_initial(vb);
+	// inflate/deflation starts as soon as balloon is ready
+	vb_work_queue(vb);
+
+	dev_info(&vb->vdev->dev, "%s done\n", __func__);
+	return 0;
+
+err_oom:
+	if (vb_acked(vb, F_OOM))
+		unregister_oom_notifier(&vb->oom_notification);
+err_vqs_drop:
+	vb_vqs_drop(vb);
+	return err;
+}
+
+static void vb_stop(struct virtio_balloon *vb)
+{
+	if (vb_acked(vb, F_REPORT)) {
+		// TODO: page_reporting_unregister()
+	}
+	if (vb_acked(vb, F_OOM)) {
+		unregister_oom_notifier(&vb->oom_notification);
+	}
+	vb_work_stop(vb);
+	for (u64 i = 0; i < ARRAY_SIZE(vb->inner); ++i) {
+		struct virtio_balloon_inner *inner = &vb->inner[i];
+		vb_inner_deflate(vb, i, inner->len);
+	}
+}
+
+static void vb_reset(struct virtio_balloon *vb)
+{
+	virtio_reset_device(vb->vdev);
+}
+
+static void vb_drop(struct virtio_balloon *vb)
+{
+	vb_stop(vb);
+	// TODO: mutex_destroy
+	// for (u64 i = 0; i < ARRAY_SIZE(vb->inner); ++i) {
+	// 	struct virtio_balloon_inner *inner = &vb->inner[i];
+	// 	mutex_destroy(&inner->lock);
+	// }
+	vb_reset(vb);
+	vb_vqs_drop(vb);
+	return;
+}
+
+static int validate(struct virtio_device *vdev)
+{
+	BUILD_BUG_ON(PAGE_SHIFT != VIRTIO_BALLOON_PFN_SHIFT);
+
+	if (!vdev->config->get) {
+		dev_err(&vdev->dev, "%s failure: config access disabled\n",
+			__func__);
+		return -EINVAL;
+	}
+
+	if (virtio_has_feature(vdev, F_HETERO) &&
+	    num_node_state(N_MEMORY) < 2) {
+		dev_err(&vdev->dev,
+			"%s failure: no heterogeneous memory presents\n",
+			__func__);
+		return -EINVAL;
+	}
+
+	__virtio_clear_bit(vdev, VIRTIO_F_ACCESS_PLATFORM);
+	return 0;
+}
+
+struct virtio_balloon *__global_instance = NULL;
+ulong __node_avail_pages(int nid)
+{
+	// See: vb_inner_page_alloc
+	struct virtio_balloon *vb = __global_instance;
+	struct virtio_balloon_inner *inner = NULL;
+	if (!vb) {
+		return node_present_pages(nid);
+	} else if (nid == first_node(node_states[N_MEMORY])) {
+		inner = &vb->inner[I_NORMAL];
+	} else if (nid == last_node(node_states[N_MEMORY])) {
+		inner = &vb->inner[I_HETERO];
+	} else {
+		return node_present_pages(nid);
+	}
+	return node_present_pages(nid) - inner->len;
+}
+EXPORT_SYMBOL_GPL(__node_avail_pages);
+
+ulong node_balloon_pages(int nid)
+{
+	struct virtio_balloon *vb = __global_instance;
+	struct virtio_balloon_inner *inner = NULL;
+	if (!vb) {
+		return -ENXIO;
+	} else if (nid == first_node(node_states[N_MEMORY])) {
+		inner = &vb->inner[I_NORMAL];
+	} else if (nid == last_node(node_states[N_MEMORY])) {
+		inner = &vb->inner[I_HETERO];
+	} else {
+		return -EINVAL;
+	}
+	return inner->len;
+}
+EXPORT_SYMBOL_GPL(node_balloon_pages);
+
+static int probe(struct virtio_device *vdev)
+{
+	if (!vdev) {
+		return -EINVAL;
+	}
+	if (__global_instance) {
+		return -EBUSY;
+	}
+	struct virtio_balloon *vb = kvzalloc(sizeof(*vb), GFP_KERNEL);
+	if (!vb) {
+		return -ENOMEM;
+	}
+
+	TRY(vb_init(vb, vdev));
+	__global_instance = vb;
+	return 0;
+}
+
+static void config_changed(struct virtio_device *vdev)
+{
+	struct virtio_balloon *vb = vdev->priv;
+	vb_work_queue(vb);
+}
+
+static void remove(struct virtio_device *vdev)
+{
+	struct virtio_balloon *vb = vdev->priv;
+
+	__global_instance = NULL;
+	vb_drop(vb);
+	kvfree(vb);
+}
+
+static int freeze(struct virtio_device *vdev)
+{
+	return -EINVAL;
+}
+
+static int restore(struct virtio_device *vdev)
+{
+	return -EINVAL;
+}
+
+static struct virtio_driver virtio_balloon_driver = {
+	.feature_table = features,
+	.feature_table_size = ARRAY_SIZE(features),
+	.driver.name = KBUILD_MODNAME,
+	.driver.owner = THIS_MODULE,
+	.id_table = id_table,
+	.validate = validate,
+	.probe = probe,
+	.remove = remove,
+	.config_changed = config_changed,
+};
+
+module_virtio_driver(virtio_balloon_driver);
+MODULE_DEVICE_TABLE(virtio, id_table);
+MODULE_AUTHOR("Junliang Hu <jlhu@cse.cuhk.edu.hk>");
+MODULE_DESCRIPTION("Enhanced Virtio balloon driver");
+MODULE_LICENSE("GPL");
diff --git a/mm/demeter/core.c b/mm/demeter/core.c
new file mode 100644
index 000000000000..501051086254
--- /dev/null
+++ b/mm/demeter/core.c
@@ -0,0 +1,908 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2021-2024 Junliang Hu
+ *
+ * Author: Junliang Hu <jlhu@cse.cuhk.edu.hk>
+ *
+ */
+
+#include <linux/sched/cputime.h>
+#include <linux/sched/clock.h>
+#include <linux/mm.h>
+#include <linux/mm_inline.h>
+#include <linux/sort.h>
+#include <../internal.h>
+
+#include "error.h"
+#include "demeter.h"
+#include "pebs.h"
+#include "module.h"
+#include "mpsc.h"
+#include "range_tree.h"
+#include "hashmap.h"
+
+enum target_worker {
+	// Main thread initiate the range splitting and merging by notifying the
+	// policy via CHAN_SPLIT_REQ
+	WORKER_MAIN,
+	// Throttle thread periodically enable the perf events and disable them
+	// in a PWM like fashion
+	WORKER_THROTTLE,
+	// Policy thread is responsible for the heavy lifting of process samples
+	// and update range accounting/splitting/merging and initiate migration
+	WORKER_POLICY,
+	// Migration thread carries out the actual migration of folios
+	WORKER_MIGRATION,
+	MAX_WORKERS
+};
+
+enum target_chan {
+	// Can only be consumed by WORKER_POLICY
+	CHAN_SAMPLE,
+	// Can only be consumed by WORKER_MIGRATION
+	CHAN_EXCG_REQ,
+	// Can only be consumed by WORKER_POLICY
+	CHAN_EXCG_RSP,
+	// Can only be consumed by WORKER_POLICY
+	CHAN_SPLT_REQ,
+	MAX_CHANS,
+};
+
+enum target_param {
+	POLICY_MSET_BUCKET = 32,
+	POLICY_TSET_BUCKET = 32,
+	POLICY_BSET_BUCKET = 32,
+	MPSC_RETRY = 2,
+	MPSC_MAX_SIZE_BYTE = 1 << 20,
+	MPSC_MAX_BATCH = 16384,
+	RMT_INITIAL_SIZE_FACTOR = 2,
+	RMT_GRANULARITY = 2ul << 20,
+	RMT_MIN_SIZE = 3,
+	RMT_MAX_SIZE = 256,
+	RMT_SPLIT_FACTOR = 1,
+	// Fit at most MIGRATION_WMARK% of the total capacity during migration
+	MIGRATION_WMARK = 90,
+	MIGRAION_MAX_BATCH = (64ul << 20) / PAGE_SIZE,
+	MIGRATION_BSET_BUCKET = 32,
+	MIGRATION_BSET_BACKOFF = 128,
+};
+
+enum target_stat {
+	STAT_OVERFLOW_HANDLER,
+	STAT_THROTTLE,
+	STAT_POLICY,
+	STAT_MIGRATION,
+	STAT_PERF_PREPARE,
+	STAT_SPLIT,
+	MAX_STATS,
+};
+static char *const target_stat_name[] = {
+	"overflow_handler", "throttle",	    "policy",
+	"migration",	    "perf_prepare", "split",
+};
+
+struct target {
+	// Currently managed task
+	struct task_struct *victim;
+	// These channels are the only sharing states between the workers
+	mpsc_t chans[MAX_CHANS];
+	struct task_struct *workers[MAX_WORKERS];
+	// Should only be used by the throttle and main thread
+	struct perf_event *events[MAX_EVENTS];
+
+	atomic_long_t stats[MAX_STATS];
+	// Should only used by the new() and drop()
+	u64 start_time;
+};
+
+// External dependencies
+extern int folio_exchange_isolated(struct folio *, struct folio *,
+				   enum migrate_mode);
+
+// Internal helpers
+static struct folio *uvirt_to_folio(struct mm_struct *mm, u64 user_addr);
+
+static inline u64 task_clock(void)
+{
+	u64 utime, stime = 0;
+	task_cputime_adjusted(current, &utime, &stime);
+	return stime;
+}
+struct stat_stopwatch {
+	struct target *target;
+	u64 (*clock)(void);
+	u64 start;
+	u64 item;
+};
+static struct stat_stopwatch stopwatch_new(struct target *target,
+					   u64 (*clock)(void), u64 item)
+{
+	return (struct stat_stopwatch){
+		.target = target,
+		.clock = clock,
+		.item = item,
+		.start = clock(),
+	};
+}
+static void stopwatch_drop(struct stat_stopwatch *t)
+{
+	atomic_long_add(t->clock() - t->start, &t->target->stats[t->item]);
+}
+DEFINE_CLASS(stat, struct stat_stopwatch, stopwatch_drop(&_T),
+	     stopwatch_new(s, clock, item), struct target *s,
+	     u64 (*clock)(void), enum target_stat item);
+
+DEFINE_LOCK_GUARD_1(mmap_read_lock, struct mm_struct, mmap_read_lock(_T->lock),
+		    mmap_read_unlock(_T->lock));
+DEFINE_CLASS(task_mm, struct mm_struct *, IS_ERR_OR_NULL(_T) ?: mmput(_T),
+	     get_task_mm(task), struct task_struct *task);
+DEFINE_CLASS(uvirt_folio, struct folio *, IS_ERR_OR_NULL(_T) ?: folio_put(_T),
+	     uvirt_to_folio(mm, user_addr), struct mm_struct *mm,
+	     u64 user_addr);
+DEFINE_CLASS(folio_get, struct folio *, folio_put(_T), ({
+		     folio_get(folio);
+		     folio;
+	     }),
+	     struct folio *folio);
+
+static struct folio *uvirt_to_folio(struct mm_struct *mm, u64 user_addr)
+{
+	guard(mmap_read_lock)(mm);
+	struct vm_area_struct *vma = vma_lookup(mm, user_addr);
+	if (!vma)
+		return ERR_PTR(-EFAULT);
+	// We only want writable anon folios
+	if (!vma_is_anonymous(vma) || !is_data_mapping(vma->vm_flags))
+		return ERR_PTR(-ENOTSUPP);
+	struct page *page = follow_page(vma, user_addr, FOLL_GET | FOLL_DUMP);
+	return IS_ERR_OR_NULL(page) ? ERR_CAST(page) : page_folio(page);
+}
+
+noinline static void target_events_overflow(struct perf_event *event,
+					    struct perf_sample_data *data,
+					    struct pt_regs *regs)
+{
+	struct target *self = event->overflow_handler_context;
+	mpsc_t ch = self->chans[CHAN_SAMPLE];
+	guard(rcu)();
+	guard(irqsave)();
+	// Not in a kthread context, try using the scheduler local_clock()
+	guard(stat)(self, local_clock, STAT_OVERFLOW_HANDLER);
+	{
+		guard(stat)(self, local_clock, STAT_PERF_PREPARE);
+		perf_prepare_sample(data, event, regs);
+	}
+	struct perf_sample s = {
+		.config = event->attr.config,
+		.config1 = event->attr.config1,
+		.pid = data->tid_entry.pid,
+		.tid = data->tid_entry.tid,
+		.time = data->time,
+		.addr = data->addr,
+		.weight = data->weight.full,
+		.phys_addr = data->phys_addr,
+	};
+	if (mpsc_send(ch, &s, sizeof(s)) < 0) {
+		// This should never happen as we created the mpsc using
+		// overwritting mode.
+		pr_err_ratelimited(
+			"%s: discard sample due to ring buffer overflow\n",
+			__func__);
+	};
+}
+static void target_events_enable(struct target *self, bool enable)
+{
+	pr_info("%s: enable=%d\n", __func__, enable);
+	void intel_pmu_drain_pebs_buffer(void);
+	for (int i = 0; i < MAX_EVENTS; i++) {
+		if (self->events[i]) {
+			if (enable)
+				perf_event_enable(self->events[i]);
+			else {
+				intel_pmu_drain_pebs_buffer();
+				perf_event_disable(self->events[i]);
+			}
+		}
+	}
+}
+static void worker_farewell(struct task_struct *task)
+{
+	char comm[64] = {};
+	get_kthread_comm(comm, sizeof(comm), task);
+	pr_info("%s: worker %pSR stopped name=%s usage=%u\n", __func__,
+		__builtin_return_address(0), comm, refcount_read(&task->usage));
+}
+struct splt_req {
+	u64 id, data;
+};
+noinline static int worker_main(struct target *self)
+{
+	mpsc_t splt_req = self->chans[CHAN_SPLT_REQ];
+	u64 period = msecs_to_jiffies(split_period_ms);
+	u64 id = 0;
+	while (!kthread_should_stop()) {
+		schedule_timeout_uninterruptible(period);
+		struct splt_req req = { .id = id++ };
+		if (mpsc_send(splt_req, &req, sizeof(req)) < 0) {
+			pr_err("%s: discard split request due to ring buffer overflow\n",
+			       __func__);
+			BUG();
+		}
+		// pr_info("%s: split request sent id=%llu\n", __func__, id - 1);
+	}
+
+	worker_farewell(current);
+	return 0;
+}
+
+noinline static int worker_throttle(struct target *self)
+{
+	u64 period = msecs_to_jiffies(throttle_pulse_period_ms);
+	u64 width = msecs_to_jiffies(throttle_pulse_width_ms);
+	BUG_ON(period == 0 || width >= period);
+	while (!kthread_should_stop()) {
+		if (width) {
+			{
+				guard(stat)(self, task_clock, STAT_THROTTLE);
+				target_events_enable(self, true);
+			}
+			schedule_timeout_uninterruptible(width);
+			{
+				guard(stat)(self, task_clock, STAT_THROTTLE);
+				target_events_enable(self, false);
+			}
+			schedule_timeout_uninterruptible(period - width);
+		} else
+			schedule_timeout_uninterruptible(period);
+	}
+
+	worker_farewell(current);
+	return 0;
+}
+
+noinline static int manage_folio(struct list_head *managed, struct folio *folio)
+{
+	extern bool folio_isolate_lru(struct folio * folio);
+	guard(folio_get)(folio);
+	if (!folio_isolate_lru(folio))
+		return -EAGAIN;
+	list_add_tail(&folio->lru, managed);
+	node_stat_mod_folio(folio, NR_ISOLATED_ANON + folio_is_file_lru(folio),
+			    folio_nr_pages(folio));
+	return 0;
+}
+noinline static void unmanage_folio(struct list_head *managed)
+{
+	extern void folio_putback_lru(struct folio *);
+	// release managed set
+	struct folio *folio, *next;
+	list_for_each_entry_safe(folio, next, managed, lru) {
+		list_del(&folio->lru);
+		node_stat_mod_folio(folio,
+				    NR_ISOLATED_ANON + folio_is_file_lru(folio),
+				    -folio_nr_pages(folio));
+		folio_putback_lru(folio);
+		// folio_add_lru(folio);
+	}
+}
+struct policy_worker {
+	pid_t pid;
+	struct range_tree *rt;
+	struct mrange **mrs; // mset > fmem + smem + tset
+	mpsc_t samplech, excg_req, excg_rsp, splt_req;
+	ulong (*node_avail_pages)(int);
+};
+struct exch_req {
+	struct list_head *promotion, *demotion;
+};
+struct exch_rsp {
+	struct list_head *promotion, *demotion;
+};
+noinline static int policy_handle_sample_one(struct policy_worker *data,
+					     struct mm_struct *mm,
+					     struct perf_sample *s)
+{
+	struct range_tree *rt = data->rt;
+	// ulong *nr_access = data->nr_access;
+	ulong vaddr = s->addr;
+	if (!vaddr)
+		return PEBS_NR_DISCARDED_NULL - PEBS_NR_DISCARDED;
+	if (s->pid != data->pid)
+		return PEBS_NR_DISCARDED_PID - PEBS_NR_DISCARDED;
+	if (!(mm->mmap_legacy_base <= vaddr && vaddr < mm->mmap_base))
+		return PEBS_NR_DISCARDED_IGNORE - PEBS_NR_DISCARDED;
+	if (mm->start_code <= vaddr && vaddr < max(mm->end_data, mm->start_brk))
+		return PEBS_NR_DISCARDED_IGNORE - PEBS_NR_DISCARDED;
+	TRY(rt_count(rt, vaddr));
+	return 0;
+}
+noinline static int policy_handle_samples(struct policy_worker *data,
+					  struct mm_struct *mm)
+{
+	mpsc_t samplech = data->samplech;
+	long rcv = 0,
+	     dis[PEBS_NR_DISCARDED_IGNORE - PEBS_NR_DISCARDED + 1] = {};
+	struct perf_sample s = {};
+	mpsc_for_each(samplech, s) {
+		long err = policy_handle_sample_one(data, mm, &s);
+		if (!err)
+			++rcv;
+		else
+			++dis[max(err, 0)];
+		if (rcv + dis[0] > MPSC_MAX_BATCH)
+			goto out;
+	}
+out:
+	count_vm_events(PEBS_NR_SAMPLED, rcv);
+	for (int i = 0; i < ARRAY_SIZE(dis); i++)
+		if (dis[i])
+			count_vm_events(PEBS_NR_DISCARDED + i, dis[i]);
+	return rcv;
+}
+noinline static int policy_send_exch_reqs(struct policy_worker *data,
+					  struct mm_struct *mm)
+{
+	struct range_tree *rt = data->rt;
+	struct mrange **mrs = data->mrs;
+	ulong (*fn)(int) = data->node_avail_pages;
+	mpsc_t excg_req = data->excg_req;
+	if (rt->min_range > rtree_exch_thresh)
+		return -EAGAIN;
+	struct list_head *promo = TRY(
+				 kmem_cache_alloc(list_head_cache, GFP_KERNEL)),
+			 *demo = TRY(
+				 kmem_cache_alloc(list_head_cache, GFP_KERNEL));
+	INIT_LIST_HEAD(promo), INIT_LIST_HEAD(demo);
+	guard(mmap_read_lock)(mm);
+	ulong rlen = rt->len;
+	TRY(rt_rank(rt, mm, mrs, &rlen));
+
+	// ranges [0, s) should be placed in smem
+	// ranges [f, olen) should be placed in fmem
+	ulong s = 0, f = rlen;
+	ulong fmem = 0, smem = 0, fmem_cap = fn(FMEM_NID),
+	      smem_cap = fn(SMEM_NID);
+	for (ulong i = 0, len = rlen; i < len; i++) {
+		struct mrange *p = mrs[i], *q = mrs[len - 1 - i];
+		if (smem_cap > (smem += p->in_smem + p->in_smem))
+			s += 1;
+		if (fmem_cap > (fmem += q->in_smem + q->in_smem) &&
+		    q->nr_access > 0)
+			f -= 1;
+	}
+
+	pr_info("%s: rank ranges count=%lu smem=[0, %lu) fmem=[%lu, %lu) smem_cap=%luM fmem_cap=%luM smem=%luM fmem=%luM\n",
+		__func__, rt->len, s, f, rt->len, smem_cap << PAGE_SHIFT >> 20,
+		fmem_cap << PAGE_SHIFT >> 20, smem << PAGE_SHIFT >> 20,
+		fmem << PAGE_SHIFT >> 20);
+	for (ulong i = 0; i < rt->len; i++) {
+		if (i == f)
+			pr_info("%s: promotion candidates starts here\n",
+				__func__);
+		mrange_show(mrs[i]);
+		if (i + 1 == s)
+			pr_info("%s: demotion candidates ends here\n",
+				__func__);
+		if (i + 1 == f)
+			pr_info("%s: candidate matching ends here\n", __func__);
+	}
+
+	// isolate all promotion candidates first
+	ulong candidates = 0;
+	for (ulong i = rlen - 1; i >= f; --i) {
+		struct mrange *r = mrs[i];
+		ulong total = r->in_smem;
+		ulong got = rt_isolate(mm, r, SMEM_NID, ULONG_MAX, manage_folio,
+				       promo);
+		candidates += got;
+	}
+	// isolate demotion candidate to match the promotion
+	ulong matched = 0;
+	for (ulong i = 0; matched < candidates && i < f; i++) {
+		struct mrange *r = mrs[i];
+		ulong total = r->in_fmem;
+		ulong got = rt_isolate(mm, r, FMEM_NID, candidates - matched,
+				       manage_folio, demo);
+		matched += got;
+	}
+
+	// send exchange request
+	struct exch_req req = {
+		.promotion = promo,
+		.demotion = demo,
+	};
+	pr_info("%s: exchange request sent promotion=%luM demotion=%luM\n",
+		__func__, candidates << PAGE_SHIFT >> 20,
+		matched << PAGE_SHIFT >> 20);
+	if (mpsc_send(excg_req, &req, sizeof(req)) < 0) {
+		pr_err("%s: discard exchange request due to ring buffer overflow\n",
+		       __func__);
+		BUG();
+	}
+	return 0;
+}
+
+noinline static int policy_handle_splt_reqs(struct policy_worker *data,
+					    struct mm_struct *mm)
+{
+	mpsc_t splt_req = data->splt_req;
+	struct range_tree *rt = data->rt;
+	ulong done = 0;
+	struct splt_req req = {};
+	mpsc_for_each(splt_req, req) {
+		ulong diff = ({
+			ulong len = rt->len;
+			rt_split(rt);
+			rt->len - len;
+		});
+		if (!diff)
+			continue;
+		pr_info("%s: split request success id=%llu\n", __func__,
+			req.id);
+		rt_show(rt);
+		long err = policy_send_exch_reqs(data, mm);
+		if (err < 0)
+			pr_err_ratelimited("%s: policy_send_exch_reqs()=%pe\n",
+					   __func__, ERR_PTR(err));
+		else
+			done += 1;
+	}
+	return done;
+}
+noinline static int policy_handle_exch_rsps(struct policy_worker *data)
+{
+	mpsc_t excg_rsp = data->excg_rsp;
+	int done = 0;
+	struct exch_rsp rsp = {};
+	mpsc_for_each(excg_rsp, rsp) {
+		++done;
+		unmanage_folio(rsp.promotion);
+		unmanage_folio(rsp.demotion);
+		kmem_cache_free(list_head_cache, rsp.demotion);
+		kmem_cache_free(list_head_cache, rsp.promotion);
+	}
+	return done;
+}
+
+static void kmalloc_cleanup(const void *p)
+{
+	if (IS_ERR_OR_NULL(p))
+		return;
+	if (IS_ERR_OR_NULL(*(void **)p))
+		return;
+	kfree(*(void **)p);
+}
+
+ulong __node_present_pages(int nid)
+{
+	return node_present_pages(nid);
+}
+EXPORT_SYMBOL_GPL(__node_present_pages);
+
+static void mm_show_layout(struct mm_struct *mm)
+{
+	if (mm->start_code > mm->start_brk) {
+		pr_info("%s: code/data is above heap: start_code=%#lx start_brk=%#lx\n",
+			__func__, mm->start_code, mm->start_brk);
+	} else {
+		pr_info("%s: code/data is below heap : start_brk=%#lx start_code=%#lx\n",
+			__func__, mm->start_brk, mm->start_code);
+	}
+	// clang-format off
+	pr_info("%s: mmap_legacy_base=%#lx ", __func__, mm->mmap_legacy_base);
+	if (mm->start_code > mm->start_brk) {
+		pr_cont("start_brk=%#lx brk=%#lx ", mm->start_brk, mm->brk);
+		pr_cont("start_code=%#lx end_code=%#lx start_data=%#lx end_data=%#lx ",
+			mm->start_code, mm->end_code, mm->start_data, mm->end_data);
+	} else {
+		pr_cont("start_code=%#lx end_code=%#lx start_data=%#lx end_data=%#lx ",
+			mm->start_code, mm->end_code, mm->start_data, mm->end_data);
+		pr_cont("start_brk=%#lx brk=%#lx ", mm->start_brk, mm->brk);
+	}
+	pr_cont("mmap_base=%#lx start_stack=%#lx arg_start=%#lx arg_end=%#lx env_start=%#lx env_end=%#lx\n",
+		mm->mmap_base, mm->start_stack, mm->arg_start, mm->arg_end, mm->env_start, mm->env_end);
+	// clang-format on
+}
+
+noinline static int worker_policy(struct target *self)
+{
+	// Shared data
+	mpsc_t samplech = self->chans[CHAN_SAMPLE];
+	mpsc_t excg_req = self->chans[CHAN_EXCG_REQ];
+	mpsc_t excg_rsp = self->chans[CHAN_EXCG_RSP];
+	mpsc_t splt_req = self->chans[CHAN_SPLT_REQ];
+
+	// Thread private data initialization
+	// rmt: the "range_tree" to record the managed ranges
+	struct range_tree __cleanup(rt_drop) rt = {};
+	{
+		CLASS(task_mm, mm)(self->victim);
+		BUG_ON(IS_ERR_OR_NULL(mm));
+		mm_show_layout(mm);
+		struct sysinfo meminfo = {};
+		si_meminfo(&meminfo);
+		ulong coverage =
+			RMT_INITIAL_SIZE_FACTOR *
+			ALIGN(meminfo.totalram * meminfo.mem_unit, 1 << 30);
+		// heap region
+		ulong start = ALIGN_DOWN(mm->start_brk, 1 << 30);
+		BUG_ON(rt_init(&rt, start, start + coverage));
+		// mmap region
+		// mmap assigns addresses in a topdown manner starting at mmap_base
+		// see: generic_get_unmapped_area_topdown()
+		ulong end = ALIGN(mm->mmap_base, 1 << 30);
+		BUG_ON(rt_insert(&rt, end - coverage, end));
+		rt_show(&rt);
+		BUG_ON(rt.len == 0);
+	}
+	__cleanup(kmalloc_cleanup) struct mrange **mrs =
+		TRY(kcalloc(RTREE_MAX_SIZE, sizeof(*mrs), GFP_KERNEL));
+	BUG_ON(!mrs);
+
+	extern ulong __node_avail_pages(int nid);
+	extern ulong node_avail_pages(int nid);
+	ulong (*fn)(int) = symbol_get(__node_avail_pages)	?:
+				   symbol_get(node_avail_pages) ?:
+				   symbol_get(__node_present_pages);
+	BUG_ON(!fn);
+
+	struct policy_worker data = {
+		.pid = self->victim->tgid,
+		.rt = &rt,
+		.mrs = mrs,
+		.samplech = samplech,
+		.excg_req = excg_req,
+		.excg_rsp = excg_rsp,
+		.splt_req = splt_req,
+		.node_avail_pages = fn,
+	};
+
+	// reporting is rate limited to every 500ms
+	DEFINE_RATELIMIT_STATE(report_rs, msecs_to_jiffies(500), 1);
+
+	u64 sample_count = 0, excg_req_count = 0, excg_rsp_count = 0,
+	    report_period = 1 << 20, next_report = report_period,
+	    initial_backoff = 500, backoff = initial_backoff;
+	while (!kthread_should_stop()) {
+		int which = mpsc_select3(excg_rsp, splt_req, samplech);
+		guard(stat)(self, task_clock, STAT_POLICY);
+		switch (which) {
+		case -ERESTARTSYS:
+			pr_warn_ratelimited("%s: interrupted\n", __func__);
+			continue;
+		case 0: {
+			excg_rsp_count += policy_handle_exch_rsps(&data);
+			break;
+		}
+		case 1: {
+			CLASS(task_mm, mm)(self->victim);
+			if (unlikely(IS_ERR_OR_NULL(mm))) {
+				pr_err("%s: victim mm=%pe\n", __func__, mm);
+				// exit early will cause kthread_stop to panic
+				schedule_timeout_interruptible(
+					msecs_to_jiffies(backoff *= 2));
+				continue;
+			} else {
+				backoff = initial_backoff;
+			}
+			guard(stat)(self, task_clock, STAT_SPLIT);
+			excg_rsp_count += policy_handle_splt_reqs(&data, mm);
+			break;
+		}
+		case 2: {
+			CLASS(task_mm, mm)(self->victim);
+			if (unlikely(IS_ERR_OR_NULL(mm))) {
+				pr_err("%s: victim mm=%pe\n", __func__, mm);
+				// exit early will cause kthread_stop to panic
+				schedule_timeout_interruptible(
+					msecs_to_jiffies(backoff *= 2));
+				continue;
+			} else {
+				backoff = initial_backoff;
+			}
+			sample_count += policy_handle_samples(&data, mm);
+			break;
+		}
+		default:
+			pr_err("%s: unknown channel or error %pe\n", __func__,
+			       ERR_PTR(which));
+			BUG();
+		}
+
+		u64 curr_report =
+			max(sample_count, max(excg_req_count, excg_rsp_count));
+		if (curr_report > next_report && __ratelimit(&report_rs)) {
+			next_report = curr_report + report_period;
+			pr_info("%s: samples=%llu excg_req=%llu excg_rsp=%llu\n",
+				__func__, sample_count, excg_req_count,
+				excg_rsp_count);
+		}
+	}
+
+	symbol_put_addr(fn);
+	worker_farewell(current);
+	return 0;
+}
+
+noinline static int migration_handle_req(struct exch_req *req,
+					 HashMapU64U64 *bset)
+{
+	struct list_head *p = req->promotion, *d = req->demotion;
+	LIST_HEAD(promotion_done);
+	LIST_HEAD(demotion_done);
+	ulong success = 0, failure = 0, blacklist = 0;
+	while (!list_empty(p) && !list_empty(d)) {
+		// fifo order
+		struct folio *folio0 = list_entry(p->next, struct folio, lru),
+			     *folio1 = list_entry(d->next, struct folio, lru);
+
+		u64 pfn;
+		pfn = folio_pfn(folio0);
+		if (HashMapU64U64_contains(bset, &pfn)) {
+			HashMapU64U64_Iter iter =
+				HashMapU64U64_find(bset, &pfn);
+			HashMapU64U64_Entry *e = HashMapU64U64_Iter_get(&iter);
+			if (++e->val > MIGRATION_BSET_BACKOFF) {
+				HashMapU64U64_erase(bset, &pfn);
+				--blacklist;
+			}
+			list_move_tail(p->next, &promotion_done);
+			continue;
+		}
+		if (!folio_test_anon(folio0)) {
+			list_move_tail(p->next, &promotion_done);
+			HashMapU64U64_Entry e = { folio_pfn(folio0), 0 };
+			CHECK_INSERTED(HashMapU64U64_insert(bset, &e), true,
+				       "cannot blacklist folio0 pfn=0x%lx",
+				       folio_pfn(folio0));
+			++blacklist;
+			continue;
+		}
+
+		pfn = folio_pfn(folio1);
+		if (HashMapU64U64_contains(bset, &pfn)) {
+			HashMapU64U64_Iter iter =
+				HashMapU64U64_find(bset, &pfn);
+			HashMapU64U64_Entry *e = HashMapU64U64_Iter_get(&iter);
+			if (++e->val > MIGRATION_BSET_BACKOFF) {
+				HashMapU64U64_erase(bset, &pfn);
+				--blacklist;
+			}
+			list_move_tail(d->next, &demotion_done);
+			continue;
+		}
+		if (!folio_test_anon(folio1)) {
+			list_move_tail(d->next, &demotion_done);
+			HashMapU64U64_Entry e = { folio_pfn(folio1), 0 };
+			CHECK_INSERTED(HashMapU64U64_insert(bset, &e), true,
+				       "cannot blacklist folio1 pfn=0x%lx",
+				       folio_pfn(folio1));
+			++blacklist;
+			continue;
+		}
+
+		int err = folio_exchange_isolated(folio0, folio1, MIGRATE_SYNC);
+		if (err) {
+			pr_err_ratelimited(
+				"%s: folio_exchange_isolated: mode=%d err=%pe [src=%p pfn=0x%lx] <-> [dst=%p pfn=0x%lx]",
+				__func__, MIGRATE_SYNC, ERR_PTR(err), folio0,
+				folio_pfn(folio0), folio1, folio_pfn(folio1));
+			++failure;
+		}
+		switch (err) {
+		case -ENOTSUPP: {
+			// folio0 failed, blacklist
+			list_move_tail(p->next, &promotion_done);
+			HashMapU64U64_Entry e = { folio_pfn(folio0), 0 };
+			CHECK_INSERTED(HashMapU64U64_insert(bset, &e), true,
+				       "cannot blacklist folio0 pfn=0x%lx",
+				       folio_pfn(folio0));
+			++blacklist;
+			break;
+		}
+		case -ENOTSUPP + 1: {
+			// folio1 failed, blacklist
+			list_move_tail(d->next, &demotion_done);
+			HashMapU64U64_Entry e = { folio_pfn(folio1), 0 };
+			CHECK_INSERTED(HashMapU64U64_insert(bset, &e), true,
+				       "cannot blacklist folio1 pfn=0x%lx",
+				       folio_pfn(folio1));
+			++blacklist;
+			break;
+		}
+		case 0:
+			// success
+			++success;
+			fallthrough;
+		default:
+			// ignore other error
+			list_move_tail(p->next, &promotion_done);
+			list_move_tail(d->next, &demotion_done);
+			break;
+		}
+	}
+	pr_info("%s: success=%lu failure=%lu blacklist=%lu\n", __func__,
+		success, failure, blacklist);
+
+	// FIXME: handle the remaining folios via the old-fashioned
+	// migrate_pages when the two lists are not balanced
+
+	unmanage_folio(&promotion_done);
+	unmanage_folio(&demotion_done);
+	return 0;
+}
+noinline static int migration_send_ack(mpsc_t excg_rsp, struct exch_req *req,
+				       int error)
+{
+	struct exch_rsp rsp = {
+		.promotion = req->promotion,
+		.demotion = req->demotion,
+	};
+	int err = mpsc_send(excg_rsp, &rsp, sizeof(rsp));
+	if (err < 0) {
+		pr_err("%s: failed to send exchange response due to ring buffer overflow\n",
+		       __func__);
+		BUG();
+	}
+	return err;
+}
+noinline static int migration_handle_requests(mpsc_t excg_req, mpsc_t excg_rsp,
+					      HashMapU64U64 *bset)
+{
+	int received = 0;
+	struct exch_req req = {};
+	mpsc_for_each(excg_req, req) {
+		++received;
+		migration_send_ack(excg_rsp, &req,
+				   migration_handle_req(&req, bset));
+	}
+	return received;
+}
+
+noinline static int worker_migration(struct target *self)
+{
+	mpsc_t excg_req = self->chans[CHAN_EXCG_REQ],
+	       excg_rsp = self->chans[CHAN_EXCG_RSP];
+	// bset: blacklisted folios which canot be migrated
+	HashMapU64U64 __cleanup(HashMapU64U64_destroy)
+		bset = HashMapU64U64_new(MIGRATION_BSET_BUCKET);
+	extern ulong node_balloon_pages(int nid);
+	// ulong (*fn)(int) = symbol_get(node_balloon_pages);
+	// BUG_ON(!fn);
+	// pr_info("%s: symbol_get(node_balloon_pages)=%pe\n", __func__,
+	// 	ERR_PTR((long)fn));
+
+	u64 excg_count = 0, report_period = 500, next_report = report_period;
+	// reporting is rate limited to every 1000ms
+	DEFINE_RATELIMIT_STATE(report_rs, msecs_to_jiffies(1000), 1);
+
+	while (!kthread_should_stop()) {
+		int err = mpsc_wait(excg_req);
+		guard(stat)(self, task_clock, STAT_MIGRATION);
+		switch (err) {
+		case -ERESTARTSYS:
+			pr_warn_ratelimited("%s: interrupted\n", __func__);
+			continue;
+		case 0:
+			// pr_info("%s: excg_req received\n", __func__);
+			excg_count += migration_handle_requests(
+				excg_req, excg_rsp, &bset);
+			// ulong fmem_cap = FMEM_NODE->node_present_pages,
+			//       smem_cap = SMEM_NODE->node_present_pages,
+			//       fmem_bln = fn(FMEM_NID), smem_bln = fn(SMEM_NID);
+			// pr_info("%s: fmem_cap=%luM smem_cap=%luM fmem_bln=%luM smem_bln=%luM\n",
+			// 	__func__, fmem_cap << PAGE_SHIFT >> 20,
+			// 	smem_cap << PAGE_SHIFT >> 20,
+			// 	fmem_bln << PAGE_SHIFT >> 20,
+			// 	smem_bln << PAGE_SHIFT >> 20);
+			break;
+		default:
+			pr_err("%s: unknown error %d\n", __func__, err);
+			BUG();
+		}
+		if (excg_count > next_report && __ratelimit(&report_rs)) {
+			next_report = excg_count + report_period;
+			pr_info("%s: excg_count=%llu\n", __func__, excg_count);
+		}
+	}
+
+	// symbol_put_addr(fn);
+	worker_farewell(current);
+	return 0;
+}
+
+static int (*worker_fns[])(void *) = {
+	[WORKER_MAIN] = (void *)worker_main,
+	[WORKER_THROTTLE] = (void *)worker_throttle,
+	[WORKER_POLICY] = (void *)worker_policy,
+	[WORKER_MIGRATION] = (void *)worker_migration,
+};
+static char *const worker_names[] = {
+	[WORKER_MAIN] = "main",
+	[WORKER_THROTTLE] = "throttle",
+	[WORKER_POLICY] = "policy",
+	[WORKER_MIGRATION] = "migration",
+};
+
+pid_t target_pid(struct target *self)
+{
+	return self->victim->tgid;
+}
+void target_drop(struct target *self)
+{
+	if (IS_ERR_OR_NULL(self))
+		return;
+	u64 total_elapsed = sched_clock() - self->start_time + 1;
+	for (int i = 0; i < MAX_STATS; ++i) {
+		long val = atomic_long_read(&self->stats[i]);
+		pr_info("%s: %s=%ld permyriad=%llu\n", __func__,
+			target_stat_name[i], val, val * 10000 / total_elapsed);
+	}
+	target_events_enable(self, false);
+	for (int i = 0; i < MAX_WORKERS; i++) {
+		struct task_struct *t = self->workers[i];
+		if (IS_ERR_OR_NULL(t))
+			continue;
+		char comm[64] = {};
+		get_kthread_comm(comm, sizeof(comm), t);
+		pr_info("%s: try to stop %s worker comm=%s usage=%u\n",
+			__func__, worker_names[i], comm,
+			refcount_read(&t->usage));
+		// The worker must not exit before we stop it
+		kthread_stop(t);
+	}
+	for (int i = 0; i < MAX_CHANS; i++) {
+		mpsc_t ch = self->chans[i];
+		!ch ?: mpsc_drop(ch);
+	}
+	struct task_struct *victim = self->victim;
+	!victim ?: put_task_struct(victim);
+	kfree(self);
+}
+struct target *target_new(pid_t pid)
+{
+	struct target *self = kzalloc(sizeof(*self), GFP_KERNEL);
+	if (!self)
+		return ERR_PTR(-ENOMEM);
+	self->victim = find_get_task_by_vpid(pid);
+	if (!self->victim) {
+		target_drop(self);
+		return ERR_PTR(-ESRCH);
+	}
+	for (int i = 0; i < MAX_CHANS; i++) {
+		mpsc_t ch = mpsc_new(MPSC_MAX_SIZE_BYTE);
+		if (IS_ERR_OR_NULL(ch)) {
+			target_drop(self);
+			return ERR_PTR(-ENOMEM);
+		}
+		self->chans[i] = ch;
+	}
+	BUILD_BUG_ON(ARRAY_SIZE(worker_fns) != MAX_WORKERS);
+	BUILD_BUG_ON(ARRAY_SIZE(worker_names) != MAX_WORKERS);
+	for (int i = 0; i < MAX_WORKERS; i++) {
+		struct task_struct *t = kthread_run(
+			worker_fns[i], self, "ht-%s/%d", worker_names[i], pid);
+		if (IS_ERR_OR_NULL(t)) {
+			target_drop(self);
+			return ERR_PTR(-ECHILD);
+		}
+		self->workers[i] = t;
+	}
+	BUILD_BUG_ON(ARRAY_SIZE(event_attrs) != MAX_EVENTS);
+	for (int i = 0; i < MAX_EVENTS; i++) {
+		struct perf_event *e = perf_event_create_kernel_counter(
+			&event_attrs[i], -1, self->victim,
+			target_events_overflow, self);
+		if (IS_ERR_OR_NULL(e)) {
+			target_drop(self);
+			return ERR_CAST(e);
+		}
+		self->events[i] = e;
+		pr_info("%s: created config=%#llx config1=%#llx sample_period=%lld\n",
+			__func__, event_attrs[i].config, event_attrs[i].config1,
+			event_attrs[i].sample_period);
+	}
+	BUILD_BUG_ON(ARRAY_SIZE(target_stat_name) != MAX_STATS);
+	self->start_time = sched_clock();
+	return self;
+}
diff --git a/mm/demeter/cwisstable.h b/mm/demeter/cwisstable.h
new file mode 100644
index 000000000000..4b25270041ad
--- /dev/null
+++ b/mm/demeter/cwisstable.h
@@ -0,0 +1,3537 @@
+// Copyright 2022 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+// Modified for kernel use by: Junliang Hu <jlhu@cse.cuhk.edu.hk>
+// - Removed C++ support;
+// - Replaced thread local storage with Linux's per-cpu;
+// - Fixed conflicting noinline macros;
+// - Use kernel flavor of memory allocation;
+// - Replaced fprintf with pr_*;
+// - Use kernel flavor of INT8_C and UINT64_C;
+// - All SSE code should not work because the limiting use of FPU;
+
+// THIS IS A GENERATED FILE! DO NOT EDIT DIRECTLY!
+// Generated using unify.py, by concatenating, in order:
+// #include "cwisstable/internal/base.h"
+// #include "cwisstable/internal/bits.h"
+// #include "cwisstable/internal/control_byte.h"
+// #include "cwisstable/internal/capacity.h"
+// #include "cwisstable/internal/probe.h"
+// #include "cwisstable/internal/absl_hash.h"
+// #include "cwisstable/hash.h"
+// #include "cwisstable/internal/extract.h"
+// #include "cwisstable/policy.h"
+// #include "cwisstable/internal/raw_table.h"
+// #include "cwisstable/declare.h"
+
+#ifndef CWISSTABLE_H_
+#define CWISSTABLE_H_
+
+#include <linux/types.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/percpu.h>
+
+#define INT8_C S8_C
+#define UINT64_C U64_C
+
+#ifdef __cplusplus
+static_assert(false, "This modified version is not compatible with C++.");
+#endif
+
+// #include <assert.h>
+// #include <stdalign.h>
+// #include <stdbool.h>
+// #include <stddef.h>
+// #include <stdlib.h>
+// #include <string.h>
+
+/// cwisstable/internal/base.h /////////////////////////////////////////////////
+/// Feature detection and basic helper macros.
+
+/// C++11 compatibility macros.
+///
+/// Atomic support, due to incompatibilities between C++ and C11 atomic syntax.
+/// - `CWISS_ATOMIC_T(Type)` names an atomic version of `Type`. We must use this
+///   instead of `_Atomic(Type)` to name an atomic type.
+/// - `CWISS_ATOMIC_INC(value)` will atomically increment `value` without
+///   performing synchronization. This is used as a weak entropy source
+///   elsewhere.
+///
+/// `extern "C"` support via `CWISS_END_EXTERN` and `CWISS_END_EXTERN`,
+/// which open and close an `extern "C"` block in C++ mode.
+#ifdef __cplusplus
+#include <atomic>
+#define CWISS_ATOMIC_T(Type_) std::atomic<Type_>
+#define CWISS_ATOMIC_INC(val_) (val_).fetch_add(1, std::memory_order_relaxed)
+
+#define CWISS_BEGIN_EXTERN extern "C" {
+#define CWISS_END_EXTERN }
+#else
+// #include <stdatomic.h>
+#define CWISS_ATOMIC_T(Type_) _Atomic(Type_)
+#define CWISS_ATOMIC_INC(val_) \
+	atomic_fetch_add_explicit(&(val_), 1, memory_order_relaxed)
+
+#define CWISS_BEGIN_EXTERN
+#define CWISS_END_EXTERN
+#endif
+
+/// Compiler detection macros.
+///
+/// The following macros are defined:
+/// - `CWISS_IS_CLANG` detects Clang.
+/// - `CWISS_IS_GCC` detects GCC (and *not* Clang pretending to be GCC).
+/// - `CWISS_IS_MSVC` detects MSCV (and *not* clang-cl).
+/// - `CWISS_IS_GCCISH` detects GCC and GCC-mode Clang.
+/// - `CWISS_IS_MSVCISH` detects MSVC and clang-cl.
+#ifdef __clang__
+#define CWISS_IS_CLANG 1
+#else
+#define CWISS_IS_CLANG 0
+#endif
+#if defined(__GNUC__)
+#define CWISS_IS_GCCISH 1
+#else
+#define CWISS_IS_GCCISH 0
+#endif
+#if defined(_MSC_VER)
+#define CWISS_IS_MSVCISH 1
+#else
+#define CWISS_IS_MSVCISH 0
+#endif
+#define CWISS_IS_GCC (CWISS_IS_GCCISH && !CWISS_IS_CLANG)
+#define CWISS_IS_MSVC (CWISS_IS_MSVCISH && !CWISS_IS_CLANG)
+
+#define CWISS_PRAGMA(pragma_) _Pragma(#pragma_)
+
+#if CWISS_IS_GCCISH
+#define CWISS_GCC_PUSH CWISS_PRAGMA(GCC diagnostic push)
+#define CWISS_GCC_ALLOW(w_) CWISS_PRAGMA(GCC diagnostic ignored w_)
+#define CWISS_GCC_POP CWISS_PRAGMA(GCC diagnostic pop)
+#else
+#define CWISS_GCC_PUSH
+#define CWISS_GCC_ALLOW(w_)
+#define CWISS_GCC_POP
+#endif
+
+/// Warning control around `CWISS` symbol definitions. These macros will
+/// disable certain false-positive warnings that `CWISS` definitions tend to
+/// emit.
+#define CWISS_BEGIN                           \
+	CWISS_GCC_PUSH                        \
+	CWISS_GCC_ALLOW("-Wunused-function")  \
+	CWISS_GCC_ALLOW("-Wunused-parameter") \
+	CWISS_GCC_ALLOW("-Wcast-qual")        \
+	CWISS_GCC_ALLOW("-Wmissing-field-initializers")
+#define CWISS_END CWISS_GCC_POP
+
+/// `CWISS_HAVE_SSE2` is nonzero if we have SSE2 support.
+///
+/// `-DCWISS_HAVE_SSE2` can be used to override it; it is otherwise detected
+/// via the usual non-portable feature-detection macros.
+#ifndef CWISS_HAVE_SSE2
+#if defined(__SSE2__) ||     \
+	(CWISS_IS_MSVCISH && \
+	 (defined(_M_X64) || (defined(_M_IX86) && _M_IX86_FP >= 2)))
+#define CWISS_HAVE_SSE2 1
+#else
+#define CWISS_HAVE_SSE2 0
+#endif
+#endif
+
+/// `CWISS_HAVE_SSSE2` is nonzero if we have SSSE2 support.
+///
+/// `-DCWISS_HAVE_SSSE2` can be used to override it; it is otherwise detected
+/// via the usual non-portable feature-detection macros.
+#ifndef CWISS_HAVE_SSSE3
+#ifdef __SSSE3__
+#define CWISS_HAVE_SSSE3 1
+#else
+#define CWISS_HAVE_SSSE3 0
+#endif
+#endif
+
+#if CWISS_HAVE_SSE2
+#include <emmintrin.h>
+#endif
+
+#if CWISS_HAVE_SSSE3
+#if !CWISS_HAVE_SSE2
+#error "Bad configuration: SSSE3 implies SSE2!"
+#endif
+#include <tmmintrin.h>
+#endif
+
+/// `CWISS_HAVE_BUILTIN` will, in Clang, detect whether a Clang language
+/// extension is enabled.
+///
+/// See https://clang.llvm.org/docs/LanguageExtensions.html.
+#ifdef __has_builtin
+#define CWISS_HAVE_CLANG_BUILTIN(x_) __has_builtin(x_)
+#else
+#define CWISS_HAVE_CLANG_BUILTIN(x_) 0
+#endif
+
+/// `CWISS_HAVE_GCC_ATTRIBUTE` detects if a particular `__attribute__(())` is
+/// present.
+///
+/// GCC: https://gcc.gnu.org/gcc-5/changes.html
+/// Clang: https://clang.llvm.org/docs/LanguageExtensions.html
+#ifdef __has_attribute
+#define CWISS_HAVE_GCC_ATTRIBUTE(x_) __has_attribute(x_)
+#else
+#define CWISS_HAVE_GCC_ATTRIBUTE(x_) 0
+#endif
+
+#ifdef __has_feature
+#define CWISS_HAVE_FEATURE(x_) __has_feature(x_)
+#else
+#define CWISS_HAVE_FEATURE(x_) 0
+#endif
+
+/// `CWISS_THREAD_LOCAL` expands to the appropriate TLS annotation, if one is
+/// available.
+/// Linux kernel has its own thread local storage implementation
+#if CWISS_IS_GCCISH
+#define CWISS_THREAD_LOCAL __thread
+#endif
+
+/// `CWISS_CHECK` will evaluate `cond_` and, if false, print an error and crash
+/// the program.
+///
+/// This is like `assert()` but unconditional.
+#define CWISS_CHECK(cond_, ...)                                               \
+	do {                                                                  \
+		if (cond_)                                                    \
+			break;                                                \
+		pr_err("CWISS_CHECK failed at %s:%d caller %pSR\n", __FILE__, \
+		       __LINE__, __builtin_return_address(0));                \
+		pr_err(__VA_ARGS__);                                          \
+		dump_stack();                                                 \
+		BUG();                                                        \
+	} while (false)
+
+/// `CWISS_DCHECK` is like `CWISS_CHECK` but is disabled by `NDEBUG`.
+#ifdef NDEBUG
+#define CWISS_DCHECK(cond_, ...) ((void)0)
+#else
+#define CWISS_DCHECK CWISS_CHECK
+#endif
+
+/// `CWISS_LIKELY` and `CWISS_UNLIKELY` provide a prediction hint to the
+/// compiler to encourage branches to be scheduled in a particular way.
+#if CWISS_HAVE_CLANG_BUILTIN(__builtin_expect) || CWISS_IS_GCC
+#define CWISS_LIKELY(cond_) (__builtin_expect(false || (cond_), true))
+#define CWISS_UNLIKELY(cond_) (__builtin_expect(false || (cond_), false))
+#else
+#define CWISS_LIKELY(cond_) (cond_)
+#define CWISS_UNLIKELY(cond_) (cond_)
+#endif
+
+/// `CWISS_INLINE_ALWAYS` informs the compiler that it should try really hard
+/// to inline a function.
+#if CWISS_HAVE_GCC_ATTRIBUTE(always_inline)
+#define CWISS_INLINE_ALWAYS __attribute__((always_inline))
+#else
+#define CWISS_INLINE_ALWAYS
+#endif
+
+/// `CWISS_INLINE_NEVER` informs the compiler that it should avoid inlining a
+/// function.
+#if CWISS_HAVE_GCC_ATTRIBUTE(__noinline__)
+#define CWISS_INLINE_NEVER __attribute__((__noinline__))
+#else
+#define CWISS_INLINE_NEVER
+#endif
+
+/// `CWISS_PREFETCH` informs the compiler that it should issue prefetch
+/// instructions.
+#if CWISS_HAVE_CLANG_BUILTIN(__builtin_prefetch) || CWISS_IS_GCC
+#define CWISS_HAVE_PREFETCH 1
+#define CWISS_PREFETCH(addr_, locality_) \
+	__builtin_prefetch(addr_, 0, locality_);
+#else
+#define CWISS_HAVE_PREFETCH 0
+#define CWISS_PREFETCH(addr_, locality_) ((void)0)
+#endif
+
+/// `CWISS_HAVE_ASAN` and `CWISS_HAVE_MSAN` detect the presence of some of the
+/// sanitizers.
+#if defined(__SANITIZE_ADDRESS__) || CWISS_HAVE_FEATURE(address_sanitizer)
+#define CWISS_HAVE_ASAN 1
+#else
+#define CWISS_HAVE_ASAN 0
+#endif
+#if defined(__SANITIZE_MEMORY__) || \
+	(CWISS_HAVE_FEATURE(memory_sanitizer) && !defined(__native_client__))
+#define CWISS_HAVE_MSAN 1
+#else
+#define CWISS_HAVE_MSAN 0
+#endif
+
+#if CWISS_HAVE_ASAN
+#include <sanitizer/asan_interface.h>
+#endif
+
+#if CWISS_HAVE_MSAN
+#include <sanitizer/msan_interface.h>
+#endif
+
+CWISS_BEGIN
+CWISS_BEGIN_EXTERN
+/// Informs a sanitizer runtime that this memory is invalid.
+static inline void CWISS_PoisonMemory(const void *m, size_t s)
+{
+#if CWISS_HAVE_ASAN
+	ASAN_POISON_MEMORY_REGION(m, s);
+#endif
+#if CWISS_HAVE_MSAN
+	__msan_poison(m, s);
+#endif
+	(void)m;
+	(void)s;
+}
+
+/// Informs a sanitizer runtime that this memory is no longer invalid.
+static inline void CWISS_UnpoisonMemory(const void *m, size_t s)
+{
+#if CWISS_HAVE_ASAN
+	ASAN_UNPOISON_MEMORY_REGION(m, s);
+#endif
+#if CWISS_HAVE_MSAN
+	__msan_unpoison(m, s);
+#endif
+	(void)m;
+	(void)s;
+}
+CWISS_END_EXTERN
+CWISS_END
+/// cwisstable/internal/base.h /////////////////////////////////////////////////
+
+/// cwisstable/internal/bits.h /////////////////////////////////////////////////
+/// Bit manipulation utilities.
+
+CWISS_BEGIN
+CWISS_BEGIN_EXTERN
+
+/// Counts the number of trailing zeroes in the binary representation of `x`.
+CWISS_INLINE_ALWAYS
+static inline uint32_t CWISS_TrailingZeroes64(uint64_t x)
+{
+#if CWISS_HAVE_CLANG_BUILTIN(__builtin_ctzll) || CWISS_IS_GCC
+	static_assert(sizeof(unsigned long long) == sizeof(x),
+		      "__builtin_ctzll does not take 64-bit arg");
+	return __builtin_ctzll(x);
+#elif CWISS_IS_MSVC
+	unsigned long result = 0;
+#if defined(_M_X64) || defined(_M_ARM64)
+	_BitScanForward64(&result, x);
+#else
+	if (((uint32_t)x) == 0) {
+		_BitScanForward(&result, (unsigned long)(x >> 32));
+		return result + 32;
+	}
+	_BitScanForward(&result, (unsigned long)(x));
+#endif
+	return result;
+#else
+	uint32_t c = 63;
+	x &= ~x + 1;
+	if (x & 0x00000000FFFFFFFF)
+		c -= 32;
+	if (x & 0x0000FFFF0000FFFF)
+		c -= 16;
+	if (x & 0x00FF00FF00FF00FF)
+		c -= 8;
+	if (x & 0x0F0F0F0F0F0F0F0F)
+		c -= 4;
+	if (x & 0x3333333333333333)
+		c -= 2;
+	if (x & 0x5555555555555555)
+		c -= 1;
+	return c;
+#endif
+}
+
+/// Counts the number of leading zeroes in the binary representation of `x`.
+CWISS_INLINE_ALWAYS
+static inline uint32_t CWISS_LeadingZeroes64(uint64_t x)
+{
+#if CWISS_HAVE_CLANG_BUILTIN(__builtin_clzll) || CWISS_IS_GCC
+	static_assert(sizeof(unsigned long long) == sizeof(x),
+		      "__builtin_clzll does not take 64-bit arg");
+	// Handle 0 as a special case because __builtin_clzll(0) is undefined.
+	return x == 0 ? 64 : __builtin_clzll(x);
+#elif CWISS_IS_MSVC
+	unsigned long result = 0;
+#if defined(_M_X64) || defined(_M_ARM64)
+	if (_BitScanReverse64(&result, x)) {
+		return 63 - result;
+	}
+#else
+	unsigned long result = 0;
+	if ((x >> 32) && _BitScanReverse(&result, (unsigned long)(x >> 32))) {
+		return 31 - result;
+	}
+	if (_BitScanReverse(&result, static_cast<unsigned long>(x))) {
+		return 63 - result;
+	}
+#endif
+	return 64;
+#else
+	uint32_t zeroes = 60;
+	if (x >> 32) {
+		zeroes -= 32;
+		x >>= 32;
+	}
+	if (x >> 16) {
+		zeroes -= 16;
+		x >>= 16;
+	}
+	if (x >> 8) {
+		zeroes -= 8;
+		x >>= 8;
+	}
+	if (x >> 4) {
+		zeroes -= 4;
+		x >>= 4;
+	}
+	return "\4\3\2\2\1\1\1\1\0\0\0\0\0\0\0"[x] + zeroes;
+#endif
+}
+
+/// Counts the number of trailing zeroes in the binary representation of `x_` in
+/// a type-generic fashion.
+#define CWISS_TrailingZeros(x_) (CWISS_TrailingZeroes64(x_))
+
+/// Counts the number of leading zeroes in the binary representation of `x_` in
+/// a type-generic fashion.
+#define CWISS_LeadingZeros(x_)       \
+	(CWISS_LeadingZeroes64(x_) - \
+	 (uint32_t)((sizeof(unsigned long long) - sizeof(x_)) * 8))
+
+/// Computes the number of bits necessary to represent `x_`, i.e., the bit index
+/// of the most significant one.
+#define CWISS_BitWidth(x_) \
+	(((uint32_t)(sizeof(x_) * 8)) - CWISS_LeadingZeros(x_))
+
+#define CWISS_RotateLeft(x_, bits_) \
+	(((x_) << bits_) | ((x_) >> (sizeof(x_) * 8 - bits_)))
+
+/// The return type of `CWISS_Mul128`.
+typedef struct {
+	uint64_t lo, hi;
+} CWISS_U128;
+
+/// Computes a double-width multiplication operation.
+static inline CWISS_U128 CWISS_Mul128(uint64_t a, uint64_t b)
+{
+	// TODO: de-intrinsics-ize this.
+	__uint128_t p = a;
+	p *= b;
+	return (CWISS_U128){ (uint64_t)p, (uint64_t)(p >> 64) };
+}
+
+/// Loads an unaligned u32.
+static inline uint32_t CWISS_Load32(const void *p)
+{
+	uint32_t v;
+	memcpy(&v, p, sizeof(v));
+	return v;
+}
+
+/// Loads an unaligned u64.
+static inline uint64_t CWISS_Load64(const void *p)
+{
+	uint64_t v;
+	memcpy(&v, p, sizeof(v));
+	return v;
+}
+
+/// Reads 9 to 16 bytes from p.
+static inline CWISS_U128 CWISS_Load9To16(const void *p, size_t len)
+{
+	const unsigned char *p8 = (const unsigned char *)p;
+	uint64_t lo = CWISS_Load64(p8);
+	uint64_t hi = CWISS_Load64(p8 + len - 8);
+	return (CWISS_U128){ lo, hi >> (128 - len * 8) };
+}
+
+/// Reads 4 to 8 bytes from p.
+static inline uint64_t CWISS_Load4To8(const void *p, size_t len)
+{
+	const unsigned char *p8 = (const unsigned char *)p;
+	uint64_t lo = CWISS_Load32(p8);
+	uint64_t hi = CWISS_Load32(p8 + len - 4);
+	return lo | (hi << (len - 4) * 8);
+}
+
+/// Reads 1 to 3 bytes from p.
+static inline uint32_t CWISS_Load1To3(const void *p, size_t len)
+{
+	const unsigned char *p8 = (const unsigned char *)p;
+	uint32_t mem0 = p8[0];
+	uint32_t mem1 = p8[len / 2];
+	uint32_t mem2 = p8[len - 1];
+	return (mem0 | (mem1 << (len / 2 * 8)) | (mem2 << ((len - 1) * 8)));
+}
+
+/// A abstract bitmask, such as that emitted by a SIMD instruction.
+///
+/// Specifically, this type implements a simple bitset whose representation is
+/// controlled by `width` and `shift`. `width` is the number of abstract bits in
+/// the bitset, while `shift` is the log-base-two of the width of an abstract
+/// bit in the representation.
+///
+/// For example, when `width` is 16 and `shift` is zero, this is just an
+/// ordinary 16-bit bitset occupying the low 16 bits of `mask`. When `width` is
+/// 8 and `shift` is 3, abstract bits are represented as the bytes `0x00` and
+/// `0x80`, and it occupies all 64 bits of the bitmask.
+typedef struct {
+	/// The mask, in the representation specified by `width` and `shift`.
+	uint64_t mask;
+	/// The number of abstract bits in the mask.
+	uint32_t width;
+	/// The log-base-two width of an abstract bit.
+	uint32_t shift;
+} CWISS_BitMask;
+
+/// Returns the index of the lowest abstract bit set in `self`.
+static inline uint32_t CWISS_BitMask_LowestBitSet(const CWISS_BitMask *self)
+{
+	return CWISS_TrailingZeros(self->mask) >> self->shift;
+}
+
+/// Returns the index of the highest abstract bit set in `self`.
+static inline uint32_t CWISS_BitMask_HighestBitSet(const CWISS_BitMask *self)
+{
+	return (uint32_t)(CWISS_BitWidth(self->mask) - 1) >> self->shift;
+}
+
+/// Return the number of trailing zero abstract bits.
+static inline uint32_t CWISS_BitMask_TrailingZeros(const CWISS_BitMask *self)
+{
+	return CWISS_TrailingZeros(self->mask) >> self->shift;
+}
+
+/// Return the number of leading zero abstract bits.
+static inline uint32_t CWISS_BitMask_LeadingZeros(const CWISS_BitMask *self)
+{
+	uint32_t total_significant_bits = self->width << self->shift;
+	uint32_t extra_bits = sizeof(self->mask) * 8 - total_significant_bits;
+	return (uint32_t)(CWISS_LeadingZeros(self->mask << extra_bits)) >>
+	       self->shift;
+}
+
+/// Iterates over the one bits in the mask.
+///
+/// If the mask is empty, returns `false`; otherwise, returns the index of the
+/// lowest one bit in the mask, and removes it from the set.
+static inline bool CWISS_BitMask_next(CWISS_BitMask *self, uint32_t *bit)
+{
+	if (self->mask == 0) {
+		return false;
+	}
+
+	*bit = CWISS_BitMask_LowestBitSet(self);
+	self->mask &= (self->mask - 1);
+	return true;
+}
+
+CWISS_END_EXTERN
+CWISS_END
+/// cwisstable/internal/bits.h /////////////////////////////////////////////////
+
+/// cwisstable/internal/control_byte.h /////////////////////////////////////////
+CWISS_BEGIN
+CWISS_BEGIN_EXTERN
+
+/// Control bytes and groups: the core of SwissTable optimization.
+///
+/// Control bytes are bytes (collected into groups of a platform-specific size)
+/// that define the state of the corresponding slot in the slot array. Group
+/// manipulation is tightly optimized to be as efficient as possible.
+
+/// A `CWISS_ControlByte` is a single control byte, which can have one of four
+/// states: empty, deleted, full (which has an associated seven-bit hash) and
+/// the sentinel. They have the following bit patterns:
+///
+/// ```
+///    empty: 1 0 0 0 0 0 0 0
+///  deleted: 1 1 1 1 1 1 1 0
+///     full: 0 h h h h h h h  // h represents the hash bits.
+/// sentinel: 1 1 1 1 1 1 1 1
+/// ```
+///
+/// These values are specifically tuned for SSE-flavored SIMD; future ports to
+/// other SIMD platforms may require choosing new values. The static_asserts
+/// below detail the source of these choices.
+typedef int8_t CWISS_ControlByte;
+#define CWISS_kEmpty (INT8_C(-128))
+#define CWISS_kDeleted (INT8_C(-2))
+#define CWISS_kSentinel (INT8_C(-1))
+// TODO: Wrap CWISS_ControlByte in a single-field struct to get strict-aliasing
+// benefits.
+
+static_assert(
+	(CWISS_kEmpty & CWISS_kDeleted & CWISS_kSentinel & 0x80) != 0,
+	"Special markers need to have the MSB to make checking for them efficient");
+static_assert(
+	CWISS_kEmpty < CWISS_kSentinel && CWISS_kDeleted < CWISS_kSentinel,
+	"CWISS_kEmpty and CWISS_kDeleted must be smaller than "
+	"CWISS_kSentinel to make the SIMD test of IsEmptyOrDeleted() efficient");
+static_assert(
+	CWISS_kSentinel == -1,
+	"CWISS_kSentinel must be -1 to elide loading it from memory into SIMD "
+	"registers (pcmpeqd xmm, xmm)");
+static_assert(CWISS_kEmpty == -128,
+	      "CWISS_kEmpty must be -128 to make the SIMD check for its "
+	      "existence efficient (psignb xmm, xmm)");
+static_assert(
+	(~CWISS_kEmpty & ~CWISS_kDeleted & CWISS_kSentinel & 0x7F) != 0,
+	"CWISS_kEmpty and CWISS_kDeleted must share an unset bit that is not "
+	"shared by CWISS_kSentinel to make the scalar test for "
+	"MatchEmptyOrDeleted() efficient");
+static_assert(CWISS_kDeleted == -2,
+	      "CWISS_kDeleted must be -2 to make the implementation of "
+	      "ConvertSpecialToEmptyAndFullToDeleted efficient");
+
+/// Returns a pointer to a control byte group that can be used by empty tables.
+static inline CWISS_ControlByte *CWISS_EmptyGroup()
+{
+	// A single block of empty control bytes for tables without any slots
+	// allocated. This enables removing a branch in the hot path of find().
+	alignas(16) static const CWISS_ControlByte kEmptyGroup[16] = {
+		CWISS_kSentinel, CWISS_kEmpty, CWISS_kEmpty, CWISS_kEmpty,
+		CWISS_kEmpty,	 CWISS_kEmpty, CWISS_kEmpty, CWISS_kEmpty,
+		CWISS_kEmpty,	 CWISS_kEmpty, CWISS_kEmpty, CWISS_kEmpty,
+		CWISS_kEmpty,	 CWISS_kEmpty, CWISS_kEmpty, CWISS_kEmpty,
+	};
+
+	// Const must be cast away here; no uses of this function will actually write
+	// to it, because it is only used for empty tables.
+	return (CWISS_ControlByte *)&kEmptyGroup;
+}
+
+/// Returns a hash seed.
+///
+/// The seed consists of the ctrl_ pointer, which adds enough entropy to ensure
+/// non-determinism of iteration order in most cases.
+static inline size_t CWISS_HashSeed(const CWISS_ControlByte *ctrl)
+{
+	// The low bits of the pointer have little or no entropy because of
+	// alignment. We shift the pointer to try to use higher entropy bits. A
+	// good number seems to be 12 bits, because that aligns with page size.
+	return ((uintptr_t)ctrl) >> 12;
+}
+
+/// Extracts the H1 portion of a hash: the high 57 bits mixed with a per-table
+/// salt.
+static inline size_t CWISS_H1(size_t hash, const CWISS_ControlByte *ctrl)
+{
+	return (hash >> 7) ^ CWISS_HashSeed(ctrl);
+}
+
+/// Extracts the H2 portion of a hash: the low 7 bits, which can be used as
+/// control byte.
+typedef uint8_t CWISS_h2_t;
+static inline CWISS_h2_t CWISS_H2(size_t hash)
+{
+	return hash & 0x7F;
+}
+
+/// Returns whether `c` is empty.
+static inline bool CWISS_IsEmpty(CWISS_ControlByte c)
+{
+	return c == CWISS_kEmpty;
+}
+
+/// Returns whether `c` is full.
+static inline bool CWISS_IsFull(CWISS_ControlByte c)
+{
+	return c >= 0;
+}
+
+/// Returns whether `c` is deleted.
+static inline bool CWISS_IsDeleted(CWISS_ControlByte c)
+{
+	return c == CWISS_kDeleted;
+}
+
+/// Returns whether `c` is empty or deleted.
+static inline bool CWISS_IsEmptyOrDeleted(CWISS_ControlByte c)
+{
+	return c < CWISS_kSentinel;
+}
+
+/// Asserts that `ctrl` points to a full control byte.
+#define CWISS_AssertIsFull(ctrl)                                                 \
+	CWISS_CHECK(                                                             \
+		(ctrl) != NULL && CWISS_IsFull(*(ctrl)),                         \
+		"Invalid operation on iterator (%p/%d). The element might have " \
+		"been erased, or the table might have rehashed.",                \
+		(ctrl), (ctrl) ? *(ctrl) : -1)
+
+/// Asserts that `ctrl` is either null OR points to a full control byte.
+#define CWISS_AssertIsValid(ctrl)                                                \
+	CWISS_CHECK(                                                             \
+		(ctrl) == NULL || CWISS_IsFull(*(ctrl)),                         \
+		"Invalid operation on iterator (%p/%d). The element might have " \
+		"been erased, or the table might have rehashed.",                \
+		(ctrl), (ctrl) ? *(ctrl) : -1)
+
+/// Constructs a `BitMask` with the correct parameters for whichever
+/// implementation of `CWISS_Group` is in use.
+#define CWISS_Group_BitMask(x)                              \
+	(CWISS_BitMask){ (uint64_t)(x), CWISS_Group_kWidth, \
+			 CWISS_Group_kShift };
+
+// TODO(#4): Port this to NEON.
+#if CWISS_HAVE_SSE2
+// Reference guide for intrinsics used below:
+//
+// * __m128i: An XMM (128-bit) word.
+//
+// * _mm_setzero_si128: Returns a zero vector.
+// * _mm_set1_epi8:     Returns a vector with the same i8 in each lane.
+//
+// * _mm_subs_epi8:    Saturating-subtracts two i8 vectors.
+// * _mm_and_si128:    Ands two i128s together.
+// * _mm_or_si128:     Ors two i128s together.
+// * _mm_andnot_si128: And-nots two i128s together.
+//
+// * _mm_cmpeq_epi8: Component-wise compares two i8 vectors for equality,
+//                   filling each lane with 0x00 or 0xff.
+// * _mm_cmpgt_epi8: Same as above, but using > rather than ==.
+//
+// * _mm_loadu_si128:  Performs an unaligned load of an i128.
+// * _mm_storeu_si128: Performs an unaligned store of a u128.
+//
+// * _mm_sign_epi8:     Retains, negates, or zeroes each i8 lane of the first
+//                      argument if the corresponding lane of the second
+//                      argument is positive, negative, or zero, respectively.
+// * _mm_movemask_epi8: Selects the sign bit out of each i8 lane and produces a
+//                      bitmask consisting of those bits.
+// * _mm_shuffle_epi8:  Selects i8s from the first argument, using the low
+//                      four bits of each i8 lane in the second argument as
+//                      indices.
+typedef __m128i CWISS_Group;
+#define CWISS_Group_kWidth ((size_t)16)
+#define CWISS_Group_kShift 0
+
+// https://github.com/abseil/abseil-cpp/issues/209
+// https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87853
+// _mm_cmpgt_epi8 is broken under GCC with -funsigned-char
+// Work around this by using the portable implementation of Group
+// when using -funsigned-char under GCC.
+static inline CWISS_Group CWISS_mm_cmpgt_epi8_fixed(CWISS_Group a,
+						    CWISS_Group b)
+{
+	if (CWISS_IS_GCC && CHAR_MIN == 0) { // std::is_unsigned_v<char>
+		const CWISS_Group mask = _mm_set1_epi8(0x80);
+		const CWISS_Group diff = _mm_subs_epi8(b, a);
+		return _mm_cmpeq_epi8(_mm_and_si128(diff, mask), mask);
+	}
+	return _mm_cmpgt_epi8(a, b);
+}
+
+static inline CWISS_Group CWISS_Group_new(const CWISS_ControlByte *pos)
+{
+	return _mm_loadu_si128((const CWISS_Group *)pos);
+}
+
+// Returns a bitmask representing the positions of slots that match hash.
+static inline CWISS_BitMask CWISS_Group_Match(const CWISS_Group *self,
+					      CWISS_h2_t hash)
+{
+	return CWISS_Group_BitMask(
+		_mm_movemask_epi8(_mm_cmpeq_epi8(_mm_set1_epi8(hash), *self)))
+}
+
+// Returns a bitmask representing the positions of empty slots.
+static inline CWISS_BitMask CWISS_Group_MatchEmpty(const CWISS_Group *self)
+{
+#if CWISS_HAVE_SSSE3
+	// This only works because ctrl_t::kEmpty is -128.
+	return CWISS_Group_BitMask(
+		_mm_movemask_epi8(_mm_sign_epi8(*self, *self)));
+#else
+	return CWISS_Group_Match(self, CWISS_kEmpty);
+#endif
+}
+
+// Returns a bitmask representing the positions of empty or deleted slots.
+static inline CWISS_BitMask
+CWISS_Group_MatchEmptyOrDeleted(const CWISS_Group *self)
+{
+	CWISS_Group special = _mm_set1_epi8((uint8_t)CWISS_kSentinel);
+	return CWISS_Group_BitMask(
+		_mm_movemask_epi8(CWISS_mm_cmpgt_epi8_fixed(special, *self)));
+}
+
+// Returns the number of trailing empty or deleted elements in the group.
+static inline uint32_t
+CWISS_Group_CountLeadingEmptyOrDeleted(const CWISS_Group *self)
+{
+	CWISS_Group special = _mm_set1_epi8((uint8_t)CWISS_kSentinel);
+	return CWISS_TrailingZeros(
+		(uint32_t)(_mm_movemask_epi8(
+				   CWISS_mm_cmpgt_epi8_fixed(special, *self)) +
+			   1));
+}
+
+static inline void
+CWISS_Group_ConvertSpecialToEmptyAndFullToDeleted(const CWISS_Group *self,
+						  CWISS_ControlByte *dst)
+{
+	CWISS_Group msbs = _mm_set1_epi8((char)-128);
+	CWISS_Group x126 = _mm_set1_epi8(126);
+#if CWISS_HAVE_SSSE3
+	CWISS_Group res = _mm_or_si128(_mm_shuffle_epi8(x126, *self), msbs);
+#else
+	CWISS_Group zero = _mm_setzero_si128();
+	CWISS_Group special_mask = CWISS_mm_cmpgt_epi8_fixed(zero, *self);
+	CWISS_Group res =
+		_mm_or_si128(msbs, _mm_andnot_si128(special_mask, x126));
+#endif
+	_mm_storeu_si128((CWISS_Group *)dst, res);
+}
+#else // CWISS_HAVE_SSE2
+typedef uint64_t CWISS_Group;
+#define CWISS_Group_kWidth ((size_t)8)
+#define CWISS_Group_kShift 3
+
+// NOTE: Endian-hostile.
+static inline CWISS_Group CWISS_Group_new(const CWISS_ControlByte *pos)
+{
+	CWISS_Group val;
+	memcpy(&val, pos, sizeof(val));
+	return val;
+}
+
+static inline CWISS_BitMask CWISS_Group_Match(const CWISS_Group *self,
+					      CWISS_h2_t hash)
+{
+	// For the technique, see:
+	// http://graphics.stanford.edu/~seander/bithacks.html##ValueInWord
+	// (Determine if a word has a byte equal to n).
+	//
+	// Caveat: there are false positives but:
+	// - they only occur if there is a real match
+	// - they never occur on ctrl_t::kEmpty, ctrl_t::kDeleted, ctrl_t::kSentinel
+	// - they will be handled gracefully by subsequent checks in code
+	//
+	// Example:
+	//   v = 0x1716151413121110
+	//   hash = 0x12
+	//   retval = (v - lsbs) & ~v & msbs = 0x0000000080800000
+	uint64_t msbs = 0x8080808080808080ULL;
+	uint64_t lsbs = 0x0101010101010101ULL;
+	uint64_t x = *self ^ (lsbs * hash);
+	return CWISS_Group_BitMask((x - lsbs) & ~x & msbs);
+}
+
+static inline CWISS_BitMask CWISS_Group_MatchEmpty(const CWISS_Group *self)
+{
+	uint64_t msbs = 0x8080808080808080ULL;
+	return CWISS_Group_BitMask((*self & (~*self << 6)) & msbs);
+}
+
+static inline CWISS_BitMask
+CWISS_Group_MatchEmptyOrDeleted(const CWISS_Group *self)
+{
+	uint64_t msbs = 0x8080808080808080ULL;
+	return CWISS_Group_BitMask((*self & (~*self << 7)) & msbs);
+}
+
+static inline uint32_t
+CWISS_Group_CountLeadingEmptyOrDeleted(const CWISS_Group *self)
+{
+	uint64_t gaps = 0x00FEFEFEFEFEFEFEULL;
+	return (CWISS_TrailingZeros(((~*self & (*self >> 7)) | gaps) + 1) +
+		7) >>
+	       3;
+}
+
+static inline void
+CWISS_Group_ConvertSpecialToEmptyAndFullToDeleted(const CWISS_Group *self,
+						  CWISS_ControlByte *dst)
+{
+	uint64_t msbs = 0x8080808080808080ULL;
+	uint64_t lsbs = 0x0101010101010101ULL;
+	uint64_t x = *self & msbs;
+	uint64_t res = (~x + (x >> 7)) & ~lsbs;
+	memcpy(dst, &res, sizeof(res));
+}
+#endif // CWISS_HAVE_SSE2
+
+CWISS_END_EXTERN
+CWISS_END
+/// cwisstable/internal/control_byte.h /////////////////////////////////////////
+
+/// cwisstable/internal/capacity.h /////////////////////////////////////////////
+/// Capacity, load factor, and allocation size computations for a SwissTable.
+///
+/// A SwissTable's backing array consists of control bytes followed by slots
+/// that may or may not contain objects.
+///
+/// The layout of the backing array, for `capacity` slots, is thus, as a
+/// pseudo-struct:
+/// ```
+/// struct CWISS_BackingArray {
+///   // Control bytes for the "real" slots.
+///   CWISS_ControlByte ctrl[capacity];
+///   // Always `CWISS_kSentinel`. This is used by iterators to find when to
+///   // stop and serves no other purpose.
+///   CWISS_ControlByte sentinel;
+///   // A copy of the first `kWidth - 1` elements of `ctrl`. This is used so
+///   // that if a probe sequence picks a value near the end of `ctrl`,
+///   // `CWISS_Group` will have valid control bytes to look at.
+///   //
+///   // As an interesting special-case, such probe windows will never choose
+///   // the zeroth slot as a candidate, because they will see `kSentinel`
+///   // instead of the correct H2 value.
+///   CWISS_ControlByte clones[kWidth - 1];
+///   // Alignment padding equal to `alignof(slot_type)`.
+///   char padding_;
+///   // The actual slot data.
+///   char slots[capacity * sizeof(slot_type)];
+/// };
+/// ```
+///
+/// The length of this array is computed by `CWISS_AllocSize()`.
+
+CWISS_BEGIN
+CWISS_BEGIN_EXTERN
+
+/// Returns he number of "cloned control bytes".
+///
+/// This is the number of control bytes that are present both at the beginning
+/// of the control byte array and at the end, such that we can create a
+/// `CWISS_Group_kWidth`-width probe window starting from any control byte.
+static inline size_t CWISS_NumClonedBytes(void)
+{
+	return CWISS_Group_kWidth - 1;
+}
+
+/// Returns whether `n` is a valid capacity (i.e., number of slots).
+///
+/// A valid capacity is a non-zero integer `2^m - 1`.
+static inline bool CWISS_IsValidCapacity(size_t n)
+{
+	return ((n + 1) & n) == 0 && n > 0;
+}
+
+/// Returns some per-call entropy.
+///
+/// Currently, the entropy is produced by XOR'ing the address of a (preferably
+/// thread-local) value with a perpetually-incrementing value.
+static inline size_t RandomSeed(void)
+{
+#ifdef CWISS_THREAD_LOCAL
+	static DEFINE_PER_CPU(size_t, counter) = {};
+	size_t *address = this_cpu_ptr(&counter);
+	size_t value = (*address)++;
+#else
+	static volatile CWISS_ATOMIC_T(size_t) counter;
+	size_t value = CWISS_ATOMIC_INC(counter);
+	size_t *address = &counter;
+#endif
+	return value ^ (size_t)address;
+}
+
+/// Mixes a randomly generated per-process seed with `hash` and `ctrl` to
+/// randomize insertion order within groups.
+CWISS_INLINE_NEVER static bool
+CWISS_ShouldInsertBackwards(size_t hash, const CWISS_ControlByte *ctrl)
+{
+	// To avoid problems with weak hashes and single bit tests, we use % 13.
+	// TODO(kfm,sbenza): revisit after we do unconditional mixing
+	return (CWISS_H1(hash, ctrl) ^ RandomSeed()) % 13 > 6;
+}
+
+/// Applies the following mapping to every byte in the control array:
+///   * kDeleted -> kEmpty
+///   * kEmpty -> kEmpty
+///   * _ -> kDeleted
+///
+/// Preconditions: `CWISS_IsValidCapacity(capacity)`,
+/// `ctrl[capacity]` == `kSentinel`, `ctrl[i] != kSentinel for i < capacity`.
+CWISS_INLINE_NEVER static void
+CWISS_ConvertDeletedToEmptyAndFullToDeleted(CWISS_ControlByte *ctrl,
+					    size_t capacity)
+{
+	CWISS_DCHECK(ctrl[capacity] == CWISS_kSentinel,
+		     "bad ctrl value at %zu: %02x", capacity, ctrl[capacity]);
+	CWISS_DCHECK(CWISS_IsValidCapacity(capacity), "invalid capacity: %zu",
+		     capacity);
+
+	for (CWISS_ControlByte *pos = ctrl; pos < ctrl + capacity;
+	     pos += CWISS_Group_kWidth) {
+		CWISS_Group g = CWISS_Group_new(pos);
+		CWISS_Group_ConvertSpecialToEmptyAndFullToDeleted(&g, pos);
+	}
+	// Copy the cloned ctrl bytes.
+	memcpy(ctrl + capacity + 1, ctrl, CWISS_NumClonedBytes());
+	ctrl[capacity] = CWISS_kSentinel;
+}
+
+/// Sets `ctrl` to `{kEmpty, ..., kEmpty, kSentinel}`, marking the entire
+/// array as deleted.
+static inline void CWISS_ResetCtrl(size_t capacity, CWISS_ControlByte *ctrl,
+				   const void *slots, size_t slot_size)
+{
+	memset(ctrl, CWISS_kEmpty, capacity + 1 + CWISS_NumClonedBytes());
+	ctrl[capacity] = CWISS_kSentinel;
+	CWISS_PoisonMemory(slots, slot_size * capacity);
+}
+
+/// Sets `ctrl[i]` to `h`.
+///
+/// Unlike setting it directly, this function will perform bounds checks and
+/// mirror the value to the cloned tail if necessary.
+static inline void CWISS_SetCtrl(size_t i, CWISS_ControlByte h, size_t capacity,
+				 CWISS_ControlByte *ctrl, const void *slots,
+				 size_t slot_size)
+{
+	CWISS_DCHECK(i < capacity, "CWISS_SetCtrl out-of-bounds: %zu >= %zu", i,
+		     capacity);
+
+	const char *slot = ((const char *)slots) + i * slot_size;
+	if (CWISS_IsFull(h)) {
+		CWISS_UnpoisonMemory(slot, slot_size);
+	} else {
+		CWISS_PoisonMemory(slot, slot_size);
+	}
+
+	// This is intentionally branchless. If `i < kWidth`, it will write to the
+	// cloned bytes as well as the "real" byte; otherwise, it will store `h`
+	// twice.
+	size_t mirrored_i = ((i - CWISS_NumClonedBytes()) & capacity) +
+			    (CWISS_NumClonedBytes() & capacity);
+	ctrl[i] = h;
+	ctrl[mirrored_i] = h;
+}
+
+/// Converts `n` into the next valid capacity, per `CWISS_IsValidCapacity`.
+static inline size_t CWISS_NormalizeCapacity(size_t n)
+{
+	return n ? SIZE_MAX >> CWISS_LeadingZeros(n) : 1;
+}
+
+// General notes on capacity/growth methods below:
+// - We use 7/8th as maximum load factor. For 16-wide groups, that gives an
+//   average of two empty slots per group.
+// - For (capacity+1) >= Group::kWidth, growth is 7/8*capacity.
+// - For (capacity+1) < Group::kWidth, growth == capacity. In this case, we
+//   never need to probe (the whole table fits in one group) so we don't need a
+//   load factor less than 1.
+
+/// Given `capacity`, applies the load factor; i.e., it returns the maximum
+/// number of values we should put into the table before a rehash.
+static inline size_t CWISS_CapacityToGrowth(size_t capacity)
+{
+	CWISS_DCHECK(CWISS_IsValidCapacity(capacity), "invalid capacity: %zu",
+		     capacity);
+	// `capacity*7/8`
+	if (CWISS_Group_kWidth == 8 && capacity == 7) {
+		// x-x/8 does not work when x==7.
+		return 6;
+	}
+	return capacity - capacity / 8;
+}
+
+/// Given `growth`, "unapplies" the load factor to find how large the capacity
+/// should be to stay within the load factor.
+///
+/// This might not be a valid capacity and `CWISS_NormalizeCapacity()` may be
+/// necessary.
+static inline size_t CWISS_GrowthToLowerboundCapacity(size_t growth)
+{
+	// `growth*8/7`
+	if (CWISS_Group_kWidth == 8 && growth == 7) {
+		// x+(x-1)/7 does not work when x==7.
+		return 8;
+	}
+	return growth + (size_t)((((int64_t)growth) - 1) / 7);
+}
+
+// The allocated block consists of `capacity + 1 + NumClonedBytes()` control
+// bytes followed by `capacity` slots, which must be aligned to `slot_align`.
+// SlotOffset returns the offset of the slots into the allocated block.
+
+/// Given the capacity of a table, computes the offset (from the start of the
+/// backing allocation) at which the slots begin.
+static inline size_t CWISS_SlotOffset(size_t capacity, size_t slot_align)
+{
+	CWISS_DCHECK(CWISS_IsValidCapacity(capacity), "invalid capacity: %zu",
+		     capacity);
+	const size_t num_control_bytes = capacity + 1 + CWISS_NumClonedBytes();
+	return (num_control_bytes + slot_align - 1) & (~slot_align + 1);
+}
+
+/// Given the capacity of a table, computes the total size of the backing
+/// array.
+static inline size_t CWISS_AllocSize(size_t capacity, size_t slot_size,
+				     size_t slot_align)
+{
+	return CWISS_SlotOffset(capacity, slot_align) + capacity * slot_size;
+}
+
+/// Whether a table is "small". A small table fits entirely into a probing
+/// group, i.e., has a capacity equal to the size of a `CWISS_Group`.
+///
+/// In small mode we are able to use the whole capacity. The extra control
+/// bytes give us at least one "empty" control byte to stop the iteration.
+/// This is important to make 1 a valid capacity.
+///
+/// In small mode only the first `capacity` control bytes after the sentinel
+/// are valid. The rest contain dummy ctrl_t::kEmpty values that do not
+/// represent a real slot. This is important to take into account on
+/// `CWISS_FindFirstNonFull()`, where we never try
+/// `CWISS_ShouldInsertBackwards()` for small tables.
+static inline bool CWISS_IsSmall(size_t capacity)
+{
+	return capacity < CWISS_Group_kWidth - 1;
+}
+
+CWISS_END_EXTERN
+CWISS_END
+/// cwisstable/internal/capacity.h /////////////////////////////////////////////
+
+/// cwisstable/internal/probe.h ////////////////////////////////////////////////
+/// Table probing functions.
+///
+/// "Probing" refers to the process of trying to find the matching entry for a
+/// given lookup by repeatedly searching for values throughout the table.
+
+CWISS_BEGIN
+CWISS_BEGIN_EXTERN
+
+/// The state for a probe sequence.
+///
+/// Currently, the sequence is a triangular progression of the form
+/// ```
+/// p(i) := kWidth/2 * (i^2 - i) + hash (mod mask + 1)
+/// ```
+///
+/// The use of `kWidth` ensures that each probe step does not overlap groups;
+/// the sequence effectively outputs the addresses of *groups* (although not
+/// necessarily aligned to any boundary). The `CWISS_Group` machinery allows us
+/// to check an entire group with minimal branching.
+///
+/// Wrapping around at `mask + 1` is important, but not for the obvious reason.
+/// As described in capacity.h, the first few entries of the control byte array
+/// is mirrored at the end of the array, which `CWISS_Group` will find and use
+/// for selecting candidates. However, when those candidates' slots are
+/// actually inspected, there are no corresponding slots for the cloned bytes,
+/// so we need to make sure we've treated those offsets as "wrapping around".
+typedef struct {
+	size_t mask_;
+	size_t offset_;
+	size_t index_;
+} CWISS_ProbeSeq;
+
+/// Creates a new probe sequence using `hash` as the initial value of the
+/// sequence and `mask` (usually the capacity of the table) as the mask to
+/// apply to each value in the progression.
+static inline CWISS_ProbeSeq CWISS_ProbeSeq_new(size_t hash, size_t mask)
+{
+	return (CWISS_ProbeSeq){
+		.mask_ = mask,
+		.offset_ = hash & mask,
+	};
+}
+
+/// Returns the slot `i` indices ahead of `self` within the bounds expressed by
+/// `mask`.
+static inline size_t CWISS_ProbeSeq_offset(const CWISS_ProbeSeq *self, size_t i)
+{
+	return (self->offset_ + i) & self->mask_;
+}
+
+/// Advances the sequence; the value can be obtained by calling
+/// `CWISS_ProbeSeq_offset()` or inspecting `offset_`.
+static inline void CWISS_ProbeSeq_next(CWISS_ProbeSeq *self)
+{
+	self->index_ += CWISS_Group_kWidth;
+	self->offset_ += self->index_;
+	self->offset_ &= self->mask_;
+}
+
+/// Begins a probing operation on `ctrl`, using `hash`.
+static inline CWISS_ProbeSeq CWISS_ProbeSeq_Start(const CWISS_ControlByte *ctrl,
+						  size_t hash, size_t capacity)
+{
+	return CWISS_ProbeSeq_new(CWISS_H1(hash, ctrl), capacity);
+}
+
+// The return value of `CWISS_FindFirstNonFull()`.
+typedef struct {
+	size_t offset;
+	size_t probe_length;
+} CWISS_FindInfo;
+
+/// Probes an array of control bits using a probe sequence derived from `hash`,
+/// and returns the offset corresponding to the first deleted or empty slot.
+///
+/// Behavior when the entire table is full is undefined.
+///
+/// NOTE: this function must work with tables having both empty and deleted
+/// slots in the same group. Such tables appear during
+/// `CWISS_RawTable_DropDeletesWithoutResize()`.
+static inline CWISS_FindInfo
+CWISS_FindFirstNonFull(const CWISS_ControlByte *ctrl, size_t hash,
+		       size_t capacity)
+{
+	CWISS_ProbeSeq seq = CWISS_ProbeSeq_Start(ctrl, hash, capacity);
+	while (true) {
+		CWISS_Group g = CWISS_Group_new(ctrl + seq.offset_);
+		CWISS_BitMask mask = CWISS_Group_MatchEmptyOrDeleted(&g);
+		if (mask.mask) {
+#ifndef NDEBUG
+			// We want to add entropy even when ASLR is not enabled.
+			// In debug build we will randomly insert in either the front or back of
+			// the group.
+			// TODO(kfm,sbenza): revisit after we do unconditional mixing
+			if (!CWISS_IsSmall(capacity) &&
+			    CWISS_ShouldInsertBackwards(hash, ctrl)) {
+				return (CWISS_FindInfo){
+					CWISS_ProbeSeq_offset(
+						&seq,
+						CWISS_BitMask_HighestBitSet(
+							&mask)),
+					seq.index_
+				};
+			}
+#endif
+			return (CWISS_FindInfo){
+				CWISS_ProbeSeq_offset(
+					&seq,
+					CWISS_BitMask_TrailingZeros(&mask)),
+				seq.index_
+			};
+		}
+		CWISS_ProbeSeq_next(&seq);
+		CWISS_DCHECK(seq.index_ <= capacity, "full table!");
+	}
+}
+
+CWISS_END_EXTERN
+CWISS_END
+/// cwisstable/internal/probe.h ////////////////////////////////////////////////
+
+/// cwisstable/internal/absl_hash.h ////////////////////////////////////////////
+/// Implementation details of AbslHash.
+
+CWISS_BEGIN
+CWISS_BEGIN_EXTERN
+
+static inline uint64_t CWISS_AbslHash_LowLevelMix(uint64_t v0, uint64_t v1)
+{
+#ifndef __aarch64__
+	// The default bit-mixer uses 64x64->128-bit multiplication.
+	CWISS_U128 p = CWISS_Mul128(v0, v1);
+	return p.hi ^ p.lo;
+#else
+	// The default bit-mixer above would perform poorly on some ARM microarchs,
+	// where calculating a 128-bit product requires a sequence of two
+	// instructions with a high combined latency and poor throughput.
+	// Instead, we mix bits using only 64-bit arithmetic, which is faster.
+	uint64_t p = v0 ^ CWISS_RotateLeft(v1, 40);
+	p *= v1 ^ CWISS_RotateLeft(v0, 39);
+	return p ^ (p >> 11);
+#endif
+}
+
+CWISS_INLINE_NEVER
+static uint64_t CWISS_AbslHash_LowLevelHash(const void *data, size_t len,
+					    uint64_t seed,
+					    const uint64_t salt[5])
+{
+	const char *ptr = (const char *)data;
+	uint64_t starting_length = (uint64_t)len;
+	uint64_t current_state = seed ^ salt[0];
+
+	if (len > 64) {
+		// If we have more than 64 bytes, we're going to handle chunks of 64
+		// bytes at a time. We're going to build up two separate hash states
+		// which we will then hash together.
+		uint64_t duplicated_state = current_state;
+
+		do {
+			uint64_t chunk[8];
+			memcpy(chunk, ptr, sizeof(chunk));
+
+			uint64_t cs0 = CWISS_AbslHash_LowLevelMix(
+				chunk[0] ^ salt[1], chunk[1] ^ current_state);
+			uint64_t cs1 = CWISS_AbslHash_LowLevelMix(
+				chunk[2] ^ salt[2], chunk[3] ^ current_state);
+			current_state = (cs0 ^ cs1);
+
+			uint64_t ds0 = CWISS_AbslHash_LowLevelMix(
+				chunk[4] ^ salt[3],
+				chunk[5] ^ duplicated_state);
+			uint64_t ds1 = CWISS_AbslHash_LowLevelMix(
+				chunk[6] ^ salt[4],
+				chunk[7] ^ duplicated_state);
+			duplicated_state = (ds0 ^ ds1);
+
+			ptr += 64;
+			len -= 64;
+		} while (len > 64);
+
+		current_state = current_state ^ duplicated_state;
+	}
+
+	// We now have a data `ptr` with at most 64 bytes and the current state
+	// of the hashing state machine stored in current_state.
+	while (len > 16) {
+		uint64_t a = CWISS_Load64(ptr);
+		uint64_t b = CWISS_Load64(ptr + 8);
+
+		current_state = CWISS_AbslHash_LowLevelMix(a ^ salt[1],
+							   b ^ current_state);
+
+		ptr += 16;
+		len -= 16;
+	}
+
+	// We now have a data `ptr` with at most 16 bytes.
+	uint64_t a = 0;
+	uint64_t b = 0;
+	if (len > 8) {
+		// When we have at least 9 and at most 16 bytes, set A to the first 64
+		// bits of the input and B to the last 64 bits of the input. Yes, they will
+		// overlap in the middle if we are working with less than the full 16
+		// bytes.
+		a = CWISS_Load64(ptr);
+		b = CWISS_Load64(ptr + len - 8);
+	} else if (len > 3) {
+		// If we have at least 4 and at most 8 bytes, set A to the first 32
+		// bits and B to the last 32 bits.
+		a = CWISS_Load32(ptr);
+		b = CWISS_Load32(ptr + len - 4);
+	} else if (len > 0) {
+		// If we have at least 1 and at most 3 bytes, read all of the provided
+		// bits into A, with some adjustments.
+		a = CWISS_Load1To3(ptr, len);
+	}
+
+	uint64_t w = CWISS_AbslHash_LowLevelMix(a ^ salt[1], b ^ current_state);
+	uint64_t z = salt[1] ^ starting_length;
+	return CWISS_AbslHash_LowLevelMix(w, z);
+}
+
+// A non-deterministic seed.
+//
+// The current purpose of this seed is to generate non-deterministic results
+// and prevent having users depend on the particular hash values.
+// It is not meant as a security feature right now, but it leaves the door
+// open to upgrade it to a true per-process random seed. A true random seed
+// costs more and we don't need to pay for that right now.
+//
+// On platforms with ASLR, we take advantage of it to make a per-process
+// random value.
+// See https://en.wikipedia.org/wiki/Address_space_layout_randomization
+//
+// On other platforms this is still going to be non-deterministic but most
+// probably per-build and not per-process.
+static const void *const CWISS_AbslHash_kSeed = &CWISS_AbslHash_kSeed;
+
+// The salt array used by LowLevelHash. This array is NOT the mechanism used to
+// make absl::Hash non-deterministic between program invocations.  See `Seed()`
+// for that mechanism.
+//
+// Any random values are fine. These values are just digits from the decimal
+// part of pi.
+// https://en.wikipedia.org/wiki/Nothing-up-my-sleeve_number
+static const uint64_t CWISS_AbslHash_kHashSalt[5] = {
+	0x243F6A8885A308D3, 0x13198A2E03707344, 0xA4093822299F31D0,
+	0x082EFA98EC4E6C89, 0x452821E638D01377,
+};
+
+#define CWISS_AbslHash_kPiecewiseChunkSize ((size_t)1024)
+
+typedef uint64_t CWISS_AbslHash_State_;
+#define CWISS_AbslHash_kInit_ ((CWISS_AbslHash_State_)CWISS_AbslHash_kSeed)
+
+static inline void CWISS_AbslHash_Mix(CWISS_AbslHash_State_ *state, uint64_t v)
+{
+	const uint64_t kMul = sizeof(size_t) == 4 ? 0xcc9e2d51 :
+						    0x9ddfea08eb382d69;
+	*state = CWISS_AbslHash_LowLevelMix(*state + v, kMul);
+}
+
+CWISS_INLINE_NEVER
+static uint64_t CWISS_AbslHash_Hash64(const void *val, size_t len)
+{
+	return CWISS_AbslHash_LowLevelHash(val, len, CWISS_AbslHash_kInit_,
+					   CWISS_AbslHash_kHashSalt);
+}
+
+CWISS_END_EXTERN
+CWISS_END
+/// cwisstable/internal/absl_hash.h ////////////////////////////////////////////
+
+/// cwisstable/hash.h //////////////////////////////////////////////////////////
+/// Hash functions.
+///
+/// This file provides some hash functions to use with cwisstable types.
+///
+/// Every hash function defines four symbols:
+///   - `CWISS_<Hash>_State`, the state of the hash function.
+///   - `CWISS_<Hash>_kInit`, the initial value of the hash state.
+///   - `void CWISS_<Hash>_Write(State*, const void*, size_t)`, write some more
+///     data into the hash state.
+///   - `size_t CWISS_<Hash>_Finish(State)`, digest the state into a final hash
+///     value.
+///
+/// Currently available are two hashes: `FxHash`, which is small and fast, and
+/// `AbslHash`, the hash function used by Abseil.
+///
+/// `AbslHash` is the default hash function.
+
+CWISS_BEGIN
+CWISS_BEGIN_EXTERN
+
+typedef size_t CWISS_FxHash_State;
+#define CWISS_FxHash_kInit ((CWISS_FxHash_State)0)
+static inline void CWISS_FxHash_Write(CWISS_FxHash_State *state,
+				      const void *val, size_t len)
+{
+	const size_t kSeed = (size_t)(UINT64_C(0x517cc1b727220a95));
+	const uint32_t kRotate = 5;
+
+	const char *p = (const char *)val;
+	CWISS_FxHash_State state_ = *state;
+	while (len > 0) {
+		size_t word = 0;
+		size_t to_read = len >= sizeof(state_) ? sizeof(state_) : len;
+		memcpy(&word, p, to_read);
+
+		state_ = CWISS_RotateLeft(state_, kRotate);
+		state_ ^= word;
+		state_ *= kSeed;
+
+		len -= to_read;
+		p += to_read;
+	}
+	*state = state_;
+}
+static inline size_t CWISS_FxHash_Finish(CWISS_FxHash_State state)
+{
+	return state;
+}
+
+typedef CWISS_AbslHash_State_ CWISS_AbslHash_State;
+#define CWISS_AbslHash_kInit CWISS_AbslHash_kInit_
+static inline void CWISS_AbslHash_Write(CWISS_AbslHash_State *state,
+					const void *val, size_t len)
+{
+	const char *val8 = (const char *)val;
+	if (CWISS_LIKELY(len < CWISS_AbslHash_kPiecewiseChunkSize)) {
+		goto CWISS_AbslHash_Write_small;
+	}
+
+	while (len >= CWISS_AbslHash_kPiecewiseChunkSize) {
+		CWISS_AbslHash_Mix(state,
+				   CWISS_AbslHash_Hash64(
+					   val8,
+					   CWISS_AbslHash_kPiecewiseChunkSize));
+		len -= CWISS_AbslHash_kPiecewiseChunkSize;
+		val8 += CWISS_AbslHash_kPiecewiseChunkSize;
+	}
+
+CWISS_AbslHash_Write_small:;
+	uint64_t v;
+	if (len > 16) {
+		v = CWISS_AbslHash_Hash64(val8, len);
+	} else if (len > 8) {
+		CWISS_U128 p = CWISS_Load9To16(val8, len);
+		CWISS_AbslHash_Mix(state, p.lo);
+		v = p.hi;
+	} else if (len >= 4) {
+		v = CWISS_Load4To8(val8, len);
+	} else if (len > 0) {
+		v = CWISS_Load1To3(val8, len);
+	} else {
+		// Empty ranges have no effect.
+		return;
+	}
+
+	CWISS_AbslHash_Mix(state, v);
+}
+static inline size_t CWISS_AbslHash_Finish(CWISS_AbslHash_State state)
+{
+	return state;
+}
+
+CWISS_END_EXTERN
+CWISS_END
+/// cwisstable/hash.h //////////////////////////////////////////////////////////
+
+/// cwisstable/internal/extract.h //////////////////////////////////////////////
+/// Macro keyword-arguments machinery.
+///
+/// This file defines a number of macros used by policy.h to implement its
+/// policy construction macros.
+///
+/// The way they work is more-or-less like this:
+///
+/// `CWISS_EXTRACT(foo, ...)` will find the first parenthesized pair that
+/// matches exactly `(foo, whatever)`, where `foo` is part of a small set of
+/// tokens defined in this file. To do so, this first expands into
+///
+/// ```
+/// CWISS_EXTRACT1(CWISS_EXTRACT_foo, (k, v), ...)
+/// ```
+///
+/// where `(k, v)` is the first pair in the macro arguments. This in turn
+/// expands into
+///
+/// ```
+/// CWISS_SELECT01(CWISS_EXTRACT_foo (k, v), CWISS_EXTRACT_VALUE, (k, v),
+/// CWISS_EXTRACT2, (needle, __VA_ARGS__), CWISS_NOTHING)
+/// ```
+///
+/// At this point, the preprocessor will expand `CWISS_EXTRACT_foo (k, v)` into
+/// `CWISS_EXTRACT_foo_k`, which will be further expanded into `tok,tok,tok` if
+/// `k` is the token `foo`, because we've defined `CWISS_EXTRACT_foo_foo` as a
+/// macro.
+///
+/// `CWISS_SELECT01` will then delete the first three arguments, and the fourth
+/// and fifth arguments will be juxtaposed.
+///
+/// In the case that `k` does not match, `CWISS_EXTRACT_foo (k, v), IDENT, (k,
+/// v),` is deleted from the call, and the rest of the macro expands into
+/// `CWISS_EXTRACT2(needle, __VA_ARGS__, _)` repeating the cycle but with a
+/// different name.
+///
+/// In the case that `k` matches, the `tok,tok,tok` is deleted, and we get
+/// `CWISS_EXTRACT_VALUE(k, v)`, which expands to `v`.
+
+#define CWISS_EXTRACT(needle_, default_, ...) \
+	(CWISS_EXTRACT_RAW(needle_, default_, __VA_ARGS__))
+
+#define CWISS_EXTRACT_RAW(needle_, default_, ...)             \
+	CWISS_EXTRACT00(CWISS_EXTRACT_##needle_, __VA_ARGS__, \
+			(needle_, default_))
+
+#define CWISS_EXTRACT_VALUE(key, val) val
+
+// NOTE: Everything below this line is generated by cwisstable/extract.py!
+// !!!
+
+#define CWISS_EXTRACT_obj_copy(key_, val_) CWISS_EXTRACT_obj_copyZ##key_
+#define CWISS_EXTRACT_obj_copyZobj_copy \
+	CWISS_NOTHING, CWISS_NOTHING, CWISS_NOTHING
+#define CWISS_EXTRACT_obj_dtor(key_, val_) CWISS_EXTRACT_obj_dtorZ##key_
+#define CWISS_EXTRACT_obj_dtorZobj_dtor \
+	CWISS_NOTHING, CWISS_NOTHING, CWISS_NOTHING
+#define CWISS_EXTRACT_key_hash(key_, val_) CWISS_EXTRACT_key_hashZ##key_
+#define CWISS_EXTRACT_key_hashZkey_hash \
+	CWISS_NOTHING, CWISS_NOTHING, CWISS_NOTHING
+#define CWISS_EXTRACT_key_eq(key_, val_) CWISS_EXTRACT_key_eqZ##key_
+#define CWISS_EXTRACT_key_eqZkey_eq CWISS_NOTHING, CWISS_NOTHING, CWISS_NOTHING
+#define CWISS_EXTRACT_alloc_alloc(key_, val_) CWISS_EXTRACT_alloc_allocZ##key_
+#define CWISS_EXTRACT_alloc_allocZalloc_alloc \
+	CWISS_NOTHING, CWISS_NOTHING, CWISS_NOTHING
+#define CWISS_EXTRACT_alloc_free(key_, val_) CWISS_EXTRACT_alloc_freeZ##key_
+#define CWISS_EXTRACT_alloc_freeZalloc_free \
+	CWISS_NOTHING, CWISS_NOTHING, CWISS_NOTHING
+#define CWISS_EXTRACT_slot_size(key_, val_) CWISS_EXTRACT_slot_sizeZ##key_
+#define CWISS_EXTRACT_slot_sizeZslot_size \
+	CWISS_NOTHING, CWISS_NOTHING, CWISS_NOTHING
+#define CWISS_EXTRACT_slot_align(key_, val_) CWISS_EXTRACT_slot_alignZ##key_
+#define CWISS_EXTRACT_slot_alignZslot_align \
+	CWISS_NOTHING, CWISS_NOTHING, CWISS_NOTHING
+#define CWISS_EXTRACT_slot_init(key_, val_) CWISS_EXTRACT_slot_initZ##key_
+#define CWISS_EXTRACT_slot_initZslot_init \
+	CWISS_NOTHING, CWISS_NOTHING, CWISS_NOTHING
+#define CWISS_EXTRACT_slot_transfer(key_, val_) \
+	CWISS_EXTRACT_slot_transferZ##key_
+#define CWISS_EXTRACT_slot_transferZslot_transfer \
+	CWISS_NOTHING, CWISS_NOTHING, CWISS_NOTHING
+#define CWISS_EXTRACT_slot_get(key_, val_) CWISS_EXTRACT_slot_getZ##key_
+#define CWISS_EXTRACT_slot_getZslot_get \
+	CWISS_NOTHING, CWISS_NOTHING, CWISS_NOTHING
+#define CWISS_EXTRACT_slot_dtor(key_, val_) CWISS_EXTRACT_slot_dtorZ##key_
+#define CWISS_EXTRACT_slot_dtorZslot_dtor \
+	CWISS_NOTHING, CWISS_NOTHING, CWISS_NOTHING
+#define CWISS_EXTRACT_modifiers(key_, val_) CWISS_EXTRACT_modifiersZ##key_
+#define CWISS_EXTRACT_modifiersZmodifiers \
+	CWISS_NOTHING, CWISS_NOTHING, CWISS_NOTHING
+
+#define CWISS_EXTRACT00(needle_, kv_, ...)                                     \
+	CWISS_SELECT00(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT01, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT01(needle_, kv_, ...)                                     \
+	CWISS_SELECT01(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT02, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT02(needle_, kv_, ...)                                     \
+	CWISS_SELECT02(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT03, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT03(needle_, kv_, ...)                                     \
+	CWISS_SELECT03(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT04, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT04(needle_, kv_, ...)                                     \
+	CWISS_SELECT04(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT05, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT05(needle_, kv_, ...)                                     \
+	CWISS_SELECT05(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT06, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT06(needle_, kv_, ...)                                     \
+	CWISS_SELECT06(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT07, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT07(needle_, kv_, ...)                                     \
+	CWISS_SELECT07(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT08, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT08(needle_, kv_, ...)                                     \
+	CWISS_SELECT08(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT09, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT09(needle_, kv_, ...)                                     \
+	CWISS_SELECT09(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT0A, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT0A(needle_, kv_, ...)                                     \
+	CWISS_SELECT0A(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT0B, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT0B(needle_, kv_, ...)                                     \
+	CWISS_SELECT0B(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT0C, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT0C(needle_, kv_, ...)                                     \
+	CWISS_SELECT0C(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT0D, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT0D(needle_, kv_, ...)                                     \
+	CWISS_SELECT0D(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT0E, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT0E(needle_, kv_, ...)                                     \
+	CWISS_SELECT0E(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT0F, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT0F(needle_, kv_, ...)                                     \
+	CWISS_SELECT0F(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT10, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT10(needle_, kv_, ...)                                     \
+	CWISS_SELECT10(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT11, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT11(needle_, kv_, ...)                                     \
+	CWISS_SELECT11(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT12, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT12(needle_, kv_, ...)                                     \
+	CWISS_SELECT12(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT13, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT13(needle_, kv_, ...)                                     \
+	CWISS_SELECT13(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT14, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT14(needle_, kv_, ...)                                     \
+	CWISS_SELECT14(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT15, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT15(needle_, kv_, ...)                                     \
+	CWISS_SELECT15(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT16, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT16(needle_, kv_, ...)                                     \
+	CWISS_SELECT16(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT17, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT17(needle_, kv_, ...)                                     \
+	CWISS_SELECT17(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT18, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT18(needle_, kv_, ...)                                     \
+	CWISS_SELECT18(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT19, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT19(needle_, kv_, ...)                                     \
+	CWISS_SELECT19(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT1A, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT1A(needle_, kv_, ...)                                     \
+	CWISS_SELECT1A(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT1B, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT1B(needle_, kv_, ...)                                     \
+	CWISS_SELECT1B(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT1C, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT1C(needle_, kv_, ...)                                     \
+	CWISS_SELECT1C(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT1D, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT1D(needle_, kv_, ...)                                     \
+	CWISS_SELECT1D(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT1E, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT1E(needle_, kv_, ...)                                     \
+	CWISS_SELECT1E(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT1F, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT1F(needle_, kv_, ...)                                     \
+	CWISS_SELECT1F(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT20, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT20(needle_, kv_, ...)                                     \
+	CWISS_SELECT20(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT21, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT21(needle_, kv_, ...)                                     \
+	CWISS_SELECT21(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT22, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT22(needle_, kv_, ...)                                     \
+	CWISS_SELECT22(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT23, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT23(needle_, kv_, ...)                                     \
+	CWISS_SELECT23(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT24, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT24(needle_, kv_, ...)                                     \
+	CWISS_SELECT24(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT25, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT25(needle_, kv_, ...)                                     \
+	CWISS_SELECT25(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT26, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT26(needle_, kv_, ...)                                     \
+	CWISS_SELECT26(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT27, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT27(needle_, kv_, ...)                                     \
+	CWISS_SELECT27(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT28, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT28(needle_, kv_, ...)                                     \
+	CWISS_SELECT28(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT29, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT29(needle_, kv_, ...)                                     \
+	CWISS_SELECT29(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT2A, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT2A(needle_, kv_, ...)                                     \
+	CWISS_SELECT2A(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT2B, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT2B(needle_, kv_, ...)                                     \
+	CWISS_SELECT2B(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT2C, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT2C(needle_, kv_, ...)                                     \
+	CWISS_SELECT2C(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT2D, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT2D(needle_, kv_, ...)                                     \
+	CWISS_SELECT2D(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT2E, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT2E(needle_, kv_, ...)                                     \
+	CWISS_SELECT2E(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT2F, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT2F(needle_, kv_, ...)                                     \
+	CWISS_SELECT2F(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT30, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT30(needle_, kv_, ...)                                     \
+	CWISS_SELECT30(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT31, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT31(needle_, kv_, ...)                                     \
+	CWISS_SELECT31(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT32, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT32(needle_, kv_, ...)                                     \
+	CWISS_SELECT32(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT33, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT33(needle_, kv_, ...)                                     \
+	CWISS_SELECT33(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT34, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT34(needle_, kv_, ...)                                     \
+	CWISS_SELECT34(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT35, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT35(needle_, kv_, ...)                                     \
+	CWISS_SELECT35(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT36, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT36(needle_, kv_, ...)                                     \
+	CWISS_SELECT36(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT37, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT37(needle_, kv_, ...)                                     \
+	CWISS_SELECT37(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT38, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT38(needle_, kv_, ...)                                     \
+	CWISS_SELECT38(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT39, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT39(needle_, kv_, ...)                                     \
+	CWISS_SELECT39(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT3A, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT3A(needle_, kv_, ...)                                     \
+	CWISS_SELECT3A(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT3B, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT3B(needle_, kv_, ...)                                     \
+	CWISS_SELECT3B(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT3C, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT3C(needle_, kv_, ...)                                     \
+	CWISS_SELECT3C(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT3D, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT3D(needle_, kv_, ...)                                     \
+	CWISS_SELECT3D(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT3E, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT3E(needle_, kv_, ...)                                     \
+	CWISS_SELECT3E(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT3F, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+#define CWISS_EXTRACT3F(needle_, kv_, ...)                                     \
+	CWISS_SELECT3F(needle_ kv_, CWISS_EXTRACT_VALUE, kv_, CWISS_EXTRACT40, \
+		       (needle_, __VA_ARGS__), CWISS_NOTHING)
+
+#define CWISS_SELECT00(x_, ...) CWISS_SELECT00_(x_, __VA_ARGS__)
+#define CWISS_SELECT01(x_, ...) CWISS_SELECT01_(x_, __VA_ARGS__)
+#define CWISS_SELECT02(x_, ...) CWISS_SELECT02_(x_, __VA_ARGS__)
+#define CWISS_SELECT03(x_, ...) CWISS_SELECT03_(x_, __VA_ARGS__)
+#define CWISS_SELECT04(x_, ...) CWISS_SELECT04_(x_, __VA_ARGS__)
+#define CWISS_SELECT05(x_, ...) CWISS_SELECT05_(x_, __VA_ARGS__)
+#define CWISS_SELECT06(x_, ...) CWISS_SELECT06_(x_, __VA_ARGS__)
+#define CWISS_SELECT07(x_, ...) CWISS_SELECT07_(x_, __VA_ARGS__)
+#define CWISS_SELECT08(x_, ...) CWISS_SELECT08_(x_, __VA_ARGS__)
+#define CWISS_SELECT09(x_, ...) CWISS_SELECT09_(x_, __VA_ARGS__)
+#define CWISS_SELECT0A(x_, ...) CWISS_SELECT0A_(x_, __VA_ARGS__)
+#define CWISS_SELECT0B(x_, ...) CWISS_SELECT0B_(x_, __VA_ARGS__)
+#define CWISS_SELECT0C(x_, ...) CWISS_SELECT0C_(x_, __VA_ARGS__)
+#define CWISS_SELECT0D(x_, ...) CWISS_SELECT0D_(x_, __VA_ARGS__)
+#define CWISS_SELECT0E(x_, ...) CWISS_SELECT0E_(x_, __VA_ARGS__)
+#define CWISS_SELECT0F(x_, ...) CWISS_SELECT0F_(x_, __VA_ARGS__)
+#define CWISS_SELECT10(x_, ...) CWISS_SELECT10_(x_, __VA_ARGS__)
+#define CWISS_SELECT11(x_, ...) CWISS_SELECT11_(x_, __VA_ARGS__)
+#define CWISS_SELECT12(x_, ...) CWISS_SELECT12_(x_, __VA_ARGS__)
+#define CWISS_SELECT13(x_, ...) CWISS_SELECT13_(x_, __VA_ARGS__)
+#define CWISS_SELECT14(x_, ...) CWISS_SELECT14_(x_, __VA_ARGS__)
+#define CWISS_SELECT15(x_, ...) CWISS_SELECT15_(x_, __VA_ARGS__)
+#define CWISS_SELECT16(x_, ...) CWISS_SELECT16_(x_, __VA_ARGS__)
+#define CWISS_SELECT17(x_, ...) CWISS_SELECT17_(x_, __VA_ARGS__)
+#define CWISS_SELECT18(x_, ...) CWISS_SELECT18_(x_, __VA_ARGS__)
+#define CWISS_SELECT19(x_, ...) CWISS_SELECT19_(x_, __VA_ARGS__)
+#define CWISS_SELECT1A(x_, ...) CWISS_SELECT1A_(x_, __VA_ARGS__)
+#define CWISS_SELECT1B(x_, ...) CWISS_SELECT1B_(x_, __VA_ARGS__)
+#define CWISS_SELECT1C(x_, ...) CWISS_SELECT1C_(x_, __VA_ARGS__)
+#define CWISS_SELECT1D(x_, ...) CWISS_SELECT1D_(x_, __VA_ARGS__)
+#define CWISS_SELECT1E(x_, ...) CWISS_SELECT1E_(x_, __VA_ARGS__)
+#define CWISS_SELECT1F(x_, ...) CWISS_SELECT1F_(x_, __VA_ARGS__)
+#define CWISS_SELECT20(x_, ...) CWISS_SELECT20_(x_, __VA_ARGS__)
+#define CWISS_SELECT21(x_, ...) CWISS_SELECT21_(x_, __VA_ARGS__)
+#define CWISS_SELECT22(x_, ...) CWISS_SELECT22_(x_, __VA_ARGS__)
+#define CWISS_SELECT23(x_, ...) CWISS_SELECT23_(x_, __VA_ARGS__)
+#define CWISS_SELECT24(x_, ...) CWISS_SELECT24_(x_, __VA_ARGS__)
+#define CWISS_SELECT25(x_, ...) CWISS_SELECT25_(x_, __VA_ARGS__)
+#define CWISS_SELECT26(x_, ...) CWISS_SELECT26_(x_, __VA_ARGS__)
+#define CWISS_SELECT27(x_, ...) CWISS_SELECT27_(x_, __VA_ARGS__)
+#define CWISS_SELECT28(x_, ...) CWISS_SELECT28_(x_, __VA_ARGS__)
+#define CWISS_SELECT29(x_, ...) CWISS_SELECT29_(x_, __VA_ARGS__)
+#define CWISS_SELECT2A(x_, ...) CWISS_SELECT2A_(x_, __VA_ARGS__)
+#define CWISS_SELECT2B(x_, ...) CWISS_SELECT2B_(x_, __VA_ARGS__)
+#define CWISS_SELECT2C(x_, ...) CWISS_SELECT2C_(x_, __VA_ARGS__)
+#define CWISS_SELECT2D(x_, ...) CWISS_SELECT2D_(x_, __VA_ARGS__)
+#define CWISS_SELECT2E(x_, ...) CWISS_SELECT2E_(x_, __VA_ARGS__)
+#define CWISS_SELECT2F(x_, ...) CWISS_SELECT2F_(x_, __VA_ARGS__)
+#define CWISS_SELECT30(x_, ...) CWISS_SELECT30_(x_, __VA_ARGS__)
+#define CWISS_SELECT31(x_, ...) CWISS_SELECT31_(x_, __VA_ARGS__)
+#define CWISS_SELECT32(x_, ...) CWISS_SELECT32_(x_, __VA_ARGS__)
+#define CWISS_SELECT33(x_, ...) CWISS_SELECT33_(x_, __VA_ARGS__)
+#define CWISS_SELECT34(x_, ...) CWISS_SELECT34_(x_, __VA_ARGS__)
+#define CWISS_SELECT35(x_, ...) CWISS_SELECT35_(x_, __VA_ARGS__)
+#define CWISS_SELECT36(x_, ...) CWISS_SELECT36_(x_, __VA_ARGS__)
+#define CWISS_SELECT37(x_, ...) CWISS_SELECT37_(x_, __VA_ARGS__)
+#define CWISS_SELECT38(x_, ...) CWISS_SELECT38_(x_, __VA_ARGS__)
+#define CWISS_SELECT39(x_, ...) CWISS_SELECT39_(x_, __VA_ARGS__)
+#define CWISS_SELECT3A(x_, ...) CWISS_SELECT3A_(x_, __VA_ARGS__)
+#define CWISS_SELECT3B(x_, ...) CWISS_SELECT3B_(x_, __VA_ARGS__)
+#define CWISS_SELECT3C(x_, ...) CWISS_SELECT3C_(x_, __VA_ARGS__)
+#define CWISS_SELECT3D(x_, ...) CWISS_SELECT3D_(x_, __VA_ARGS__)
+#define CWISS_SELECT3E(x_, ...) CWISS_SELECT3E_(x_, __VA_ARGS__)
+#define CWISS_SELECT3F(x_, ...) CWISS_SELECT3F_(x_, __VA_ARGS__)
+
+#define CWISS_SELECT00_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT01_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT02_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT03_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT04_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT05_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT06_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT07_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT08_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT09_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT0A_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT0B_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT0C_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT0D_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT0E_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT0F_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT10_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT11_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT12_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT13_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT14_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT15_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT16_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT17_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT18_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT19_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT1A_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT1B_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT1C_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT1D_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT1E_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT1F_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT20_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT21_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT22_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT23_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT24_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT25_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT26_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT27_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT28_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT29_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT2A_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT2B_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT2C_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT2D_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT2E_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT2F_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT30_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT31_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT32_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT33_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT34_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT35_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT36_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT37_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT38_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT39_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT3A_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT3B_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT3C_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT3D_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT3E_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+#define CWISS_SELECT3F_(ignored_, _call_, _args_, call_, args_, ...) call_ args_
+/// cwisstable/internal/extract.h //////////////////////////////////////////////
+
+/// cwisstable/policy.h ////////////////////////////////////////////////////////
+/// Hash table policies.
+///
+/// Table policies are `cwisstable`'s generic code mechanism. All code in
+/// `cwisstable`'s internals is completely agnostic to:
+/// - The layout of the elements.
+/// - The storage strategy for the elements (inline, indirect in the heap).
+/// - Hashing, comparison, and allocation.
+///
+/// This information is provided to `cwisstable`'s internals by way of a
+/// *policy*: a vtable describing how to move elements around, hash them,
+/// compare them, allocate storage for them, and so on and on. This design is
+/// inspired by Abseil's equivalent, which is a template parameter used for
+/// sharing code between all the SwissTable-backed containers.
+///
+/// Unlike Abseil, policies are part of `cwisstable`'s public interface. Due to
+/// C's lack of any mechanism for detecting the gross properties of types,
+/// types with unwritten invariants, such as C strings (NUL-terminated byte
+/// arrays), users must be able to carefully describe to `cwisstable` how to
+/// correctly do things to their type. DESIGN.md goes into detailed rationale
+/// for this polymorphism strategy.
+///
+/// # Defining a Policy
+///
+/// Policies are defined as read-only globals and passed around by pointer to
+/// different `cwisstable` functions; macros are provided for doing this, since
+/// most of these functions will not vary significantly from one type to
+/// another. There are four of them:
+///
+/// - `CWISS_DECLARE_FLAT_SET_POLICY(kPolicy, Type, ...)`
+/// - `CWISS_DECLARE_FLAT_MAP_POLICY(kPolicy, Key, Value, ...)`
+/// - `CWISS_DECLARE_NODE_SET_POLICY(kPolicy, Type, ...)`
+/// - `CWISS_DECLARE_NODE_MAP_POLICY(kPolicy, Key, Value, ...)`
+///
+/// These correspond to the four SwissTable types in Abseil: two map types and
+/// two set types; "flat" means that elements are stored inline in the backing
+/// array, whereas "node" means that the element is stored in its own heap
+/// allocation, making it stable across rehashings (which SwissTable does more
+/// or less whenever it feels like it).
+///
+/// Each macro expands to a read-only global variable definition (with the name
+/// `kPolicy`, i.e, the first variable) dedicated for the specified type(s).
+/// The arguments that follow are overrides for the default values of each field
+/// in the policy; all but the size and alignment fields of `CWISS_ObjectPolicy`
+/// may be overridden. To override the field `kPolicy.foo.bar`, pass
+/// `(foo_bar, value)` to the macro. If multiple such pairs are passed in, the
+/// first one found wins. `examples/stringmap.c` provides an example of how to
+/// use this functionality.
+///
+/// For "common" uses, where the key and value are plain-old-data, `declare.h`
+/// has dedicated macros, and fussing with policies directly is unnecessary.
+
+CWISS_BEGIN
+CWISS_BEGIN_EXTERN
+
+/// A policy describing the basic laying properties of a type.
+///
+/// This type describes how to move values of a particular type around.
+typedef struct {
+	/// The layout of the stored object.
+	size_t size, align;
+
+	/// Performs a deep copy of `src` onto a fresh location `dst`.
+	void (*copy)(void *dst, const void *src);
+
+	/// Destroys an object.
+	///
+	/// This member may, as an optimization, be null. This will cause it to
+	/// behave as a no-op, and may be more efficient than making this an empty
+	/// function.
+	void (*dtor)(void *val);
+} CWISS_ObjectPolicy;
+
+/// A policy describing the hashing properties of a type.
+///
+/// This type describes the necessary information for putting a value into a
+/// hash table.
+///
+/// A *heterogenous* key policy is one whose equality function expects different
+/// argument types, which can be used for so-called heterogenous lookup: finding
+/// an element of a table by comparing it to a somewhat different type. If the
+/// table element is, for example, a `std::string`[1]-like type, it could still
+/// be found via a non-owning version like a `std::string_view`[2]. This is
+/// important for making efficient use of a SwissTable.
+///
+/// [1]: For non C++ programmers: a growable string type implemented as a
+///      `struct { char* ptr; size_t size, capacity; }`.
+/// [2]: Similarly, a `std::string_view` is a pointer-length pair to a string
+///      *somewhere*; unlike a C-style string, it might be a substring of a
+///      larger allocation elsewhere.
+typedef struct {
+	/// Computes the hash of a value.
+	///
+	/// This function must be such that if two elements compare equal, they must
+	/// have the same hash (but not vice-versa).
+	///
+	/// If this policy is heterogenous, this function must be defined so that
+	/// given the original key policy of the table's element type, if
+	/// `hetero->eq(a, b)` holds, then `hetero->hash(a) == original->hash(b)`.
+	/// In other words, the obvious condition for a hash table to work correctly
+	/// with this policy.
+	size_t (*hash)(const void *val);
+
+	/// Compares two values for equality.
+	///
+	/// This function is actually not symmetric: the first argument will always be
+	/// the value being searched for, and the second will be a pointer to the
+	/// candidate entry. In particular, this means they can be different types:
+	/// in C++ parlance, `needle` could be a `std::string_view`, while `candidate`
+	/// could be a `std::string`.
+	bool (*eq)(const void *needle, const void *candidate);
+} CWISS_KeyPolicy;
+
+/// A policy for allocation.
+///
+/// This type provides access to a custom allocator.
+typedef struct {
+	/// Allocates memory.
+	///
+	/// This function must never fail and never return null, unlike `malloc`. This
+	/// function does not need to tolerate zero sized allocations.
+	void *(*alloc)(size_t size, size_t align);
+
+	/// Deallocates memory allocated by `alloc`.
+	///
+	/// This function is passed the same size/alignment as was passed to `alloc`,
+	/// allowing for sized-delete optimizations.
+	void (*free)(void *array, size_t size, size_t align);
+} CWISS_AllocPolicy;
+
+/// A policy for allocating space for slots.
+///
+/// This allows us to distinguish between inline storage (more cache-friendly)
+/// and outline (pointer-stable).
+typedef struct {
+	/// The layout of a slot value.
+	///
+	/// Usually, this will be the same as for the object type, *or* the layout
+	/// of a pointer (for outline storage).
+	size_t size, align;
+
+	/// Initializes a new slot at the given location.
+	///
+	/// This function does not initialize the value *in* the slot; it simply sets
+	/// up the slot so that a value can be `memcpy`'d or otherwise emplaced into
+	/// the slot.
+	void (*init)(void *slot);
+
+	/// Destroys a slot, including the destruction of the value it contains.
+	///
+	/// This function may, as an optimization, be null. This will cause it to
+	/// behave as a no-op.
+	void (*del)(void *slot);
+
+	/// Transfers a slot.
+	///
+	/// `dst` must be uninitialized; `src` must be initialized. After this
+	/// function, their roles will be switched: `dst` will be initialized and
+	/// contain the value from `src`; `src` will be initialized.
+	///
+	/// This function need not actually copy the underlying value.
+	void (*transfer)(void *dst, void *src);
+
+	/// Extracts a pointer to the value inside the a slot.
+	///
+	/// This function does not need to tolerate nulls.
+	void *(*get)(void *slot);
+} CWISS_SlotPolicy;
+
+/// A hash table policy.
+///
+/// See the header documentation for more information.
+typedef struct {
+	const CWISS_ObjectPolicy *obj;
+	const CWISS_KeyPolicy *key;
+	const CWISS_AllocPolicy *alloc;
+	const CWISS_SlotPolicy *slot;
+} CWISS_Policy;
+
+/// Declares a hash set policy with inline storage for the given type.
+///
+/// See the header documentation for more information.
+#define CWISS_DECLARE_FLAT_SET_POLICY(kPolicy_, Type_, ...) \
+	CWISS_DECLARE_POLICY_(kPolicy_, Type_, Type_, __VA_ARGS__)
+
+/// Declares a hash map policy with inline storage for the given key and value
+/// types.
+///
+/// See the header documentation for more information.
+#define CWISS_DECLARE_FLAT_MAP_POLICY(kPolicy_, K_, V_, ...) \
+	typedef struct {                                     \
+		K_ k;                                        \
+		V_ v;                                        \
+	} kPolicy_##_Entry;                                  \
+	CWISS_DECLARE_POLICY_(kPolicy_, kPolicy_##_Entry, K_, __VA_ARGS__)
+
+/// Declares a hash set policy with pointer-stable storage for the given type.
+///
+/// See the header documentation for more information.
+#define CWISS_DECLARE_NODE_SET_POLICY(kPolicy_, Type_, ...)                \
+	CWISS_DECLARE_NODE_FUNCTIONS_(kPolicy_, Type_, Type_, __VA_ARGS__) \
+	CWISS_DECLARE_POLICY_(kPolicy_, Type_, Type_, __VA_ARGS__,         \
+			      CWISS_NODE_OVERRIDES_(kPolicy_))
+
+/// Declares a hash map policy with pointer-stable storage for the given key and
+/// value types.
+///
+/// See the header documentation for more information.
+#define CWISS_DECLARE_NODE_MAP_POLICY(kPolicy_, K_, V_, ...)               \
+	typedef struct {                                                   \
+		K_ k;                                                      \
+		V_ v;                                                      \
+	} kPolicy_##_Entry;                                                \
+	CWISS_DECLARE_NODE_FUNCTIONS_(kPolicy_, kPolicy_##_Entry, K_,      \
+				      __VA_ARGS__)                         \
+	CWISS_DECLARE_POLICY_(kPolicy_, kPolicy_##_Entry, K_, __VA_ARGS__, \
+			      CWISS_NODE_OVERRIDES_(kPolicy_))
+
+// ---- PUBLIC API ENDS HERE! ----
+
+#define CWISS_DECLARE_POLICY_(kPolicy_, Type_, Key_, ...)                     \
+	CWISS_BEGIN                                                           \
+	CWISS_EXTRACT_RAW(modifiers, static, __VA_ARGS__)                     \
+	inline void kPolicy_##_DefaultCopy(void *dst, const void *src)        \
+	{                                                                     \
+		memcpy(dst, src, sizeof(Type_));                              \
+	}                                                                     \
+	CWISS_EXTRACT_RAW(modifiers, static, __VA_ARGS__)                     \
+	inline size_t kPolicy_##_DefaultHash(const void *val)                 \
+	{                                                                     \
+		CWISS_AbslHash_State state = CWISS_AbslHash_kInit;            \
+		CWISS_AbslHash_Write(&state, val, sizeof(Key_));              \
+		return CWISS_AbslHash_Finish(state);                          \
+	}                                                                     \
+	CWISS_EXTRACT_RAW(modifiers, static, __VA_ARGS__)                     \
+	inline bool kPolicy_##_DefaultEq(const void *a, const void *b)        \
+	{                                                                     \
+		return memcmp(a, b, sizeof(Key_)) == 0;                       \
+	}                                                                     \
+	CWISS_EXTRACT_RAW(modifiers, static, __VA_ARGS__)                     \
+	inline void kPolicy_##_DefaultSlotInit(void *slot)                    \
+	{                                                                     \
+	}                                                                     \
+	CWISS_EXTRACT_RAW(modifiers, static, __VA_ARGS__)                     \
+	inline void kPolicy_##_DefaultSlotTransfer(void *dst, void *src)      \
+	{                                                                     \
+		memcpy(dst, src, sizeof(Type_));                              \
+	}                                                                     \
+	CWISS_EXTRACT_RAW(modifiers, static, __VA_ARGS__)                     \
+	inline void *kPolicy_##_DefaultSlotGet(void *slot)                    \
+	{                                                                     \
+		return slot;                                                  \
+	}                                                                     \
+	CWISS_EXTRACT_RAW(modifiers, static, __VA_ARGS__)                     \
+	inline void kPolicy_##_DefaultSlotDtor(void *slot)                    \
+	{                                                                     \
+		if (CWISS_EXTRACT(obj_dtor, NULL, __VA_ARGS__) != NULL) {     \
+			CWISS_EXTRACT(obj_dtor, (void (*)(void *))NULL,       \
+				      __VA_ARGS__)                            \
+			(slot);                                               \
+		}                                                             \
+	}                                                                     \
+                                                                              \
+	CWISS_EXTRACT_RAW(modifiers, static, __VA_ARGS__)                     \
+	const CWISS_ObjectPolicy kPolicy_##_ObjectPolicy = {                  \
+		sizeof(Type_),                                                \
+		alignof(Type_),                                               \
+		CWISS_EXTRACT(obj_copy, kPolicy_##_DefaultCopy, __VA_ARGS__), \
+		CWISS_EXTRACT(obj_dtor, NULL, __VA_ARGS__),                   \
+	};                                                                    \
+	CWISS_EXTRACT_RAW(modifiers, static, __VA_ARGS__)                     \
+	const CWISS_KeyPolicy kPolicy_##_KeyPolicy = {                        \
+		CWISS_EXTRACT(key_hash, kPolicy_##_DefaultHash, __VA_ARGS__), \
+		CWISS_EXTRACT(key_eq, kPolicy_##_DefaultEq, __VA_ARGS__),     \
+	};                                                                    \
+	CWISS_EXTRACT_RAW(modifiers, static, __VA_ARGS__)                     \
+	const CWISS_AllocPolicy kPolicy_##_AllocPolicy = {                    \
+		CWISS_EXTRACT(alloc_alloc, CWISS_DefaultMalloc, __VA_ARGS__), \
+		CWISS_EXTRACT(alloc_free, CWISS_DefaultFree, __VA_ARGS__),    \
+	};                                                                    \
+	CWISS_EXTRACT_RAW(modifiers, static, __VA_ARGS__)                     \
+	const CWISS_SlotPolicy kPolicy_##_SlotPolicy = {                      \
+		CWISS_EXTRACT(slot_size, sizeof(Type_), __VA_ARGS__),         \
+		CWISS_EXTRACT(slot_align, sizeof(Type_), __VA_ARGS__),        \
+		CWISS_EXTRACT(slot_init, kPolicy_##_DefaultSlotInit,          \
+			      __VA_ARGS__),                                   \
+		CWISS_EXTRACT(slot_dtor, kPolicy_##_DefaultSlotDtor,          \
+			      __VA_ARGS__),                                   \
+		CWISS_EXTRACT(slot_transfer, kPolicy_##_DefaultSlotTransfer,  \
+			      __VA_ARGS__),                                   \
+		CWISS_EXTRACT(slot_get, kPolicy_##_DefaultSlotGet,            \
+			      __VA_ARGS__),                                   \
+	};                                                                    \
+	CWISS_END                                                             \
+	CWISS_EXTRACT_RAW(modifiers, static, __VA_ARGS__)                     \
+	const CWISS_Policy kPolicy_ = {                                       \
+		&kPolicy_##_ObjectPolicy,                                     \
+		&kPolicy_##_KeyPolicy,                                        \
+		&kPolicy_##_AllocPolicy,                                      \
+		&kPolicy_##_SlotPolicy,                                       \
+	}
+
+#define CWISS_DECLARE_NODE_FUNCTIONS_(kPolicy_, Type_, ...)                  \
+	CWISS_BEGIN                                                          \
+	static inline void kPolicy_##_NodeSlotInit(void *slot)               \
+	{                                                                    \
+		void *node = CWISS_EXTRACT(alloc_alloc, CWISS_DefaultMalloc, \
+					   __VA_ARGS__)(sizeof(Type_),       \
+							alignof(Type_));     \
+		memcpy(slot, &node, sizeof(node));                           \
+	}                                                                    \
+	static inline void kPolicy_##_NodeSlotDtor(void *slot)               \
+	{                                                                    \
+		if (CWISS_EXTRACT(obj_dtor, NULL, __VA_ARGS__) != NULL) {    \
+			CWISS_EXTRACT(obj_dtor, (void (*)(void *))NULL,      \
+				      __VA_ARGS__)                           \
+			(*(void **)slot);                                    \
+		}                                                            \
+		CWISS_EXTRACT(alloc_free, CWISS_DefaultFree, __VA_ARGS__)    \
+		(*(void **)slot, sizeof(Type_), alignof(Type_));             \
+	}                                                                    \
+	static inline void kPolicy_##_NodeSlotTransfer(void *dst, void *src) \
+	{                                                                    \
+		memcpy(dst, src, sizeof(void *));                            \
+	}                                                                    \
+	static inline void *kPolicy_##_NodeSlotGet(void *slot)               \
+	{                                                                    \
+		return *((void **)slot);                                     \
+	}                                                                    \
+	CWISS_END
+
+#define CWISS_NODE_OVERRIDES_(kPolicy_)                             \
+	(slot_size, sizeof(void *)), (slot_align, alignof(void *)), \
+		(slot_init, kPolicy_##_NodeSlotInit),               \
+		(slot_dtor, kPolicy_##_NodeSlotDtor),               \
+		(slot_transfer, kPolicy_##_NodeSlotTransfer),       \
+		(slot_get, kPolicy_##_NodeSlotGet)
+
+static inline void *CWISS_DefaultMalloc(size_t size, size_t align)
+{
+	void *p = kvmalloc(size, GFP_KERNEL); // TODO: Check alignment.
+	CWISS_CHECK(p != NULL, "malloc() returned null");
+	return p;
+}
+static inline void CWISS_DefaultFree(void *array, size_t size, size_t align)
+{
+	kvfree(array);
+}
+
+CWISS_END_EXTERN
+CWISS_END
+/// cwisstable/policy.h ////////////////////////////////////////////////////////
+
+/// cwisstable/internal/raw_table.h ////////////////////////////////////////////
+/// The SwissTable implementation.
+///
+/// `CWISS_RawTable` is the core data structure that all SwissTables wrap.
+///
+/// All functions in this header take a `const CWISS_Policy*`, which describes
+/// how to manipulate the elements in a table. The same pointer (i.e., same
+/// address and provenance) passed to the function that created the
+/// `CWISS_RawTable` MUST be passed to all subsequent function calls, and it
+/// must not be mutated at any point between those calls. Failure to adhere to
+/// these requirements is UB.
+///
+/// It is STRONGLY recommended that this pointer point to a const global.
+
+CWISS_BEGIN
+CWISS_BEGIN_EXTERN
+
+/// A SwissTable.
+///
+/// This is absl::container_internal::raw_hash_set in Abseil.
+typedef struct {
+	/// The control bytes (and, also, a pointer to the base of the backing array).
+	///
+	/// This contains `capacity_ + 1 + CWISS_NumClonedBytes()` entries.
+	CWISS_ControlByte *ctrl_;
+	/// The beginning of the slots, located at `CWISS_SlotOffset()` bytes after
+	/// `ctrl_`. May be null for empty tables.
+	char *slots_;
+	/// The number of filled slots.
+	size_t size_;
+	/// The total number of available slots.
+	size_t capacity_;
+	/// The number of slots we can still fill before a rehash. See
+	/// `CWISS_CapacityToGrowth()`.
+	size_t growth_left_;
+} CWISS_RawTable;
+
+/// Prints full details about the internal state of `self` to `stderr`.
+static inline void CWISS_RawTable_dump(const CWISS_Policy *policy,
+				       const CWISS_RawTable *self)
+{
+	pr_err("ptr: %p, len: %zu, cap: %zu, growth: %zu\n", self->ctrl_,
+	       self->size_, self->capacity_, self->growth_left_);
+	if (self->capacity_ == 0) {
+		return;
+	}
+
+	size_t ctrl_bytes = self->capacity_ + CWISS_NumClonedBytes();
+	for (size_t i = 0; i <= ctrl_bytes; ++i) {
+		pr_err("[%4zu] %p / ", i, &self->ctrl_[i]);
+		switch (self->ctrl_[i]) {
+		case CWISS_kSentinel:
+			pr_cont("kSentinel: //\n");
+			continue;
+		case CWISS_kEmpty:
+			pr_cont("   kEmpty");
+			break;
+		case CWISS_kDeleted:
+			pr_cont(" kDeleted");
+			break;
+		default:
+			pr_cont(" H2(0x%02x)", self->ctrl_[i]);
+			break;
+		}
+
+		if (i >= self->capacity_) {
+			pr_cont(": <mirrored>\n");
+			continue;
+		}
+
+		char *slot = self->slots_ + i * policy->slot->size;
+		pr_cont(": %p /", slot);
+		for (size_t j = 0; j < policy->slot->size; ++j) {
+			pr_cont(" %02x", (unsigned char)slot[j]);
+		}
+		char *elem = (char *)policy->slot->get(slot);
+		if (elem != slot && CWISS_IsFull(self->ctrl_[i])) {
+			pr_cont(" ->");
+			for (size_t j = 0; j < policy->obj->size; ++j) {
+				pr_cont(" %02x", (unsigned char)elem[j]);
+			}
+		}
+		pr_cont("\n");
+	}
+}
+
+/// An iterator into a SwissTable.
+///
+/// Unlike a C++ iterator, there is no "end" to compare to. Instead,
+/// `CWISS_RawIter_get()` will yield a null pointer once the iterator is
+/// exhausted.
+///
+/// Invariants:
+/// - `ctrl_` and `slot_` are always in sync (i.e., the pointed to control byte
+///   corresponds to the pointed to slot), or both are null. `set_` may be null
+///   in the latter case.
+/// - `ctrl_` always points to a full slot.
+typedef struct {
+	CWISS_RawTable *set_;
+	CWISS_ControlByte *ctrl_;
+	char *slot_;
+} CWISS_RawIter;
+
+/// Fixes up `ctrl_` to point to a full by advancing it and `slot_` until they
+/// reach one.
+///
+/// If a sentinel is reached, we null both of them out instead.
+static inline void CWISS_RawIter_SkipEmptyOrDeleted(const CWISS_Policy *policy,
+						    CWISS_RawIter *self)
+{
+	while (CWISS_IsEmptyOrDeleted(*self->ctrl_)) {
+		CWISS_Group g = CWISS_Group_new(self->ctrl_);
+		uint32_t shift = CWISS_Group_CountLeadingEmptyOrDeleted(&g);
+		self->ctrl_ += shift;
+		self->slot_ += shift * policy->slot->size;
+	}
+
+	// Not sure why this is a branch rather than a cmov; Abseil uses a branch.
+	if (CWISS_UNLIKELY(*self->ctrl_ == CWISS_kSentinel)) {
+		self->ctrl_ = NULL;
+		self->slot_ = NULL;
+	}
+}
+
+/// Creates a valid iterator starting at the `index`th slot.
+static inline CWISS_RawIter CWISS_RawTable_iter_at(const CWISS_Policy *policy,
+						   CWISS_RawTable *self,
+						   size_t index)
+{
+	CWISS_RawIter iter = {
+		self,
+		self->ctrl_ + index,
+		self->slots_ + index * policy->slot->size,
+	};
+	CWISS_RawIter_SkipEmptyOrDeleted(policy, &iter);
+	CWISS_AssertIsValid(iter.ctrl_);
+	return iter;
+}
+
+/// Creates an iterator for `self`.
+static inline CWISS_RawIter CWISS_RawTable_iter(const CWISS_Policy *policy,
+						CWISS_RawTable *self)
+{
+	return CWISS_RawTable_iter_at(policy, self, 0);
+}
+
+/// Creates a valid iterator starting at the `index`th slot, accepting a `const`
+/// pointer instead.
+static inline CWISS_RawIter CWISS_RawTable_citer_at(const CWISS_Policy *policy,
+						    const CWISS_RawTable *self,
+						    size_t index)
+{
+	return CWISS_RawTable_iter_at(policy, (CWISS_RawTable *)self, index);
+}
+
+/// Creates an iterator for `self`, accepting a `const` pointer instead.
+static inline CWISS_RawIter CWISS_RawTable_citer(const CWISS_Policy *policy,
+						 const CWISS_RawTable *self)
+{
+	return CWISS_RawTable_iter(policy, (CWISS_RawTable *)self);
+}
+
+/// Returns a pointer into the currently pointed-to slot (*not* to the slot
+/// itself, but rather its contents).
+///
+/// Returns null if the iterator has been exhausted.
+static inline void *CWISS_RawIter_get(const CWISS_Policy *policy,
+				      const CWISS_RawIter *self)
+{
+	CWISS_AssertIsValid(self->ctrl_);
+	if (self->slot_ == NULL) {
+		return NULL;
+	}
+
+	return policy->slot->get(self->slot_);
+}
+
+/// Advances the iterator and returns the result of `CWISS_RawIter_get()`.
+///
+/// Calling on an empty iterator is UB.
+static inline void *CWISS_RawIter_next(const CWISS_Policy *policy,
+				       CWISS_RawIter *self)
+{
+	CWISS_AssertIsFull(self->ctrl_);
+	++self->ctrl_;
+	self->slot_ += policy->slot->size;
+
+	CWISS_RawIter_SkipEmptyOrDeleted(policy, self);
+	return CWISS_RawIter_get(policy, self);
+}
+
+/// Erases, but does not destroy, the value pointed to by `it`.
+static inline void CWISS_RawTable_EraseMetaOnly(const CWISS_Policy *policy,
+						CWISS_RawIter it)
+{
+	CWISS_DCHECK(CWISS_IsFull(*it.ctrl_), "erasing a dangling iterator");
+	--it.set_->size_;
+	const size_t index = (size_t)(it.ctrl_ - it.set_->ctrl_);
+	const size_t index_before = (index - CWISS_Group_kWidth) &
+				    it.set_->capacity_;
+	CWISS_Group g_after = CWISS_Group_new(it.ctrl_);
+	CWISS_BitMask empty_after = CWISS_Group_MatchEmpty(&g_after);
+	CWISS_Group g_before = CWISS_Group_new(it.set_->ctrl_ + index_before);
+	CWISS_BitMask empty_before = CWISS_Group_MatchEmpty(&g_before);
+
+	// We count how many consecutive non empties we have to the right and to the
+	// left of `it`. If the sum is >= kWidth then there is at least one probe
+	// window that might have seen a full group.
+	bool was_never_full =
+		empty_before.mask && empty_after.mask &&
+		(size_t)(CWISS_BitMask_TrailingZeros(&empty_after) +
+			 CWISS_BitMask_LeadingZeros(&empty_before)) <
+			CWISS_Group_kWidth;
+
+	CWISS_SetCtrl(index, was_never_full ? CWISS_kEmpty : CWISS_kDeleted,
+		      it.set_->capacity_, it.set_->ctrl_, it.set_->slots_,
+		      policy->slot->size);
+	it.set_->growth_left_ += was_never_full;
+	// infoz().RecordErase();
+}
+
+/// Computes a lower bound for the expected available growth and applies it to
+/// `self_`.
+static inline void CWISS_RawTable_ResetGrowthLeft(const CWISS_Policy *policy,
+						  CWISS_RawTable *self)
+{
+	self->growth_left_ =
+		CWISS_CapacityToGrowth(self->capacity_) - self->size_;
+}
+
+/// Allocates a backing array for `self` and initializes its control bits. This
+/// reads `capacity_` and updates all other fields based on the result of the
+/// allocation.
+///
+/// This does not free the currently held array; `capacity_` must be nonzero.
+static inline void CWISS_RawTable_InitializeSlots(const CWISS_Policy *policy,
+						  CWISS_RawTable *self)
+{
+	CWISS_DCHECK(self->capacity_, "capacity should be nonzero");
+	// Folks with custom allocators often make unwarranted assumptions about the
+	// behavior of their classes vis-a-vis trivial destructability and what
+	// calls they will or wont make.  Avoid sampling for people with custom
+	// allocators to get us out of this mess.  This is not a hard guarantee but
+	// a workaround while we plan the exact guarantee we want to provide.
+	//
+	// People are often sloppy with the exact type of their allocator (sometimes
+	// it has an extra const or is missing the pair, but rebinds made it work
+	// anyway).  To avoid the ambiguity, we work off SlotAlloc which we have
+	// bound more carefully.
+	//
+	// NOTE(mcyoung): Not relevant in C but kept in case we decide to do custom
+	// alloc.
+	// if (std::is_same<SlotAlloc, std::allocator<slot_type>>::value && slots_ == nullptr) {
+	//	infoz() = Sample(sizeof(slot_type));
+	// }
+
+	char *mem = (char *) // Cast for C++.
+		    policy->alloc->alloc(CWISS_AllocSize(self->capacity_,
+							 policy->slot->size,
+							 policy->slot->align),
+					 policy->slot->align);
+
+	self->ctrl_ = (CWISS_ControlByte *)mem;
+	self->slots_ =
+		mem + CWISS_SlotOffset(self->capacity_, policy->slot->align);
+	CWISS_ResetCtrl(self->capacity_, self->ctrl_, self->slots_,
+			policy->slot->size);
+	CWISS_RawTable_ResetGrowthLeft(policy, self);
+
+	// infoz().RecordStorageChanged(size_, capacity_);
+}
+
+/// Destroys all slots in the backing array, frees the backing array, and clears
+/// all top-level book-keeping data.
+static inline void CWISS_RawTable_DestroySlots(const CWISS_Policy *policy,
+					       CWISS_RawTable *self)
+{
+	if (!self->capacity_)
+		return;
+
+	if (policy->slot->del != NULL) {
+		for (size_t i = 0; i != self->capacity_; ++i) {
+			if (CWISS_IsFull(self->ctrl_[i])) {
+				policy->slot->del(self->slots_ +
+						  i * policy->slot->size);
+			}
+		}
+	}
+
+	policy->alloc->free(self->ctrl_,
+			    CWISS_AllocSize(self->capacity_, policy->slot->size,
+					    policy->slot->align),
+			    policy->slot->align);
+	self->ctrl_ = CWISS_EmptyGroup();
+	self->slots_ = NULL;
+	self->size_ = 0;
+	self->capacity_ = 0;
+	self->growth_left_ = 0;
+}
+
+/// Grows the table to the given capacity, triggering a rehash.
+static inline void CWISS_RawTable_Resize(const CWISS_Policy *policy,
+					 CWISS_RawTable *self,
+					 size_t new_capacity)
+{
+	CWISS_DCHECK(CWISS_IsValidCapacity(new_capacity),
+		     "invalid capacity: %zu", new_capacity);
+
+	CWISS_ControlByte *old_ctrl = self->ctrl_;
+	char *old_slots = self->slots_;
+	const size_t old_capacity = self->capacity_;
+	self->capacity_ = new_capacity;
+	CWISS_RawTable_InitializeSlots(policy, self);
+
+	size_t total_probe_length = 0;
+	for (size_t i = 0; i != old_capacity; ++i) {
+		if (CWISS_IsFull(old_ctrl[i])) {
+			size_t hash = policy->key->hash(policy->slot->get(
+				old_slots + i * policy->slot->size));
+			CWISS_FindInfo target = CWISS_FindFirstNonFull(
+				self->ctrl_, hash, self->capacity_);
+			size_t new_i = target.offset;
+			total_probe_length += target.probe_length;
+			CWISS_SetCtrl(new_i, CWISS_H2(hash), self->capacity_,
+				      self->ctrl_, self->slots_,
+				      policy->slot->size);
+			policy->slot->transfer(
+				self->slots_ + new_i * policy->slot->size,
+				old_slots + i * policy->slot->size);
+		}
+	}
+	if (old_capacity) {
+		CWISS_UnpoisonMemory(old_slots,
+				     policy->slot->size * old_capacity);
+		policy->alloc->free(old_ctrl,
+				    CWISS_AllocSize(old_capacity,
+						    policy->slot->size,
+						    policy->slot->align),
+				    policy->slot->align);
+	}
+	// infoz().RecordRehash(total_probe_length);
+}
+
+/// Prunes control bits to remove as many tombstones as possible.
+///
+/// See the comment on `CWISS_RawTable_rehash_and_grow_if_necessary()`.
+CWISS_INLINE_NEVER
+static void CWISS_RawTable_DropDeletesWithoutResize(const CWISS_Policy *policy,
+						    CWISS_RawTable *self)
+{
+	CWISS_DCHECK(CWISS_IsValidCapacity(self->capacity_),
+		     "invalid capacity: %zu", self->capacity_);
+	CWISS_DCHECK(!CWISS_IsSmall(self->capacity_),
+		     "unexpected small capacity: %zu", self->capacity_);
+	// Algorithm:
+	// - mark all DELETED slots as EMPTY
+	// - mark all FULL slots as DELETED
+	// - for each slot marked as DELETED
+	//     hash = Hash(element)
+	//     target = find_first_non_full(hash)
+	//     if target is in the same group
+	//       mark slot as FULL
+	//     else if target is EMPTY
+	//       transfer element to target
+	//       mark slot as EMPTY
+	//       mark target as FULL
+	//     else if target is DELETED
+	//       swap current element with target element
+	//       mark target as FULL
+	//       repeat procedure for current slot with moved from element (target)
+	CWISS_ConvertDeletedToEmptyAndFullToDeleted(self->ctrl_,
+						    self->capacity_);
+	size_t total_probe_length = 0;
+	// Unfortunately because we do not know this size statically, we need to take
+	// a trip to the allocator. Alternatively we could use a variable length
+	// alloca...
+	void *slot =
+		policy->alloc->alloc(policy->slot->size, policy->slot->align);
+
+	for (size_t i = 0; i != self->capacity_; ++i) {
+		if (!CWISS_IsDeleted(self->ctrl_[i]))
+			continue;
+
+		char *old_slot = self->slots_ + i * policy->slot->size;
+		size_t hash = policy->key->hash(policy->slot->get(old_slot));
+
+		const CWISS_FindInfo target = CWISS_FindFirstNonFull(
+			self->ctrl_, hash, self->capacity_);
+		const size_t new_i = target.offset;
+		total_probe_length += target.probe_length;
+
+		char *new_slot = self->slots_ + new_i * policy->slot->size;
+
+		// Verify if the old and new i fall within the same group wrt the hash.
+		// If they do, we don't need to move the object as it falls already in the
+		// best probe we can.
+		const size_t probe_offset =
+			CWISS_ProbeSeq_Start(self->ctrl_, hash, self->capacity_)
+				.offset_;
+#define CWISS_ProbeIndex(pos_) \
+	(((pos_ - probe_offset) & self->capacity_) / CWISS_Group_kWidth)
+
+		// Element doesn't move.
+		if (CWISS_LIKELY(CWISS_ProbeIndex(new_i) ==
+				 CWISS_ProbeIndex(i))) {
+			CWISS_SetCtrl(i, CWISS_H2(hash), self->capacity_,
+				      self->ctrl_, self->slots_,
+				      policy->slot->size);
+			continue;
+		}
+		if (CWISS_IsEmpty(self->ctrl_[new_i])) {
+			// Transfer element to the empty spot.
+			// SetCtrl poisons/unpoisons the slots so we have to call it at the
+			// right time.
+			CWISS_SetCtrl(new_i, CWISS_H2(hash), self->capacity_,
+				      self->ctrl_, self->slots_,
+				      policy->slot->size);
+			policy->slot->transfer(new_slot, old_slot);
+			CWISS_SetCtrl(i, CWISS_kEmpty, self->capacity_,
+				      self->ctrl_, self->slots_,
+				      policy->slot->size);
+		} else {
+			CWISS_DCHECK(CWISS_IsDeleted(self->ctrl_[new_i]),
+				     "bad ctrl value at %zu: %02x", new_i,
+				     self->ctrl_[new_i]);
+			CWISS_SetCtrl(new_i, CWISS_H2(hash), self->capacity_,
+				      self->ctrl_, self->slots_,
+				      policy->slot->size);
+			// Until we are done rehashing, DELETED marks previously FULL slots.
+			// Swap i and new_i elements.
+
+			policy->slot->transfer(slot, old_slot);
+			policy->slot->transfer(old_slot, new_slot);
+			policy->slot->transfer(new_slot, slot);
+			--i; // repeat
+		}
+#undef CWISS_ProbeSeq_Start_index
+	}
+	CWISS_RawTable_ResetGrowthLeft(policy, self);
+	policy->alloc->free(slot, policy->slot->size, policy->slot->align);
+	// infoz().RecordRehash(total_probe_length);
+}
+
+/// Called whenever the table *might* need to conditionally grow.
+///
+/// This function is an optimization opportunity to perform a rehash even when
+/// growth is unnecessary, because vacating tombstones is beneficial for
+/// performance in the long-run.
+static inline void
+CWISS_RawTable_rehash_and_grow_if_necessary(const CWISS_Policy *policy,
+					    CWISS_RawTable *self)
+{
+	if (self->capacity_ == 0) {
+		CWISS_RawTable_Resize(policy, self, 1);
+	} else if (self->capacity_ > CWISS_Group_kWidth &&
+		   // Do these calculations in 64-bit to avoid overflow.
+		   self->size_ * UINT64_C(32) <=
+			   self->capacity_ * UINT64_C(25)) {
+		// Squash DELETED without growing if there is enough capacity.
+		//
+		// Rehash in place if the current size is <= 25/32 of capacity_.
+		// Rationale for such a high factor: 1) drop_deletes_without_resize() is
+		// faster than resize, and 2) it takes quite a bit of work to add
+		// tombstones.  In the worst case, seems to take approximately 4
+		// insert/erase pairs to create a single tombstone and so if we are
+		// rehashing because of tombstones, we can afford to rehash-in-place as
+		// long as we are reclaiming at least 1/8 the capacity without doing more
+		// than 2X the work.  (Where "work" is defined to be size() for rehashing
+		// or rehashing in place, and 1 for an insert or erase.)  But rehashing in
+		// place is faster per operation than inserting or even doubling the size
+		// of the table, so we actually afford to reclaim even less space from a
+		// resize-in-place.  The decision is to rehash in place if we can reclaim
+		// at about 1/8th of the usable capacity (specifically 3/28 of the
+		// capacity) which means that the total cost of rehashing will be a small
+		// fraction of the total work.
+		//
+		// Here is output of an experiment using the BM_CacheInSteadyState
+		// benchmark running the old case (where we rehash-in-place only if we can
+		// reclaim at least 7/16*capacity_) vs. this code (which rehashes in place
+		// if we can recover 3/32*capacity_).
+		//
+		// Note that although in the worst-case number of rehashes jumped up from
+		// 15 to 190, but the number of operations per second is almost the same.
+		//
+		// Abridged output of running BM_CacheInSteadyState benchmark from
+		// raw_hash_set_benchmark.   N is the number of insert/erase operations.
+		//
+		//      | OLD (recover >= 7/16        | NEW (recover >= 3/32)
+		// size |    N/s LoadFactor NRehashes |    N/s LoadFactor NRehashes
+		//  448 | 145284       0.44        18 | 140118       0.44        19
+		//  493 | 152546       0.24        11 | 151417       0.48        28
+		//  538 | 151439       0.26        11 | 151152       0.53        38
+		//  583 | 151765       0.28        11 | 150572       0.57        50
+		//  628 | 150241       0.31        11 | 150853       0.61        66
+		//  672 | 149602       0.33        12 | 150110       0.66        90
+		//  717 | 149998       0.35        12 | 149531       0.70       129
+		//  762 | 149836       0.37        13 | 148559       0.74       190
+		//  807 | 149736       0.39        14 | 151107       0.39        14
+		//  852 | 150204       0.42        15 | 151019       0.42        15
+		CWISS_RawTable_DropDeletesWithoutResize(policy, self);
+	} else {
+		// Otherwise grow the container.
+		CWISS_RawTable_Resize(policy, self, self->capacity_ * 2 + 1);
+	}
+}
+
+/// Prefetches the backing array to dodge potential TLB misses.
+/// This is intended to overlap with execution of calculating the hash for a
+/// key.
+static inline void CWISS_RawTable_PrefetchHeapBlock(const CWISS_Policy *policy,
+						    const CWISS_RawTable *self)
+{
+	CWISS_PREFETCH(self->ctrl_, 1);
+}
+
+/// Issues CPU prefetch instructions for the memory needed to find or insert
+/// a key.
+///
+/// NOTE: This is a very low level operation and should not be used without
+/// specific benchmarks indicating its importance.
+static inline void CWISS_RawTable_Prefetch(const CWISS_Policy *policy,
+					   const CWISS_RawTable *self,
+					   const void *key)
+{
+	(void)key;
+#if CWISS_HAVE_PREFETCH
+	CWISS_RawTable_PrefetchHeapBlock(policy, self);
+	CWISS_ProbeSeq seq = CWISS_ProbeSeq_Start(
+		self->ctrl_, policy->key->hash(key), self->capacity_);
+	CWISS_PREFETCH(self->ctrl_ + seq.offset_, 3);
+	CWISS_PREFETCH(self->ctrl_ + seq.offset_ * policy->slot->size, 3);
+#endif
+}
+
+/// The return type of `CWISS_RawTable_PrepareInsert()`.
+typedef struct {
+	size_t index;
+	bool inserted;
+} CWISS_PrepareInsert;
+
+/// Given the hash of a value not currently in the table, finds the next viable
+/// slot index to insert it at.
+///
+/// If the table does not actually have space, UB.
+CWISS_INLINE_NEVER
+static size_t CWISS_RawTable_PrepareInsert(const CWISS_Policy *policy,
+					   CWISS_RawTable *self, size_t hash)
+{
+	CWISS_FindInfo target =
+		CWISS_FindFirstNonFull(self->ctrl_, hash, self->capacity_);
+	if (CWISS_UNLIKELY(self->growth_left_ == 0 &&
+			   !CWISS_IsDeleted(self->ctrl_[target.offset]))) {
+		CWISS_RawTable_rehash_and_grow_if_necessary(policy, self);
+		target = CWISS_FindFirstNonFull(self->ctrl_, hash,
+						self->capacity_);
+	}
+	++self->size_;
+	self->growth_left_ -= CWISS_IsEmpty(self->ctrl_[target.offset]);
+	CWISS_SetCtrl(target.offset, CWISS_H2(hash), self->capacity_,
+		      self->ctrl_, self->slots_, policy->slot->size);
+	// infoz().RecordInsert(hash, target.probe_length);
+	return target.offset;
+}
+
+/// Attempts to find `key` in the table; if it isn't found, returns where to
+/// insert it, instead.
+static inline CWISS_PrepareInsert
+CWISS_RawTable_FindOrPrepareInsert(const CWISS_Policy *policy,
+				   const CWISS_KeyPolicy *key_policy,
+				   CWISS_RawTable *self, const void *key)
+{
+	CWISS_RawTable_PrefetchHeapBlock(policy, self);
+	size_t hash = key_policy->hash(key);
+	CWISS_ProbeSeq seq =
+		CWISS_ProbeSeq_Start(self->ctrl_, hash, self->capacity_);
+	while (true) {
+		CWISS_Group g = CWISS_Group_new(self->ctrl_ + seq.offset_);
+		CWISS_BitMask match = CWISS_Group_Match(&g, CWISS_H2(hash));
+		uint32_t i;
+		while (CWISS_BitMask_next(&match, &i)) {
+			size_t idx = CWISS_ProbeSeq_offset(&seq, i);
+			char *slot = self->slots_ + idx * policy->slot->size;
+			if (CWISS_LIKELY(key_policy->eq(
+				    key, policy->slot->get(slot))))
+				return (CWISS_PrepareInsert){ idx, false };
+		}
+		if (CWISS_LIKELY(CWISS_Group_MatchEmpty(&g).mask))
+			break;
+		CWISS_ProbeSeq_next(&seq);
+		CWISS_DCHECK(seq.index_ <= self->capacity_, "full table!");
+	}
+	return (CWISS_PrepareInsert){
+		CWISS_RawTable_PrepareInsert(policy, self, hash), true
+	};
+}
+
+/// Prepares a slot to insert an element into.
+///
+/// This function does all the work of calling the appropriate policy functions
+/// to initialize the slot.
+static inline void *CWISS_RawTable_PreInsert(const CWISS_Policy *policy,
+					     CWISS_RawTable *self, size_t i)
+{
+	void *dst = self->slots_ + i * policy->slot->size;
+	policy->slot->init(dst);
+	return policy->slot->get(dst);
+}
+
+/// Creates a new empty table with the given capacity.
+static inline CWISS_RawTable CWISS_RawTable_new(const CWISS_Policy *policy,
+						size_t capacity)
+{
+	CWISS_RawTable self = {
+		.ctrl_ = CWISS_EmptyGroup(),
+	};
+
+	if (capacity != 0) {
+		self.capacity_ = CWISS_NormalizeCapacity(capacity);
+		CWISS_RawTable_InitializeSlots(policy, &self);
+	}
+
+	return self;
+}
+
+/// Ensures that at least `n` more elements can be inserted without a resize
+/// (although this function my itself resize and rehash the table).
+static inline void CWISS_RawTable_reserve(const CWISS_Policy *policy,
+					  CWISS_RawTable *self, size_t n)
+{
+	if (n <= self->size_ + self->growth_left_) {
+		return;
+	}
+
+	n = CWISS_NormalizeCapacity(CWISS_GrowthToLowerboundCapacity(n));
+	CWISS_RawTable_Resize(policy, self, n);
+
+	// This is after resize, to ensure that we have completed the allocation
+	// and have potentially sampled the hashtable.
+	// infoz().RecordReservation(n);
+}
+
+/// Creates a duplicate of this table.
+static inline CWISS_RawTable CWISS_RawTable_dup(const CWISS_Policy *policy,
+						const CWISS_RawTable *self)
+{
+	CWISS_RawTable copy = CWISS_RawTable_new(policy, 0);
+
+	CWISS_RawTable_reserve(policy, &copy, self->size_);
+	// Because the table is guaranteed to be empty, we can do something faster
+	// than a full `insert`. In particular we do not need to take a trip to
+	// `CWISS_RawTable_rehash_and_grow_if_necessary()` because we are already
+	// big enough (since `self` is a priori) and tombstones cannot be created
+	// during this process.
+	for (CWISS_RawIter iter = CWISS_RawTable_citer(policy, self);
+	     CWISS_RawIter_get(policy, &iter);
+	     CWISS_RawIter_next(policy, &iter)) {
+		void *v = CWISS_RawIter_get(policy, &iter);
+		size_t hash = policy->key->hash(v);
+
+		CWISS_FindInfo target = CWISS_FindFirstNonFull(copy.ctrl_, hash,
+							       copy.capacity_);
+		CWISS_SetCtrl(target.offset, CWISS_H2(hash), copy.capacity_,
+			      copy.ctrl_, copy.slots_, policy->slot->size);
+		void *slot =
+			CWISS_RawTable_PreInsert(policy, &copy, target.offset);
+		policy->obj->copy(slot, v);
+		// infoz().RecordInsert(hash, target.probe_length);
+	}
+	copy.size_ = self->size_;
+	copy.growth_left_ -= self->size_;
+	return copy;
+}
+
+/// Destroys this table, destroying its elements and freeing the backing array.
+static inline void CWISS_RawTable_destroy(const CWISS_Policy *policy,
+					  CWISS_RawTable *self)
+{
+	CWISS_RawTable_DestroySlots(policy, self);
+}
+
+/// Returns whether the table is empty.
+static inline bool CWISS_RawTable_empty(const CWISS_Policy *policy,
+					const CWISS_RawTable *self)
+{
+	return !self->size_;
+}
+
+/// Returns the number of elements in the table.
+static inline size_t CWISS_RawTable_size(const CWISS_Policy *policy,
+					 const CWISS_RawTable *self)
+{
+	return self->size_;
+}
+
+/// Returns the total capacity of the table, which is different from the number
+/// of elements that would cause it to get resized.
+static inline size_t CWISS_RawTable_capacity(const CWISS_Policy *policy,
+					     const CWISS_RawTable *self)
+{
+	return self->capacity_;
+}
+
+/// Clears the table, erasing every element contained therein.
+static inline void CWISS_RawTable_clear(const CWISS_Policy *policy,
+					CWISS_RawTable *self)
+{
+	// Iterating over this container is O(bucket_count()). When bucket_count()
+	// is much greater than size(), iteration becomes prohibitively expensive.
+	// For clear() it is more important to reuse the allocated array when the
+	// container is small because allocation takes comparatively long time
+	// compared to destruction of the elements of the container. So we pick the
+	// largest bucket_count() threshold for which iteration is still fast and
+	// past that we simply deallocate the array.
+	if (self->capacity_ > 127) {
+		CWISS_RawTable_DestroySlots(policy, self);
+
+		// infoz().RecordClearedReservation();
+	} else if (self->capacity_) {
+		if (policy->slot->del != NULL) {
+			for (size_t i = 0; i != self->capacity_; ++i) {
+				if (CWISS_IsFull(self->ctrl_[i])) {
+					policy->slot->del(
+						self->slots_ +
+						i * policy->slot->size);
+				}
+			}
+		}
+
+		self->size_ = 0;
+		CWISS_ResetCtrl(self->capacity_, self->ctrl_, self->slots_,
+				policy->slot->size);
+		CWISS_RawTable_ResetGrowthLeft(policy, self);
+	}
+	CWISS_DCHECK(!self->size_, "size was still nonzero");
+	// infoz().RecordStorageChanged(0, capacity_);
+}
+
+/// The return type of `CWISS_RawTable_insert()`.
+typedef struct {
+	/// An iterator referring to the relevant element.
+	CWISS_RawIter iter;
+	/// True if insertion actually occurred; false if the element was already
+	/// present.
+	bool inserted;
+} CWISS_Insert;
+
+/// "Inserts" `val` into the table if it isn't already present.
+///
+/// This function does not perform insertion; it behaves exactly like
+/// `CWISS_RawTable_insert()` up until it would copy-initialize the new
+/// element, instead returning a valid iterator pointing to uninitialized data.
+///
+/// This allows, for example, lazily constructing the parts of the element that
+/// do not figure into the hash or equality.
+///
+/// If this function returns `true` in `inserted`, the caller has *no choice*
+/// but to insert, i.e., they may not change their minds at that point.
+///
+/// `key_policy` is a possibly heterogenous key policy for comparing `key`'s
+/// type to types in the map. `key_policy` may be `&policy->key`.
+static inline CWISS_Insert
+CWISS_RawTable_deferred_insert(const CWISS_Policy *policy,
+			       const CWISS_KeyPolicy *key_policy,
+			       CWISS_RawTable *self, const void *key)
+{
+	CWISS_PrepareInsert res = CWISS_RawTable_FindOrPrepareInsert(
+		policy, key_policy, self, key);
+
+	if (res.inserted) {
+		CWISS_RawTable_PreInsert(policy, self, res.index);
+	}
+	return (CWISS_Insert){ CWISS_RawTable_citer_at(policy, self, res.index),
+			       res.inserted };
+}
+
+/// Inserts `val` (by copy) into the table if it isn't already present.
+///
+/// Returns an iterator pointing to the element in the map and whether it was
+/// just inserted or was already present.
+static inline CWISS_Insert CWISS_RawTable_insert(const CWISS_Policy *policy,
+						 CWISS_RawTable *self,
+						 const void *val)
+{
+	CWISS_PrepareInsert res = CWISS_RawTable_FindOrPrepareInsert(
+		policy, policy->key, self, val);
+
+	if (res.inserted) {
+		void *slot = CWISS_RawTable_PreInsert(policy, self, res.index);
+		policy->obj->copy(slot, val);
+	}
+	return (CWISS_Insert){ CWISS_RawTable_citer_at(policy, self, res.index),
+			       res.inserted };
+}
+
+/// Tries to find the corresponding entry for `key` using `hash` as a hint.
+/// If not found, returns a null iterator.
+///
+/// `key_policy` is a possibly heterogenous key policy for comparing `key`'s
+/// type to types in the map. `key_policy` may be `&policy->key`.
+///
+/// If `hash` is not actually the hash of `key`, UB.
+static inline CWISS_RawIter CWISS_RawTable_find_hinted(
+	const CWISS_Policy *policy, const CWISS_KeyPolicy *key_policy,
+	const CWISS_RawTable *self, const void *key, size_t hash)
+{
+	CWISS_ProbeSeq seq =
+		CWISS_ProbeSeq_Start(self->ctrl_, hash, self->capacity_);
+	while (true) {
+		CWISS_Group g = CWISS_Group_new(self->ctrl_ + seq.offset_);
+		CWISS_BitMask match = CWISS_Group_Match(&g, CWISS_H2(hash));
+		uint32_t i;
+		while (CWISS_BitMask_next(&match, &i)) {
+			char *slot =
+				self->slots_ + CWISS_ProbeSeq_offset(&seq, i) *
+						       policy->slot->size;
+			if (CWISS_LIKELY(key_policy->eq(
+				    key, policy->slot->get(slot))))
+				return CWISS_RawTable_citer_at(
+					policy, self,
+					CWISS_ProbeSeq_offset(&seq, i));
+		}
+		if (CWISS_LIKELY(CWISS_Group_MatchEmpty(&g).mask))
+			return (CWISS_RawIter){ 0 };
+		CWISS_ProbeSeq_next(&seq);
+		CWISS_DCHECK(seq.index_ <= self->capacity_, "full table!");
+	}
+}
+
+/// Tries to find the corresponding entry for `key`.
+/// If not found, returns a null iterator.
+///
+/// `key_policy` is a possibly heterogenous key policy for comparing `key`'s
+/// type to types in the map. `key_policy` may be `&policy->key`.
+static inline CWISS_RawIter
+CWISS_RawTable_find(const CWISS_Policy *policy,
+		    const CWISS_KeyPolicy *key_policy,
+		    const CWISS_RawTable *self, const void *key)
+{
+	return CWISS_RawTable_find_hinted(policy, key_policy, self, key,
+					  key_policy->hash(key));
+}
+
+/// Erases the element pointed to by the given valid iterator.
+/// This function will invalidate the iterator.
+static inline void CWISS_RawTable_erase_at(const CWISS_Policy *policy,
+					   CWISS_RawIter it)
+{
+	CWISS_AssertIsFull(it.ctrl_);
+	if (policy->slot->del != NULL) {
+		policy->slot->del(it.slot_);
+	}
+	CWISS_RawTable_EraseMetaOnly(policy, it);
+}
+
+/// Erases the entry corresponding to `key`, if present. Returns true if
+/// deletion occured.
+///
+/// `key_policy` is a possibly heterogenous key policy for comparing `key`'s
+/// type to types in the map. `key_policy` may be `&policy->key`.
+static inline bool CWISS_RawTable_erase(const CWISS_Policy *policy,
+					const CWISS_KeyPolicy *key_policy,
+					CWISS_RawTable *self, const void *key)
+{
+	CWISS_RawIter it = CWISS_RawTable_find(policy, key_policy, self, key);
+	if (it.slot_ == NULL)
+		return false;
+	CWISS_RawTable_erase_at(policy, it);
+	return true;
+}
+
+/// Triggers a rehash, growing to at least a capacity of `n`.
+static inline void CWISS_RawTable_rehash(const CWISS_Policy *policy,
+					 CWISS_RawTable *self, size_t n)
+{
+	if (n == 0 && self->capacity_ == 0)
+		return;
+	if (n == 0 && self->size_ == 0) {
+		CWISS_RawTable_DestroySlots(policy, self);
+		// infoz().RecordStorageChanged(0, 0);
+		// infoz().RecordClearedReservation();
+		return;
+	}
+
+	// bitor is a faster way of doing `max` here. We will round up to the next
+	// power-of-2-minus-1, so bitor is good enough.
+	size_t m = CWISS_NormalizeCapacity(
+		n | CWISS_GrowthToLowerboundCapacity(self->size_));
+	// n == 0 unconditionally rehashes as per the standard.
+	if (n == 0 || m > self->capacity_) {
+		CWISS_RawTable_Resize(policy, self, m);
+
+		// This is after resize, to ensure that we have completed the allocation
+		// and have potentially sampled the hashtable.
+		// infoz().RecordReservation(n);
+	}
+}
+
+/// Returns whether `key` is contained in this table.
+///
+/// `key_policy` is a possibly heterogenous key policy for comparing `key`'s
+/// type to types in the map. `key_policy` may be `&policy->key`.
+static inline bool CWISS_RawTable_contains(const CWISS_Policy *policy,
+					   const CWISS_KeyPolicy *key_policy,
+					   const CWISS_RawTable *self,
+					   const void *key)
+{
+	return CWISS_RawTable_find(policy, key_policy, self, key).slot_ != NULL;
+}
+
+CWISS_END_EXTERN
+CWISS_END
+/// cwisstable/internal/raw_table.h ////////////////////////////////////////////
+
+/// cwisstable/declare.h ///////////////////////////////////////////////////////
+/// SwissTable code generation macros.
+///
+/// This file is the entry-point for users of `cwisstable`. It exports six
+/// macros for generating different kinds of tables. Four correspond to Abseil's
+/// four SwissTable containers:
+///
+/// - `CWISS_DECLARE_FLAT_HASHSET(Set, Type)`
+/// - `CWISS_DECLARE_FLAT_HASHMAP(Map, Key, Value)`
+/// - `CWISS_DECLARE_NODE_HASHSET(Set, Type)`
+/// - `CWISS_DECLARE_NODE_HASHMAP(Map, Key, Value)`
+///
+/// These expand to a type (with the same name as the first argument) and and
+/// a collection of strongly-typed functions associated to it (the generated
+/// API is described below). These macros use the default policy (see policy.h)
+/// for each of the four containers; custom policies may be used instead via
+/// the following macros:
+///
+/// - `CWISS_DECLARE_HASHSET_WITH(Set, Type, kPolicy)`
+/// - `CWISS_DECLARE_HASHMAP_WITH(Map, Key, Value, kPolicy)`
+///
+/// `kPolicy` must be a constant global variable referring to an appropriate
+/// property for the element types of the container.
+///
+/// The generated API is safe: the functions are well-typed and automatically
+/// pass the correct policy pointer. Because the pointer is a constant
+/// expression, it promotes devirtualization when inlining.
+///
+/// # Generated API
+///
+/// See `set_api.h` and `map_api.h` for detailed listings of what the generated
+/// APIs look like.
+
+CWISS_BEGIN
+CWISS_BEGIN_EXTERN
+
+/// Generates a new hash set type with inline storage and the default
+/// plain-old-data policies.
+///
+/// See header documentation for examples of generated API.
+#define CWISS_DECLARE_FLAT_HASHSET(HashSet_, Type_)                       \
+	CWISS_DECLARE_FLAT_SET_POLICY(HashSet_##_kPolicy, Type_, (_, _)); \
+	CWISS_DECLARE_HASHSET_WITH(HashSet_, Type_, HashSet_##_kPolicy)
+
+/// Generates a new hash set type with outline storage and the default
+/// plain-old-data policies.
+///
+/// See header documentation for examples of generated API.
+#define CWISS_DECLARE_NODE_HASHSET(HashSet_, Type_)                       \
+	CWISS_DECLARE_NODE_SET_POLICY(HashSet_##_kPolicy, Type_, (_, _)); \
+	CWISS_DECLARE_HASHSET_WITH(HashSet_, Type_, HashSet_##_kPolicy)
+
+/// Generates a new hash map type with inline storage and the default
+/// plain-old-data policies.
+///
+/// See header documentation for examples of generated API.
+#define CWISS_DECLARE_FLAT_HASHMAP(HashMap_, K_, V_)                       \
+	CWISS_DECLARE_FLAT_MAP_POLICY(HashMap_##_kPolicy, K_, V_, (_, _)); \
+	CWISS_DECLARE_HASHMAP_WITH(HashMap_, K_, V_, HashMap_##_kPolicy)
+
+/// Generates a new hash map type with outline storage and the default
+/// plain-old-data policies.
+///
+/// See header documentation for examples of generated API.
+#define CWISS_DECLARE_NODE_HASHMAP(HashMap_, K_, V_)                       \
+	CWISS_DECLARE_NODE_MAP_POLICY(HashMap_##_kPolicy, K_, V_, (_, _)); \
+	CWISS_DECLARE_HASHMAP_WITH(HashMap_, K_, V_, HashMap_##_kPolicy)
+
+/// Generates a new hash set type using the given policy.
+///
+/// See header documentation for examples of generated API.
+#define CWISS_DECLARE_HASHSET_WITH(HashSet_, Type_, kPolicy_)             \
+	typedef Type_ HashSet_##_Entry;                                   \
+	typedef Type_ HashSet_##_Key;                                     \
+	CWISS_DECLARE_COMMON_(HashSet_, HashSet_##_Entry, HashSet_##_Key, \
+			      kPolicy_)
+
+/// Generates a new hash map type using the given policy.
+///
+/// See header documentation for examples of generated API.
+#define CWISS_DECLARE_HASHMAP_WITH(HashMap_, K_, V_, kPolicy_)            \
+	typedef struct {                                                  \
+		K_ key;                                                   \
+		V_ val;                                                   \
+	} HashMap_##_Entry;                                               \
+	typedef K_ HashMap_##_Key;                                        \
+	CWISS_DECLARE_COMMON_(HashMap_, HashMap_##_Entry, HashMap_##_Key, \
+			      kPolicy_)
+
+/// Declares a heterogenous lookup for an existing SwissTable type.
+///
+/// This macro will expect to find the following functions:
+///   - size_t <Table>_<Key>_hash(const Key*);
+///   - bool <Table>_<Key>_eq(const Key*, const <Table>_Key*);
+///
+/// These functions will be used to build the heterogenous key policy.
+#define CWISS_DECLARE_LOOKUP(HashSet_, Key_) \
+	CWISS_DECLARE_LOOKUP_NAMED(HashSet_, Key_, Key_)
+
+/// Declares a heterogenous lookup for an existing SwissTable type.
+///
+/// This is like `CWISS_DECLARE_LOOKUP`, but allows customizing the name used
+/// in the `_by_` prefix on the names, as well as the names of the extension
+/// point functions.
+#define CWISS_DECLARE_LOOKUP_NAMED(HashSet_, LookupName_, Key_)                 \
+	CWISS_BEGIN                                                             \
+	static inline size_t HashSet_##_##LookupName_##_SyntheticHash(          \
+		const void *val)                                                \
+	{                                                                       \
+		return HashSet_##_##LookupName_##_hash((const Key_ *)val);      \
+	}                                                                       \
+	static inline bool HashSet_##_##LookupName_##_SyntheticEq(              \
+		const void *a, const void *b)                                   \
+	{                                                                       \
+		return HashSet_##_##LookupName_##_eq(                           \
+			(const Key_ *)a, (const HashSet_##_Entry *)b);          \
+	}                                                                       \
+	static const CWISS_KeyPolicy HashSet_##_##LookupName_##_kPolicy = {     \
+		HashSet_##_##LookupName_##_SyntheticHash,                       \
+		HashSet_##_##LookupName_##_SyntheticEq,                         \
+	};                                                                      \
+                                                                                \
+	static inline const CWISS_KeyPolicy *HashSet_##_##LookupName_##_policy( \
+		void)                                                           \
+	{                                                                       \
+		return &HashSet_##_##LookupName_##_kPolicy;                     \
+	}                                                                       \
+                                                                                \
+	static inline HashSet_##_Insert                                         \
+		HashSet_##_deferred_insert_by_##LookupName_(HashSet_ *self,     \
+							    const Key_ *key)    \
+	{                                                                       \
+		CWISS_Insert ret = CWISS_RawTable_deferred_insert(              \
+			HashSet_##_policy(),                                    \
+			&HashSet_##_##LookupName_##_kPolicy, &self->set_,       \
+			key);                                                   \
+		return (HashSet_##_Insert){ { ret.iter }, ret.inserted };       \
+	}                                                                       \
+	static inline HashSet_##_CIter                                          \
+		HashSet_##_cfind_hinted_by_##LookupName_(                       \
+			const HashSet_ *self, const Key_ *key, size_t hash)     \
+	{                                                                       \
+		return (HashSet_##_CIter){ CWISS_RawTable_find_hinted(          \
+			HashSet_##_policy(),                                    \
+			&HashSet_##_##LookupName_##_kPolicy, &self->set_, key,  \
+			hash) };                                                \
+	}                                                                       \
+	static inline HashSet_##_Iter HashSet_##_find_hinted_by_##LookupName_(  \
+		HashSet_ *self, const Key_ *key, size_t hash)                   \
+	{                                                                       \
+		return (HashSet_##_Iter){ CWISS_RawTable_find_hinted(           \
+			HashSet_##_policy(),                                    \
+			&HashSet_##_##LookupName_##_kPolicy, &self->set_, key,  \
+			hash) };                                                \
+	}                                                                       \
+                                                                                \
+	static inline HashSet_##_CIter HashSet_##_cfind_by_##LookupName_(       \
+		const HashSet_ *self, const Key_ *key)                          \
+	{                                                                       \
+		return (HashSet_##_CIter){ CWISS_RawTable_find(                 \
+			HashSet_##_policy(),                                    \
+			&HashSet_##_##LookupName_##_kPolicy, &self->set_,       \
+			key) };                                                 \
+	}                                                                       \
+	static inline HashSet_##_Iter HashSet_##_find_by_##LookupName_(         \
+		HashSet_ *self, const Key_ *key)                                \
+	{                                                                       \
+		return (HashSet_##_Iter){ CWISS_RawTable_find(                  \
+			HashSet_##_policy(),                                    \
+			&HashSet_##_##LookupName_##_kPolicy, &self->set_,       \
+			key) };                                                 \
+	}                                                                       \
+                                                                                \
+	static inline bool HashSet_##_contains_by_##LookupName_(                \
+		const HashSet_ *self, const Key_ *key)                          \
+	{                                                                       \
+		return CWISS_RawTable_contains(                                 \
+			HashSet_##_policy(),                                    \
+			&HashSet_##_##LookupName_##_kPolicy, &self->set_,       \
+			key);                                                   \
+	}                                                                       \
+                                                                                \
+	static inline bool HashSet_##_erase_by_##LookupName_(HashSet_ *self,    \
+							     const Key_ *key)   \
+	{                                                                       \
+		return CWISS_RawTable_erase(                                    \
+			HashSet_##_policy(),                                    \
+			&HashSet_##_##LookupName_##_kPolicy, &self->set_,       \
+			key);                                                   \
+	}                                                                       \
+                                                                                \
+	CWISS_END                                                               \
+	/* Force a semicolon. */                                                \
+	struct HashSet_##_##LookupName_##_NeedsTrailingSemicolon_ {             \
+		int x;                                                          \
+	}
+
+// ---- PUBLIC API ENDS HERE! ----
+
+#define CWISS_DECLARE_COMMON_(HashSet_, Type_, Key_, kPolicy_)                 \
+	CWISS_BEGIN                                                            \
+	static inline const CWISS_Policy *HashSet_##_policy(void)              \
+	{                                                                      \
+		return &kPolicy_;                                              \
+	}                                                                      \
+                                                                               \
+	typedef struct {                                                       \
+		CWISS_RawTable set_;                                           \
+	} HashSet_;                                                            \
+	static inline void HashSet_##_dump(const HashSet_ *self)               \
+	{                                                                      \
+		CWISS_RawTable_dump(&kPolicy_, &self->set_);                   \
+	}                                                                      \
+                                                                               \
+	static inline HashSet_ HashSet_##_new(size_t bucket_count)             \
+	{                                                                      \
+		return (HashSet_){ CWISS_RawTable_new(&kPolicy_,               \
+						      bucket_count) };         \
+	}                                                                      \
+	static inline HashSet_ HashSet_##_dup(const HashSet_ *that)            \
+	{                                                                      \
+		return (HashSet_){ CWISS_RawTable_dup(&kPolicy_,               \
+						      &that->set_) };          \
+	}                                                                      \
+	static inline void HashSet_##_destroy(HashSet_ *self)                  \
+	{                                                                      \
+		CWISS_RawTable_destroy(&kPolicy_, &self->set_);                \
+	}                                                                      \
+                                                                               \
+	typedef struct {                                                       \
+		CWISS_RawIter it_;                                             \
+	} HashSet_##_Iter;                                                     \
+	static inline HashSet_##_Iter HashSet_##_iter(HashSet_ *self)          \
+	{                                                                      \
+		return (HashSet_##_Iter){ CWISS_RawTable_iter(&kPolicy_,       \
+							      &self->set_) };  \
+	}                                                                      \
+	static inline Type_ *HashSet_##_Iter_get(const HashSet_##_Iter *it)    \
+	{                                                                      \
+		return (Type_ *)CWISS_RawIter_get(&kPolicy_, &it->it_);        \
+	}                                                                      \
+	static inline Type_ *HashSet_##_Iter_next(HashSet_##_Iter *it)         \
+	{                                                                      \
+		return (Type_ *)CWISS_RawIter_next(&kPolicy_, &it->it_);       \
+	}                                                                      \
+                                                                               \
+	typedef struct {                                                       \
+		CWISS_RawIter it_;                                             \
+	} HashSet_##_CIter;                                                    \
+	static inline HashSet_##_CIter HashSet_##_citer(const HashSet_ *self)  \
+	{                                                                      \
+		return (HashSet_##_CIter){ CWISS_RawTable_citer(               \
+			&kPolicy_, &self->set_) };                             \
+	}                                                                      \
+	static inline const Type_ *HashSet_##_CIter_get(                       \
+		const HashSet_##_CIter *it)                                    \
+	{                                                                      \
+		return (const Type_ *)CWISS_RawIter_get(&kPolicy_, &it->it_);  \
+	}                                                                      \
+	static inline const Type_ *HashSet_##_CIter_next(HashSet_##_CIter *it) \
+	{                                                                      \
+		return (const Type_ *)CWISS_RawIter_next(&kPolicy_, &it->it_); \
+	}                                                                      \
+	static inline HashSet_##_CIter HashSet_##_Iter_const(                  \
+		HashSet_##_Iter it)                                            \
+	{                                                                      \
+		return (HashSet_##_CIter){ it.it_ };                           \
+	}                                                                      \
+                                                                               \
+	static inline void HashSet_##_reserve(HashSet_ *self, size_t n)        \
+	{                                                                      \
+		CWISS_RawTable_reserve(&kPolicy_, &self->set_, n);             \
+	}                                                                      \
+	static inline void HashSet_##_rehash(HashSet_ *self, size_t n)         \
+	{                                                                      \
+		CWISS_RawTable_rehash(&kPolicy_, &self->set_, n);              \
+	}                                                                      \
+                                                                               \
+	static inline bool HashSet_##_empty(const HashSet_ *self)              \
+	{                                                                      \
+		return CWISS_RawTable_empty(&kPolicy_, &self->set_);           \
+	}                                                                      \
+	static inline size_t HashSet_##_size(const HashSet_ *self)             \
+	{                                                                      \
+		return CWISS_RawTable_size(&kPolicy_, &self->set_);            \
+	}                                                                      \
+	static inline size_t HashSet_##_capacity(const HashSet_ *self)         \
+	{                                                                      \
+		return CWISS_RawTable_capacity(&kPolicy_, &self->set_);        \
+	}                                                                      \
+                                                                               \
+	static inline void HashSet_##_clear(HashSet_ *self)                    \
+	{                                                                      \
+		return CWISS_RawTable_clear(&kPolicy_, &self->set_);           \
+	}                                                                      \
+                                                                               \
+	typedef struct {                                                       \
+		HashSet_##_Iter iter;                                          \
+		bool inserted;                                                 \
+	} HashSet_##_Insert;                                                   \
+	static inline HashSet_##_Insert HashSet_##_deferred_insert(            \
+		HashSet_ *self, const Key_ *key)                               \
+	{                                                                      \
+		CWISS_Insert ret = CWISS_RawTable_deferred_insert(             \
+			&kPolicy_, kPolicy_.key, &self->set_, key);            \
+		return (HashSet_##_Insert){ { ret.iter }, ret.inserted };      \
+	}                                                                      \
+	static inline HashSet_##_Insert HashSet_##_insert(HashSet_ *self,      \
+							  const Type_ *val)    \
+	{                                                                      \
+		CWISS_Insert ret =                                             \
+			CWISS_RawTable_insert(&kPolicy_, &self->set_, val);    \
+		return (HashSet_##_Insert){ { ret.iter }, ret.inserted };      \
+	}                                                                      \
+                                                                               \
+	static inline HashSet_##_CIter HashSet_##_cfind_hinted(                \
+		const HashSet_ *self, const Key_ *key, size_t hash)            \
+	{                                                                      \
+		return (HashSet_##_CIter){ CWISS_RawTable_find_hinted(         \
+			&kPolicy_, kPolicy_.key, &self->set_, key, hash) };    \
+	}                                                                      \
+	static inline HashSet_##_Iter HashSet_##_find_hinted(                  \
+		HashSet_ *self, const Key_ *key, size_t hash)                  \
+	{                                                                      \
+		return (HashSet_##_Iter){ CWISS_RawTable_find_hinted(          \
+			&kPolicy_, kPolicy_.key, &self->set_, key, hash) };    \
+	}                                                                      \
+	static inline HashSet_##_CIter HashSet_##_cfind(const HashSet_ *self,  \
+							const Key_ *key)       \
+	{                                                                      \
+		return (HashSet_##_CIter){ CWISS_RawTable_find(                \
+			&kPolicy_, kPolicy_.key, &self->set_, key) };          \
+	}                                                                      \
+	static inline HashSet_##_Iter HashSet_##_find(HashSet_ *self,          \
+						      const Key_ *key)         \
+	{                                                                      \
+		return (HashSet_##_Iter){ CWISS_RawTable_find(                 \
+			&kPolicy_, kPolicy_.key, &self->set_, key) };          \
+	}                                                                      \
+                                                                               \
+	static inline bool HashSet_##_contains(const HashSet_ *self,           \
+					       const Key_ *key)                \
+	{                                                                      \
+		return CWISS_RawTable_contains(&kPolicy_, kPolicy_.key,        \
+					       &self->set_, key);              \
+	}                                                                      \
+                                                                               \
+	static inline void HashSet_##_erase_at(HashSet_##_Iter it)             \
+	{                                                                      \
+		CWISS_RawTable_erase_at(&kPolicy_, it.it_);                    \
+	}                                                                      \
+	static inline bool HashSet_##_erase(HashSet_ *self, const Key_ *key)   \
+	{                                                                      \
+		return CWISS_RawTable_erase(&kPolicy_, kPolicy_.key,           \
+					    &self->set_, key);                 \
+	}                                                                      \
+                                                                               \
+	CWISS_END                                                              \
+	/* Force a semicolon. */ struct HashSet_##_NeedsTrailingSemicolon_ {   \
+		int x;                                                         \
+	}
+
+CWISS_END_EXTERN
+CWISS_END
+/// cwisstable/declare.h ///////////////////////////////////////////////////////
+
+#undef INT8_C
+#undef UINT64_C
+
+#endif // CWISSTABLE_H_
diff --git a/mm/demeter/error.h b/mm/demeter/error.h
new file mode 100644
index 000000000000..3fdba0748474
--- /dev/null
+++ b/mm/demeter/error.h
@@ -0,0 +1,82 @@
+#ifndef DEMETER_PLACEMENT_ERROR_H
+#define DEMETER_PLACEMENT_ERROR_H
+
+#include <linux/err.h>
+#include <linux/errno.h>
+#include <linux/errname.h>
+
+// IS_ERR_VALUE() supports upto MAX_ERRNO number of error codes. However, the
+// kernel only defined 531 of them with the largest being ENOGRACE.
+enum error_code {
+	MAX_USED = ENOGRACE,
+	// Bad pebs sample
+	ECBADSAMPLE,
+	// Address not of interest to us
+	ECNOTCARE,
+
+	MAX_CODES,
+};
+
+inline const char *ecname(int err)
+{
+#define E(err) [err - MAX_USED] = "-" #err
+	static char const *names[] = {
+		E(ECBADSAMPLE),
+		E(ECNOTCARE),
+	};
+#undef E
+	if (err <= MAX_USED)
+		return errname(err);
+	if (err < MAX_CODES)
+		return names[err - MAX_USED];
+	return NULL;
+}
+
+#define TRY1(val)                                                        \
+	({                                                               \
+		__auto_type __val = (val);                               \
+		bool __is_pointer = __builtin_classify_type(__val) == 5; \
+		if ((ulong)(__val) >= (ulong)(-MAX_ERRNO) ||             \
+		    (__is_pointer && !__val))                            \
+			return (long)__val ?: -EFAULT;                   \
+		__val;                                                   \
+	})
+
+#define TRYn(val, ...)                                                      \
+	({                                                                  \
+		__auto_type __val = (val);                                  \
+		bool __is_pointer = __builtin_classify_type(__val) == 5;    \
+		if ((ulong)(__val) >= (ulong)(-MAX_ERRNO) ||                \
+		    (__is_pointer && !__val)) {                             \
+			pr_err("TRY(%pe) failed at %s:%d %s caller %pSR\n", \
+			       (void *)(ulong)__val, __FILE__, __LINE__,    \
+			       __func__, __builtin_return_address(0));      \
+			pr_err(__VA_ARGS__);                                \
+			return (long)__val ?: -EFAULT;                      \
+		}                                                           \
+		__val;                                                      \
+	})
+
+#define GET_TRY(_0, _1, _2, _3, _4, _5, _6, _7, _8, _9, NAME, ...) NAME
+#define TRY(...)                                                             \
+	GET_TRY(_0, ##__VA_ARGS__, TRYn, TRYn, TRYn, TRYn, TRYn, TRYn, TRYn, \
+		TRYn, TRY1, BUILD_BUG)                                             \
+	(__VA_ARGS__)
+
+#define UNWRAP(val, ...)                                                             \
+	({                                                                           \
+		__auto_type __val = (val);                                           \
+		bool __is_pointer = __builtin_classify_type(__val) == 5;             \
+		if ((ulong)(__val) >= (ulong)(-MAX_ERRNO) ||                         \
+		    (__is_pointer && !__val)) {                                      \
+			pr_err("RTREE_UNWRAP(%pe) failed at %s:%d %s caller %pSR\n", \
+			       (void *)(ulong)__val, __FILE__, __LINE__,             \
+			       __func__, __builtin_return_address(0));               \
+			pr_err(__VA_ARGS__);                                         \
+			dump_stack();                                                \
+			BUG();                                                       \
+		}                                                                    \
+		__val;                                                               \
+	})
+
+#endif // DEMETER_PLACEMENT_ERROR_H
diff --git a/mm/demeter/demeter.h b/mm/demeter/demeter.h
new file mode 100644
index 000000000000..b7ae7801eb5a
--- /dev/null
+++ b/mm/demeter/demeter.h
@@ -0,0 +1,22 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2021-2024 Junliang Hu
+ *
+ * Author: Junliang Hu <jlhu@cse.cuhk.edu.hk>
+ *
+ */
+
+#ifndef DEMETER_H
+#define DEMETER_H
+
+#include <linux/init.h>
+
+extern void __exit demeter_sysfs_exit(void);
+extern int __init demeter_sysfs_init(void);
+
+struct target;
+extern noinline struct target *target_new(pid_t pid);
+extern noinline void target_drop(struct target *t);
+extern pid_t target_pid(struct target *t);
+
+#endif // !DEMETER_H
diff --git a/mm/demeter/hashmap.h b/mm/demeter/hashmap.h
new file mode 100644
index 000000000000..9e4132b0626b
--- /dev/null
+++ b/mm/demeter/hashmap.h
@@ -0,0 +1,64 @@
+#ifndef DEMETER_PLACEMENT_HASHMAP_H
+#define DEMETER_PLACEMENT_HASHMAP_H
+#include <linux/types.h>
+
+#include "vector.h"
+#include "cwisstable.h"
+
+#define CHECK_INSERTED(ins, success, ...)                              \
+	({                                                             \
+		HashMapU64U64_Insert __insert = (ins);                 \
+		HashMapU64U64_Entry *__entry =                         \
+			HashMapU64U64_Iter_get(&__insert.iter);        \
+		CWISS_CHECK(__insert.inserted == (success) && __entry, \
+			    __VA_ARGS__);                              \
+		__entry;                                               \
+	})
+
+
+// HashMapU64U64: A hash map from u64 to u64
+CWISS_DECLARE_FLAT_HASHMAP(HashMapU64U64, u64, u64);
+noinline static inline HashMapU64U64_Entry *
+HashMapU64U64_update(HashMapU64U64 *self, HashMapU64U64_Entry const *e)
+{
+	HashMapU64U64_Entry *r = CHECK_INSERTED(
+		HashMapU64U64_insert(self, e), false,
+		"cannot update a non-existant entry key=%llx val=%llu", e->key,
+		e->val);
+	r->val = e->val;
+	return r;
+}
+
+noinline static inline void HashMapU64U64_swap(HashMapU64U64 *self,
+					       u64 const *keyx, u64 const *keyy)
+{
+	HashMapU64U64_Iter iterx = HashMapU64U64_find(self, keyx);
+	HashMapU64U64_Entry *x = HashMapU64U64_Iter_get(&iterx);
+	CWISS_CHECK(
+		x != NULL,
+		"getting a non-existant entry via keyx from the map keyx=0x%llx keyy=0x%llx",
+		*keyx, *keyy);
+
+	HashMapU64U64_Iter itery = HashMapU64U64_find(self, keyy);
+	HashMapU64U64_Entry *y = HashMapU64U64_Iter_get(&itery);
+	CWISS_CHECK(
+		y != NULL,
+		"getting a non-existant entry via keyy from the map keyx=0x%llx keyy=0x%llx",
+		*keyx, *keyy);
+
+	swap(x->val, y->val);
+}
+noinline static inline HashMapU64U64_Entry *
+HashMapU64U64_get_or_insert(HashMapU64U64 *map, u64 key, u64 val)
+{
+	HashMapU64U64_Iter iter = HashMapU64U64_find(map, &key);
+	HashMapU64U64_Entry *e = HashMapU64U64_Iter_get(&iter);
+	if (e != NULL) {
+		return e;
+	}
+	HashMapU64U64_Entry ne = { key, val };
+	return CHECK_INSERTED(HashMapU64U64_insert(map, &ne), true,
+			      "insertion failed key=%llx val=%llu", key, val);
+}
+
+#endif // DEMETER_PLACEMENT_HASHMAP_H
diff --git a/mm/demeter/module.c b/mm/demeter/module.c
new file mode 100644
index 000000000000..5a8deb921408
--- /dev/null
+++ b/mm/demeter/module.c
@@ -0,0 +1,154 @@
+#include <linux/module.h>
+
+#include "demeter.h"
+#include "module.h"
+
+ulong load_latency_sample_period = LOAD_LATENCY_SAMPLE_PERIOD;
+module_param_named(load_latency_sample_period, load_latency_sample_period,
+		   ulong, 0644);
+MODULE_PARM_DESC(load_latency_sample_period,
+		 "Sample period for ldlat event, defaults to 17");
+
+ulong load_latency_threshold = LOAD_LATENCY_THRESHOLD;
+module_param_named(load_latency_threshold, load_latency_threshold, ulong, 0644);
+MODULE_PARM_DESC(load_latency_threshold,
+		 "Load latency threshold for ldlat event, defaults to 64");
+
+ulong retired_stores_sample_period = RETIRED_STORES_SAMPLE_PERIOD;
+module_param_named(retired_stores_sample_period, retired_stores_sample_period,
+		   ulong, 0644);
+MODULE_PARM_DESC(retired_stores_sample_period,
+		 "Sample period for retired stores event, defaults to 17");
+
+ulong load_l3_miss_sample_period = LOAD_L3_MISS_SAMPLE_PERIOD;
+module_param_named(load_l3_miss_sample_period, load_l3_miss_sample_period,
+		   ulong, 0644);
+MODULE_PARM_DESC(load_l3_miss_sample_period,
+		 "Sample period for local DRAM L3 miss event, defaults to 17");
+
+ulong throttle_pulse_width_ms = THROTTLE_PULSE_WIDTH_MS;
+module_param_named(throttle_pulse_width_ms, throttle_pulse_width_ms, ulong,
+		   0644);
+MODULE_PARM_DESC(throttle_pulse_width_ms,
+		 "Throttle pulse width in ms, defaults to 1000");
+
+ulong throttle_pulse_period_ms = THROTTLE_PULSE_PERIOD_MS;
+module_param_named(throttle_pulse_period_ms, throttle_pulse_period_ms, ulong,
+		   0644);
+MODULE_PARM_DESC(throttle_pulse_period_ms,
+		 "Throttle pulse period in ms, defaults to 5000");
+
+ulong split_period_ms = SPLI_PERIOD_MS;
+module_param_named(split_period_ms, split_period_ms, ulong, 0644);
+MODULE_PARM_DESC(split_period_ms, "Split period in ms, defaults to 200");
+
+ulong rtree_split_thresh = RTREE_SPLIT_THRESH;
+module_param_named(rtree_split_thresh, rtree_split_thresh, ulong, 0644);
+MODULE_PARM_DESC(rtree_split_thresh,
+		 "Split threshold in bytes, defaults to 15");
+
+ulong rtree_exch_thresh = RTREE_EXCH_THRESH;
+module_param_named(rtree_exch_thresh, rtree_exch_thresh, ulong, 0644);
+MODULE_PARM_DESC(rtree_exch_thresh,
+		 "Exchange threshold in bytes, defaults to 2^20");
+
+DEFINE_STATIC_KEY_TRUE(should_decay_sketch);
+struct kmem_cache *list_head_cache;
+
+static void intel_pmu_print_debug_all(void)
+{
+	int cpu;
+	for_each_online_cpu(cpu) {
+		void perf_event_print_debug(void);
+		smp_call_on_cpu(cpu, (int (*)(void *))perf_event_print_debug,
+				NULL, false);
+	}
+}
+
+struct perf_event_attr event_attrs[MAX_EVENTS] = {
+	[EVENT_LOAD] = {
+		.type = PERF_TYPE_RAW,
+		.config = MEM_TRANS_RETIRED_LOAD_LATENCY,
+		.config1 = LOAD_LATENCY_THRESHOLD,
+		.sample_type = PERF_SAMPLE_TID | PERF_SAMPLE_TIME | PERF_SAMPLE_ADDR |
+			       PERF_SAMPLE_WEIGHT | PERF_SAMPLE_PHYS_ADDR,
+		.sample_period = LOAD_LATENCY_SAMPLE_PERIOD ,
+		.inherit = 1,
+		.precise_ip = 3,
+		// .disabled = 1,
+		.exclude_kernel = 1,
+		.exclude_hv = 1,
+		.exclude_callchain_kernel = 1,
+	},
+	// [EVENT_LOAD] = {
+	// 	.type = PERF_TYPE_RAW,
+	// 	.config = MEM_LOAD_L3_MISS_RETIRED_LOCAL_DRAM,
+	// 	.sample_type = PERF_SAMPLE_TID | PERF_SAMPLE_TIME | PERF_SAMPLE_ADDR |
+	// 		       PERF_SAMPLE_WEIGHT | PERF_SAMPLE_PHYS_ADDR,
+	// 	.sample_period = LOAD_L3_MISS_SAMPLE_PERIOD,
+	// 	.inherit = 1,
+	// 	.precise_ip = 3,
+	// 	// .disabled = 1,
+	// 	.exclude_kernel = 1,
+	// 	.exclude_hv = 1,
+	// 	.exclude_callchain_kernel = 1,
+	// },
+	[EVENT_STORE] = {
+		.type = PERF_TYPE_RAW,
+		.config = MEM_INST_RETIRED_ALL_STORES,
+		.sample_type = PERF_SAMPLE_TID | PERF_SAMPLE_TIME | PERF_SAMPLE_ADDR |
+			       PERF_SAMPLE_WEIGHT | PERF_SAMPLE_PHYS_ADDR,
+		.sample_period = RETIRED_STORES_SAMPLE_PERIOD ,
+		.inherit = 1,
+		.precise_ip = 3,
+		// .disabled = 1,
+		.exclude_kernel = 1,
+		.exclude_hv = 1,
+		.exclude_callchain_kernel = 1,
+	},
+};
+static inline void event_attrs_update_param(void)
+{
+	event_attrs[EVENT_LOAD].sample_period = load_latency_sample_period;
+	event_attrs[EVENT_LOAD].config1 = load_latency_threshold;
+
+	// event_attrs[EVENT_LOAD].sample_period = load_l3_miss_sample_period;
+	event_attrs[EVENT_STORE].sample_period = retired_stores_sample_period;
+	pr_info("%s: local_dram_miss_sample_period=%lu retired_stores_sample_period=%lu load_latency_sample_period=%lu load_latency_threshold=%lu\n",
+		__func__, load_l3_miss_sample_period,
+		retired_stores_sample_period, load_latency_sample_period,
+		load_latency_threshold);
+}
+
+static u64 num_possible_pages(void)
+{
+	u64 spanned = 0;
+	int nid;
+	for_each_node_state(nid, N_MEMORY) {
+		spanned += node_spanned_pages(nid);
+	}
+	return spanned;
+}
+
+static __init int init(void)
+{
+	list_head_cache = KMEM_CACHE(list_head, 0);
+	if (!list_head_cache) {
+		pr_err("Failed to create list_head cache\n");
+		return -ENOMEM;
+	}
+	event_attrs_update_param();
+	return demeter_sysfs_init();
+}
+
+static __exit void exit(void)
+{
+	demeter_sysfs_exit();
+	kmem_cache_destroy(list_head_cache);
+}
+
+module_init(init);
+module_exit(exit);
+MODULE_AUTHOR("Junliang Hu <jlhu@cse.cuhk.edu.hk>");
+MODULE_DESCRIPTION("Memory placement optimization module");
+MODULE_LICENSE("GPL");
diff --git a/mm/demeter/module.h b/mm/demeter/module.h
new file mode 100644
index 000000000000..98f469d31279
--- /dev/null
+++ b/mm/demeter/module.h
@@ -0,0 +1,60 @@
+#ifndef DEMETER_PLACEMENT_MODULE_H
+#define DEMETER_PLACEMENT_MODULE_H
+
+#include <linux/perf_event.h>
+#include <linux/prime_numbers.h>
+
+#define FMEM_NID (first_node(node_states[N_MEMORY]))
+#define SMEM_NID (last_node(node_states[N_MEMORY]))
+#define FMEM_NODE (NODE_DATA(FMEM_NID))
+#define SMEM_NODE (NODE_DATA(SMEM_NID))
+
+enum module_param_defaults {
+	LOAD_LATENCY_SAMPLE_PERIOD = 4093,
+	LOAD_LATENCY_THRESHOLD = 60,
+	RETIRED_STORES_SAMPLE_PERIOD = 65535,
+	LOAD_L3_MISS_SAMPLE_PERIOD = 4093,
+	SDS_WIDTH_AUTO = 8192,
+	SDS_DEPTH = 4,
+	ASYNCHRONOUS_ARCHITECTURE = true,
+	DECAY_SKETCH = true,
+	THROTTLE_PULSE_WIDTH_MS = 0,
+	THROTTLE_PULSE_PERIOD_MS = 5000,
+	SPLI_PERIOD_MS = 500,
+};
+enum rtree_param_defaults {
+	RTREE_SPLIT_N = 2,
+	RTREE_GRANULARITY = 2ul << 20,
+	RTREE_SIGNIFICANCE_FACTOR = 2,
+	// TODO: make this value configurable and adaptive
+	RTREE_SPLIT_THRESH = 15,
+	RTREE_EXCH_THRESH = RTREE_GRANULARITY,
+	RTREE_MAX_SIZE = 2048,
+	RTREE_COOL_AGE = 3,
+};
+enum event_config {
+	MEM_TRANS_RETIRED_LOAD_LATENCY = 0x01cd,
+	MEM_INST_RETIRED_ALL_STORES = 0x82d0,
+	MEM_LOAD_L3_MISS_RETIRED_LOCAL_DRAM = 0x01d3,
+};
+enum target_event {
+	EVENT_LOAD,
+	EVENT_STORE,
+	MAX_EVENTS,
+};
+extern struct perf_event_attr event_attrs[MAX_EVENTS];
+
+extern ulong load_latency_sample_period;
+extern ulong load_latency_threshold;
+extern ulong retired_stores_sample_period;
+extern ulong throttle_pulse_width_ms;
+extern ulong throttle_pulse_period_ms;
+extern ulong split_period_ms;
+extern ulong rtree_split_thresh;
+extern ulong rtree_exch_thresh;
+
+extern struct kmem_cache *list_head_cache;
+
+DECLARE_STATIC_KEY_TRUE(use_asynchronous_architecture);
+
+#endif // !DEMETER_PLACEMENT_MODULE_H
diff --git a/mm/demeter/mpsc.h b/mm/demeter/mpsc.h
new file mode 100644
index 000000000000..780b97ec2594
--- /dev/null
+++ b/mm/demeter/mpsc.h
@@ -0,0 +1,91 @@
+#ifndef DEMETER_MPSC_H
+#define DEMETER_MPSC_H
+#include <linux/ring_buffer.h>
+
+typedef struct trace_buffer *mpsc_t;
+noinline static inline mpsc_t mpsc_new(size_t bytes_per_cpu)
+{
+	return ring_buffer_alloc(bytes_per_cpu, RB_FL_OVERWRITE);
+}
+noinline static inline ssize_t mpsc_send(mpsc_t chan, void *src, size_t len)
+{
+	struct ring_buffer_event *e = ring_buffer_lock_reserve(chan, len);
+	if (!e)
+		return -EAGAIN;
+	void *ptr = ring_buffer_event_data(e);
+	memcpy(ptr, src, len);
+	ring_buffer_unlock_commit(chan);
+	return len;
+}
+noinline static inline ssize_t mpsc_recv_cpu(mpsc_t chan, int cpu, void *dst,
+					     size_t len)
+{
+	struct ring_buffer_event *e =
+		ring_buffer_consume(chan, cpu, NULL, NULL);
+	if (!e)
+		return -EAGAIN;
+	void *ptr = ring_buffer_event_data(e);
+	size_t size = min(len, ring_buffer_event_length(e));
+	memcpy(dst, ptr, size);
+	return size;
+}
+noinline static inline ssize_t mpsc_recv(mpsc_t chan, void *dst, size_t len)
+{
+	int cpu;
+	for_each_online_cpu(cpu) {
+		struct ring_buffer_event *e =
+			ring_buffer_consume(chan, cpu, NULL, NULL);
+		if (!e)
+			continue;
+		void *ptr = ring_buffer_event_data(e);
+		size_t size = min(len, ring_buffer_event_length(e));
+		memcpy(dst, ptr, size);
+		return size;
+	}
+	return -EAGAIN;
+}
+noinline static inline void mpsc_drop(mpsc_t chan)
+{
+	ring_buffer_free(chan);
+}
+static inline bool mpsc_wait_always(void *p)
+{
+	return false;
+}
+noinline static inline int mpsc_wait(mpsc_t chan)
+{
+	return ring_buffer_wait(chan, RING_BUFFER_ALL_CPUS, 0, mpsc_wait_always,
+				NULL);
+}
+extern int ring_buffer_select2(struct trace_buffer *b0, ring_buffer_cond_fn c0,
+			       void *d0, struct trace_buffer *b1,
+			       ring_buffer_cond_fn c1, void *d1);
+extern int ring_buffer_select3(struct trace_buffer *b0, ring_buffer_cond_fn c0,
+			       void *d0, struct trace_buffer *b1,
+			       ring_buffer_cond_fn c1, void *d1,
+			       struct trace_buffer *b2, ring_buffer_cond_fn c2,
+			       void *d2);
+noinline static inline int mpsc_select2(mpsc_t ch0, mpsc_t ch1)
+{
+	return ring_buffer_select2(ch0, mpsc_wait_always, NULL, ch1,
+				   mpsc_wait_always, NULL);
+}
+noinline static inline int mpsc_select3(mpsc_t ch0, mpsc_t ch1, mpsc_t ch2)
+{
+	return ring_buffer_select3(ch0, mpsc_wait_always, NULL, ch1,
+				   mpsc_wait_always, NULL, ch2,
+				   mpsc_wait_always, NULL);
+}
+
+// CAREFUL: This macro does not support break
+#define mpsc_for_each(ch, elem)                                        \
+	for (int __cpu, __done = 0; !__done; __done = true)            \
+		for_each_online_cpu(__cpu)                             \
+			for (ssize_t __fail = 0; __fail < MPSC_RETRY;) \
+				if (sizeof(elem) !=                    \
+				    mpsc_recv_cpu(ch, __cpu, &elem,    \
+						  sizeof(elem))) {     \
+					++__fail;                      \
+				} else
+
+#endif // !DEMETER_MPSC_H
diff --git a/mm/demeter/pebs.h b/mm/demeter/pebs.h
new file mode 100644
index 000000000000..fe7ec6bb74df
--- /dev/null
+++ b/mm/demeter/pebs.h
@@ -0,0 +1,16 @@
+#ifndef DEMETER_PLACEMENT_PEBS_H
+#define DEMETER_PLACEMENT_PEBS_H
+
+#include <linux/perf_event.h>
+// #include <../../kernel/events/internal.h>
+
+struct perf_sample {
+	u32 config, config1;
+	u32 pid, tid;
+	u64 time;
+	u64 addr;
+	u64 weight;
+	u64 phys_addr;
+};
+
+#endif // !DEMETER_PLACEMENT_PEBS_H
diff --git a/mm/demeter/range_tree.h b/mm/demeter/range_tree.h
new file mode 100644
index 000000000000..ea4972f88737
--- /dev/null
+++ b/mm/demeter/range_tree.h
@@ -0,0 +1,335 @@
+#ifndef DEMETER_PLACEMENT_RANGE_TREE_H
+#define DEMETER_PLACEMENT_RANGE_TREE_H
+
+#include <linux/mm.h>
+#include <linux/maple_tree.h>
+#include <linux/sort.h>
+
+#include "module.h"
+#include "error.h"
+
+struct mrange {
+	ulong start, end;
+	// We record the access count, but we rank them based on the frequency
+	ulong age, nr_access;
+	ulong in_fmem, in_smem;
+};
+
+noinline static inline struct mrange *mrange_new(ulong start, ulong end,
+						 ulong age, ulong nr_access)
+{
+	struct mrange *r = kmalloc(sizeof(*r), GFP_KERNEL);
+	if (!r)
+		return ERR_PTR(-ENOMEM);
+	*r = (struct mrange){
+		.start = start,
+		.end = end,
+		.age = age,
+		.nr_access = nr_access,
+	};
+	return r;
+}
+
+noinline static inline void mrange_drop(struct mrange *r)
+{
+	kfree(r);
+}
+
+static inline ulong mrange_freq(struct mrange const *r)
+{
+	return r->nr_access * RTREE_GRANULARITY / (r->end - r->start + 1);
+}
+
+static inline void mrange_show(struct mrange const *r)
+{
+	static char const *units[] = {
+		"B", "KiB", "MiB", "GiB", "TiB", //"PiB", "EiB",
+	};
+	ulong len = r->end - r->start, ui = ARRAY_SIZE(units), unit;
+	while (len < (unit = 1ul << (--ui * 10))) {
+	}
+
+	pr_info("%s: managed range [%#lx, %#lx) len=%lu.%03lu%s freq=%lu age=%lu nr_access=%lu in_fmem=%lu in_smem=%lu\n",
+		__func__, r->start, r->end, len / unit,
+		(len % unit) * 1000 / unit, units[ui], mrange_freq(r), r->age,
+		r->nr_access, r->in_fmem, r->in_smem);
+}
+
+// In theory the maximum range we need to cover is 128TiB under 48bit virtual
+// address. But in practice we only need to cover [start_brk, mmap_base).
+// An example is: [0x555558200000, 0x7f34ab400000) ~43TiB. We assume 64TiB, so
+// total 2^25 segments. So for a perfect binary tree, the possible number of
+// tree elements is 2^25 * 2. This is quite a large number, so we should use
+// something else as the underlying storage than a simple array.
+// CREDIT: https://codeforces.com/blog/entry/18051
+struct range_tree {
+	// Assume a full segment with sparse storage to simplify implementation
+	// We index into the segment tree using the index off the start address
+	// with a granularity of SEG_TREE_GRANULARITY
+	ulong len, age, min_range;
+	// Use the maple_tree as a sparse array
+	struct maple_tree tree;
+};
+
+noinline static inline int rt_init(struct range_tree *self, ulong start,
+				   ulong end)
+{
+	pr_info("%s: start=%#lx end=%#lx\n", __func__, start, end);
+	BUILD_BUG_ON(RTREE_GRANULARITY < PAGE_SIZE);
+	start = round_down(start, RTREE_GRANULARITY);
+	end = round_up(end, RTREE_GRANULARITY);
+	self->len = 1;
+	self->age = 0;
+	self->min_range = end - start;
+	mt_init(&self->tree);
+	UNWRAP(mtree_insert_range(&self->tree, start, end - 1,
+				  UNWRAP(mrange_new(start, end, self->age, 0)),
+				  GFP_KERNEL));
+	return 0;
+}
+
+noinline static inline void rt_drop(struct range_tree *self)
+{
+	struct mrange *r;
+	ulong start = 0;
+	mt_for_each(&self->tree, r, start, ULONG_MAX) {
+		mrange_drop(r);
+	}
+}
+
+noinline static inline void rt_show(struct range_tree *self)
+{
+	return;
+	pr_info("%s: managed range count=%lu age=%lu\n", __func__, self->len,
+		self->age);
+	struct mrange *r;
+	ulong start = 0;
+	mt_for_each(&self->tree, r, start, ULONG_MAX) {
+		mrange_show(r);
+		// pr_info("%s: managed range [%#lx, %#lx) len=%luM age=%lu nr_access=%lu in_fmem=%lu in_smem=%lu\n",
+		// 	__func__, r->start, r->end, (r->end - r->start) >> 20,
+		// 	r->age, r->nr_access, r->in_fmem, r->in_smem);
+	}
+}
+
+noinline static inline int rt_count(struct range_tree *self, ulong addr)
+{
+	ulong start = addr;
+	struct mrange *r = mt_find(&self->tree, &start, ULONG_MAX);
+	if (!r || r->start > addr) {
+		pr_err_ratelimited("%s: address %#lx is not in any range\n",
+				   __func__, addr);
+		return PEBS_NR_DISCARDED_ERROR - PEBS_NR_DISCARDED;
+	}
+	r->nr_access += 1;
+	return 0;
+}
+
+noinline static inline int rt_insert(struct range_tree *self, ulong start,
+				   ulong end)
+{
+	pr_info("%s: start=%#lx end=%#lx\n", __func__, start, end);
+	start = round_down(start, RTREE_GRANULARITY);
+	end = round_up(end, RTREE_GRANULARITY);
+	UNWRAP(mtree_insert_range(&self->tree, start, end - 1,
+				  UNWRAP(mrange_new(start, end, self->age, 0)),
+				  GFP_KERNEL));
+	self->len += 1;
+	return 0;
+}
+
+
+// Split a managed range if its access count is sigificanitly higher than the
+// neighboring ranges.
+noinline static inline int rt_split(struct range_tree *self)
+{
+	ulong start = 0;
+	for (struct mrange *curr = mt_find(&self->tree, &start, ULONG_MAX);
+	     curr; curr = mt_find_after(&self->tree, &start, ULONG_MAX)) {
+		// TODO: fine tune a better decaying factor
+		curr->nr_access = curr->nr_access / 2;
+	}
+	start = 0;
+	for (struct mrange *
+		     curr = mt_find(&self->tree, &start, ULONG_MAX),
+		    *next = mt_find_after(&self->tree, &start, ULONG_MAX),
+		    *prev = NULL;
+	     curr && self->len < RTREE_MAX_SIZE; prev = curr, curr = next,
+		    next = mt_find_after(&self->tree, &start, ULONG_MAX)) {
+		if (curr->end - curr->start < RTREE_SPLIT_N * RTREE_GRANULARITY)
+			continue;
+		if (curr->nr_access < rtree_split_thresh * num_online_cpus())
+			continue;
+		// We should allow:
+		// 1. the root node which has no neighbors
+		// 2. curr->nr_access larger than at least one of its neighbors
+		int score = !prev && !next;
+		if (prev &&
+		    prev->nr_access + rtree_split_thresh * num_online_cpus() *
+					      RTREE_SIGNIFICANCE_FACTOR <
+			    curr->nr_access)
+			score += 1;
+		if (next &&
+		    next->nr_access + rtree_split_thresh * num_online_cpus() *
+					      RTREE_SIGNIFICANCE_FACTOR <
+			    curr->nr_access)
+			score += 1;
+		if (score < 1)
+			continue;
+
+		pr_info("%s: splitting range [%#lx, %#lx)\n", __func__,
+			curr->start, curr->end);
+		!prev ?: mrange_show(prev);
+		mrange_show(curr);
+		!next ?: mrange_show(next);
+		// rt_show(self);
+		// Split the range
+		BUG_ON(mtree_erase(&self->tree, curr->start) != curr);
+		// ulong mid = round_down(curr->start / 2 + (curr->end + 1) / 2,
+		// 		       RTREE_GRANULARITY);
+		self->age += 1;
+		ulong edges[RTREE_SPLIT_N + 1] = {
+			[0] = curr->start, [RTREE_SPLIT_N] = curr->end
+		};
+		for (ulong i = 1,
+			   step = (curr->end - curr->start) / RTREE_SPLIT_N;
+		     i < RTREE_SPLIT_N; ++i) {
+			edges[i] = round_down(curr->start + i * step,
+					      RTREE_GRANULARITY);
+			BUG_ON(edges[i] <= edges[i - 1]);
+		}
+		struct mrange *ins = NULL;
+		ulong min_range = ULONG_MAX;
+		for (ulong i = 0, nr_access = curr->nr_access / RTREE_SPLIT_N;
+		     i < RTREE_SPLIT_N; ++i) {
+			pr_info("%s: inserting range [%#lx, %#lx)\n", __func__,
+				edges[i], edges[i + 1]);
+			min_range = min(min_range, edges[i + 1] - edges[i]);
+			UNWRAP(mtree_insert_range(
+				&self->tree, edges[i], edges[i + 1] - 1,
+				ins = UNWRAP(mrange_new(edges[i], edges[i + 1],
+							self->age, nr_access)),
+				GFP_KERNEL));
+			mrange_show(ins);
+		}
+		mrange_drop(curr);
+		self->min_range = min(self->min_range, min_range);
+		// Prevent stuck at splitting the same range
+		curr = ins;
+		self->len += RTREE_SPLIT_N - 1;
+	}
+	// Newly created ranges can be observed by checking self->len
+	return 0;
+}
+
+static inline bool rt_should_cool(struct range_tree const *self,
+				  struct mrange const *r)
+{
+	return r->age + RTREE_COOL_AGE > self->age;
+}
+
+static inline int rt_rank_cmp(const void *a, const void *b, const void *pri)
+{
+	// struct range_tree const *self = pri;
+	struct mrange const *ra = *(struct mrange **)a,
+			    *rb = *(struct mrange **)b;
+	// Comparison priority: freq >> age >> -(in_fmem + in_smem)
+	// Comparison priority: freq >> age
+	return mrange_freq(ra) - mrange_freq(rb)     ?:
+		       ra->nr_access - rb->nr_access ?:
+						       ra->age - rb->age;
+}
+
+// See: for_each_vma_range
+#define vma_for_each(__mm, __start, __end, __vma)      \
+	for (VMA_ITERATOR((__vmi), (__mm), (__start)); \
+	     ((__vma) = vma_find(&(__vmi), (__end))) != NULL;)
+// Make sure the page reference is released
+#define folio_for_each(__vma, __start, __end, __folio)                       \
+	for (ulong __addr = max((__start), (__vma)->vm_start),               \
+		   __e = min((__end), (__vma)->vm_end), __step;              \
+	     __addr < __e; __addr += __step)                                 \
+		for ((__folio) = ({                                          \
+			     __step = PAGE_SIZE;                             \
+			     struct page *__page = follow_page(              \
+				     (__vma), __addr, FOLL_GET | FOLL_DUMP); \
+			     IS_ERR_OR_NULL(__page) ? NULL :                 \
+						      page_folio(__page);    \
+		     });                                                     \
+		     (__folio);                                              \
+		     (__folio) = ({                                          \
+			     __step = folio_nr_pages((__folio)) * PAGE_SIZE; \
+			     folio_put((__folio));                           \
+			     NULL;                                           \
+		     }))
+
+// TODO: calculate exchange candidates by walking every leaf's intersected vma,
+// populating the in_fmem/in_smem fields and sort them based on access count
+// r is assumed to be an output array to store self->len elements
+// Output ranges are sorted by the access count in ascending order.
+// If they are equal, then we sort by the number of folios in decending order.
+// i.e. the range that is least accessed and has the most folios will be first.
+noinline static inline int rt_rank(struct range_tree *self,
+				   struct mm_struct *locked_mm,
+				   struct mrange **out, ulong *len)
+{
+	ulong start = 0, i = 0;
+	struct mrange *r;
+	mt_for_each(&self->tree, r, start, ULONG_MAX) {
+		out[i++] = r;
+		struct vm_area_struct *vma;
+		vma_for_each(locked_mm, r->start, r->end, vma) {
+			struct folio *folio;
+			folio_for_each(vma, r->start, r->end, folio) {
+				// Only rank private anon folios for now
+				int nid = folio_test_anon(folio) ?
+						  folio_nid(folio) :
+						  NUMA_NO_NODE;
+				r->in_fmem += nid == FMEM_NID;
+				r->in_smem += nid == SMEM_NID;
+			}
+		}
+	}
+
+	// Comparison priority: freq >> age >> -(in_fmem + in_smem)
+	// Sort order: ascending
+	sort_r(out, self->len, sizeof(*out), rt_rank_cmp, NULL, self);
+
+	for (ulong i = 0; i < self->len; ++i) {
+		struct mrange *r = out[self->len - 1 - i];
+		if (rt_should_cool(self, r))
+			continue;
+		*len = self->len - i;
+		break;
+	}
+
+	return 0;
+}
+
+// isolate the folios that are on the given node using the provided function to
+// the given list
+noinline static inline int
+rt_isolate(struct mm_struct *locked_mm, struct mrange *r, int nid, ulong need,
+	   int (*isolate)(struct list_head *list, struct folio *folio),
+	   struct list_head *list)
+{
+	int success = 0;
+	struct vm_area_struct *vma;
+	vma_for_each(locked_mm, r->start, r->end, vma) {
+		struct folio *folio;
+		folio_for_each(vma, r->start, r->end, folio) {
+			if (folio_nid(folio) != nid) {
+				continue;
+			}
+			success += isolate(list, folio) == 0;
+			if (success >= need) {
+				return success;
+			}
+		}
+	}
+	return success;
+}
+#undef folio_for_each
+#undef vma_for_each
+
+#endif // DEMETER_PLACEMENT_RANGE_TREE_H
diff --git a/mm/demeter/sysfs.c b/mm/demeter/sysfs.c
new file mode 100644
index 000000000000..3d91d673d394
--- /dev/null
+++ b/mm/demeter/sysfs.c
@@ -0,0 +1,257 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2021-2024 Junliang Hu
+ *
+ * Author: Junliang Hu <jlhu@cse.cuhk.edu.hk>
+ *
+ */
+#include <linux/mutex.h>
+#include <linux/kobject.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+
+#include "demeter.h"
+
+DEFINE_MUTEX(demeter_sysfs_lock);
+
+struct demeter_sysfs_target {
+	struct kobject kobj;
+	struct target *target;
+};
+
+static struct demeter_sysfs_target *demeter_sysfs_target_alloc(void)
+{
+	return kzalloc(sizeof(struct demeter_sysfs_target),
+		       GFP_KERNEL | __GFP_NOWARN);
+}
+static inline bool demeter_sysfs_target_running(struct demeter_sysfs_target *t)
+{
+	return t->target && target_pid(t->target);
+}
+static int demeter_sysfs_target_add_dirs(struct demeter_sysfs_target *t)
+{
+	return 0;
+}
+static void demeter_sysfs_target_rm_dirs(struct demeter_sysfs_target *t)
+{
+}
+
+static void demeter_sysfs_target_release(struct kobject *kobj)
+{
+	struct demeter_sysfs_target *t =
+		container_of(kobj, struct demeter_sysfs_target, kobj);
+	if (t->target) {
+		target_drop(t->target);
+	}
+	kfree(t);
+}
+static ssize_t pid_show(struct kobject *kobj, struct kobj_attribute *attr,
+			char *buf)
+{
+	struct demeter_sysfs_target *t =
+		container_of(kobj, struct demeter_sysfs_target, kobj);
+	if (!t->target) {
+		return sysfs_emit(buf, "%d\n", -1);
+	}
+	return sysfs_emit(buf, "%d\n", target_pid(t->target));
+}
+static ssize_t pid_store(struct kobject *kobj, struct kobj_attribute *attr,
+			 const char *buf, size_t count)
+{
+	struct demeter_sysfs_target *t =
+		container_of(kobj, struct demeter_sysfs_target, kobj);
+	int pid, err = kstrtoint(buf, 10, &pid);
+	if (err || pid < -1) {
+		return -EINVAL;
+	}
+	if (!mutex_trylock(&demeter_sysfs_lock)) {
+		return -EBUSY;
+	}
+	if (t->target) {
+		target_drop(t->target);
+		t->target = NULL;
+	}
+	if (pid == -1) {
+		mutex_unlock(&demeter_sysfs_lock);
+		return count;
+	}
+	// Wait a while to avoid conflict with userspace initialization
+	schedule_timeout_interruptible(msecs_to_jiffies(2000));
+	struct target *target = target_new(pid);
+	if (IS_ERR(target)) {
+		mutex_unlock(&demeter_sysfs_lock);
+		return PTR_ERR(target);
+	}
+	t->target = target;
+	mutex_unlock(&demeter_sysfs_lock);
+	return count;
+}
+static struct kobj_attribute demeter_sysfs_target_pid_attr =
+	__ATTR_RW_MODE(pid, 0600);
+static struct attribute *demeter_sysfs_target_attrs[] = {
+	&demeter_sysfs_target_pid_attr.attr,
+	NULL,
+};
+ATTRIBUTE_GROUPS(demeter_sysfs_target);
+static const struct kobj_type demeter_sysfs_target_ktype = {
+	.release = demeter_sysfs_target_release,
+	.sysfs_ops = &kobj_sysfs_ops,
+	.default_groups = demeter_sysfs_target_groups,
+};
+
+struct demeter_sysfs_targets {
+	struct kobject kobj;
+	struct demeter_sysfs_target **targets;
+	int nr;
+};
+
+static bool demeter_sysfs_targets_busy(struct demeter_sysfs_target **targets,
+				      int nr_targets)
+{
+	for (int i = 0; i < nr_targets; ++i) {
+		if (demeter_sysfs_target_running(targets[i])) {
+			return true;
+		}
+	}
+	return false;
+}
+static void demeter_sysfs_targets_rm_dirs(struct demeter_sysfs_targets *targets)
+{
+	for (int i = 0; i < targets->nr; ++i) {
+		struct demeter_sysfs_target *t = targets->targets[i];
+		demeter_sysfs_target_rm_dirs(t);
+		kobject_put(&t->kobj);
+	}
+	targets->nr = 0;
+	if (targets->targets)
+		kfree(targets->targets);
+	targets->targets = NULL;
+}
+static int demeter_sysfs_targets_add_dirs(struct demeter_sysfs_targets *targets,
+					 int nr_targets)
+{
+	if (demeter_sysfs_targets_busy(targets->targets, targets->nr)) {
+		return -EBUSY;
+	}
+	demeter_sysfs_targets_rm_dirs(targets);
+	if (nr_targets == 0) {
+		return 0;
+	}
+	struct demeter_sysfs_target **targets_arr =
+		kcalloc(nr_targets, sizeof(struct demeter_sysfs_target *),
+			GFP_KERNEL | __GFP_NOWARN);
+	if (!targets_arr) {
+		return -ENOMEM;
+	}
+	targets->targets = targets_arr;
+	for (int i = 0; i < nr_targets; ++i) {
+		struct demeter_sysfs_target *t = demeter_sysfs_target_alloc();
+		if (!t) {
+			demeter_sysfs_targets_rm_dirs(targets);
+			return -ENOMEM;
+		}
+		int err = kobject_init_and_add(&t->kobj,
+					       &demeter_sysfs_target_ktype,
+					       &targets->kobj, "%d", i);
+		if (err) {
+			goto err;
+		}
+		err = demeter_sysfs_target_add_dirs(t);
+		if (err) {
+			goto err;
+		}
+		targets->targets[i] = t;
+		targets->nr = i + 1;
+		continue;
+err:
+		demeter_sysfs_targets_rm_dirs(targets);
+		kobject_put(&t->kobj);
+		return err;
+	}
+	return 0;
+}
+static ssize_t nr_targets_show(struct kobject *kobj,
+			       struct kobj_attribute *attr, char *buf)
+{
+	struct demeter_sysfs_targets *targets =
+		container_of(kobj, struct demeter_sysfs_targets, kobj);
+	return sysfs_emit(buf, "%d\n", targets->nr);
+}
+static ssize_t nr_targets_store(struct kobject *kobj,
+				struct kobj_attribute *attr, const char *buf,
+				size_t count)
+{
+	int nr, err = kstrtoint(buf, 10, &nr);
+	if (err || nr < 0) {
+		return -EINVAL;
+	}
+	struct demeter_sysfs_targets *targets =
+		container_of(kobj, struct demeter_sysfs_targets, kobj);
+	if (!mutex_trylock(&demeter_sysfs_lock))
+		return -EBUSY;
+	err = demeter_sysfs_targets_add_dirs(targets, nr);
+	mutex_unlock(&demeter_sysfs_lock);
+	if (err)
+		return err;
+
+	return count;
+}
+static struct demeter_sysfs_targets *demeter_sysfs_targets_alloc(void)
+{
+	return kzalloc(sizeof(struct demeter_sysfs_targets),
+		       GFP_KERNEL | __GFP_NOWARN);
+}
+static void demeter_sysfs_targets_release(struct kobject *kobj)
+{
+	kfree(container_of(kobj, struct demeter_sysfs_targets, kobj));
+}
+static struct kobj_attribute demeter_sysfs_targets_nr_attr =
+	__ATTR_RW_MODE(nr_targets, 0600);
+static struct attribute *demeter_sysfs_targets_attrs[] = {
+	&demeter_sysfs_targets_nr_attr.attr,
+	NULL,
+};
+ATTRIBUTE_GROUPS(demeter_sysfs_targets);
+static const struct kobj_type demeter_sysfs_targets_ktype = {
+	.release = demeter_sysfs_targets_release,
+	.sysfs_ops = &kobj_sysfs_ops,
+	.default_groups = demeter_sysfs_targets_groups,
+};
+
+static struct kobject *demeter_sysfs_root;
+static struct demeter_sysfs_targets *demeter_sysfs_targets;
+int __init demeter_sysfs_init(void)
+{
+	demeter_sysfs_root = kobject_create_and_add("demeter", mm_kobj);
+	if (!demeter_sysfs_root) {
+		return -ENOMEM;
+	}
+
+	demeter_sysfs_targets = demeter_sysfs_targets_alloc();
+	int err = kobject_init_and_add(&demeter_sysfs_targets->kobj,
+				       &demeter_sysfs_targets_ktype,
+				       demeter_sysfs_root, "targets");
+	if (err) {
+		kobject_put(&demeter_sysfs_targets->kobj);
+		kobject_put(demeter_sysfs_root);
+		return err;
+	}
+
+	return 0;
+}
+void __exit demeter_sysfs_exit(void)
+{
+	if (!demeter_sysfs_root) {
+		return;
+	}
+	if (!demeter_sysfs_targets) {
+		kobject_put(demeter_sysfs_root);
+		return;
+	}
+	demeter_sysfs_targets_rm_dirs(demeter_sysfs_targets);
+
+	kobject_put(&demeter_sysfs_targets->kobj);
+	kobject_put(demeter_sysfs_root);
+}
+// module_init(demeter_sysfs_init);
+// module_exit(demeter_sysfs_exit);
diff --git a/mm/demeter/vector.c b/mm/demeter/vector.c
new file mode 100644
index 000000000000..9d0c5af009cb
--- /dev/null
+++ b/mm/demeter/vector.c
@@ -0,0 +1,140 @@
+#include "vector.h"
+
+int vector_new(struct vector *v, size_t elem_size, size_t cap)
+{
+	void *data = kvcalloc(cap, elem_size, GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+	*v = (struct vector){
+		.data = data, .len = 0, .cap = cap, .elem_size = elem_size
+	};
+	return 0;
+};
+
+void vector_drop(struct vector *v)
+{
+	if (v->data)
+		kvfree(v->data);
+}
+
+void *vector_at(struct vector *v, size_t i)
+{
+	VECTOR_CHECK(i < v->len, "at() out of range i=%zu len=%zu\n", i,
+		     v->len);
+	// if (i >= v->len)
+	// 	return NULL;
+	return v->data + i * v->elem_size;
+}
+
+void vector_grow(struct vector *v, size_t cap)
+{
+	void *data = kvrealloc(v->data, v->cap * v->elem_size,
+			       cap * v->elem_size, GFP_KERNEL);
+	VECTOR_CHECK(data != NULL, "kvrealloc() returned null");
+	v->data = data;
+	v->cap = cap;
+}
+
+void vector_resize(struct vector *v, void const *elem, size_t new_size)
+{
+	if (new_size > v->cap) {
+		vector_grow(v, new_size);
+	}
+	for (size_t i = v->len; i < new_size; i++) {
+		memcpy(v->data + i * v->elem_size, elem, v->elem_size);
+	}
+	v->len = new_size;
+}
+void vector_push_back(struct vector *v, void const *elem)
+{
+	if (v->len == v->cap)
+		vector_grow(v, 2 * v->cap);
+	memcpy(v->data + v->len * v->elem_size, elem, v->elem_size);
+	v->len++;
+	// pr_info_ratelimited("%s: new_len=%zu elem=[0x%llx 0x%llx]", __func__, v->len,
+	// 	*(u64 *)elem, *(u64 *)(elem + 8));
+}
+
+void vector_pop_back(struct vector *v)
+{
+	VECTOR_CHECK(v->len > 0, "pop_back() on empty vector");
+	v->len--;
+}
+
+void vector_erase(struct vector *v, size_t i)
+{
+	VECTOR_CHECK(i < v->len, "erase() out of range");
+	memmove(v->data + i * v->elem_size, v->data + (i + 1) * v->elem_size,
+		(v->len - i - 1) * v->elem_size);
+	v->len--;
+}
+
+void vector_clear(struct vector *v)
+{
+	v->len = 0;
+}
+
+void *vector_front(struct vector *v)
+{
+	VECTOR_CHECK(v->len > 0, "front() on empty vector");
+	return v->data;
+}
+void *vector_back(struct vector *v)
+{
+	VECTOR_CHECK(v->len > 0, "back() on empty vector");
+	return v->data + (v->len - 1) * v->elem_size;
+}
+
+static void swap_data(void *a, void *b, void *tmp, size_t size)
+{
+	memcpy(tmp, a, size);
+	memcpy(a, b, size);
+	memcpy(b, tmp, size);
+}
+void vector_swap_stack(struct vector *v, size_t i, size_t j)
+{
+	char buf[VECTOR_SWAP_STACK_SIZE];
+	swap_data(v->data + i * v->elem_size, v->data + j * v->elem_size, buf,
+		  v->elem_size);
+}
+void vector_swap(struct vector *v, size_t i, size_t j)
+{
+	VECTOR_CHECK(i < v->len && j < v->len,
+		     "swap() out of range len=%zu i=%zu j=%zu\n", v->len, i, j);
+	switch (v->elem_size) {
+	case 1:
+		swap(*(u8 *)(v->data + i), *(u8 *)(v->data + j));
+		break;
+	case 2:
+		swap(*(u16 *)(v->data + i * 2), *(u16 *)(v->data + j * 2));
+		break;
+	case 4:
+		swap(*(u32 *)(v->data + i * 4), *(u32 *)(v->data + j * 4));
+		break;
+	case 8:
+		swap(*(u64 *)(v->data + i * 8), *(u64 *)(v->data + j * 8));
+		break;
+	case 16:
+		swap(*(u64 *)(v->data + i * 16), *(u64 *)(v->data + j * 16));
+		swap(*(u64 *)(v->data + i * 16 + 8),
+		     *(u64 *)(v->data + j * 16 + 8));
+		break;
+	default:
+		if (v->elem_size <= VECTOR_SWAP_STACK_SIZE) {
+			vector_swap_stack(v, i, j);
+		} else {
+			void *t = kvmalloc(v->elem_size, GFP_KERNEL);
+			swap_data(v->data + i * v->elem_size,
+				  v->data + j * v->elem_size, t, v->elem_size);
+			kvfree(t);
+		}
+	}
+}
+size_t vector_size(struct vector *v)
+{
+	return v->len;
+}
+bool vector_empty(struct vector *v)
+{
+	return v->len == 0;
+}
diff --git a/mm/demeter/vector.h b/mm/demeter/vector.h
new file mode 100644
index 000000000000..e6ccb4d9ed8f
--- /dev/null
+++ b/mm/demeter/vector.h
@@ -0,0 +1,54 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * A C++'s std::vector implementation in C.
+ *
+ * Copyright (C) 2021-2024 Junliang Hu
+ *
+ * Author: Junliang Hu <jlhu@cse.cuhk.edu.hk>
+ *
+ */
+
+#ifndef VECTOR_H
+#define VECTOR_H
+
+#include <linux/types.h>
+#include <linux/slab.h>
+
+#define VECTOR_CHECK(cond_, ...)                                               \
+	do {                                                                   \
+		if (cond_)                                                     \
+			break;                                                 \
+		pr_err("VECTOR_CHECK failed at %s:%d caller %pSR\n", __FILE__, \
+		       __LINE__, __builtin_return_address(0));                 \
+		pr_err(__VA_ARGS__);                                           \
+		dump_stack();                                                  \
+		BUG();                                                         \
+	} while (false)
+
+struct vector {
+	size_t elem_size;
+	size_t len;
+	size_t cap;
+	void *data;
+};
+
+noinline int vector_new(struct vector *v, size_t elem_size, size_t cap);
+noinline void vector_drop(struct vector *v);
+noinline void *vector_at(struct vector *v, size_t i);
+noinline void vector_grow(struct vector *v, size_t cap);
+noinline void vector_resize(struct vector *v, void const *elem,
+			    size_t new_size);
+noinline void vector_push_back(struct vector *v, void const *elem);
+noinline void vector_pop_back(struct vector *v);
+noinline void vector_erase(struct vector *v, size_t i);
+noinline void vector_clear(struct vector *v);
+noinline void *vector_front(struct vector *v);
+noinline void *vector_back(struct vector *v);
+
+enum { VECTOR_SWAP_STACK_SIZE = 256 };
+noinline void vector_swap_stack(struct vector *v, size_t i, size_t j);
+noinline void vector_swap(struct vector *v, size_t i, size_t j);
+noinline size_t vector_size(struct vector *v);
+noinline bool vector_empty(struct vector *v);
+
+#endif /* VECTOR_H */
diff --git a/mm/migrate.c b/mm/migrate.c
index a8c6f466e33a..1464e66b51e2 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -378,7 +378,7 @@ void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd)
 }
 #endif
 
-static int folio_expected_refs(struct address_space *mapping,
+int folio_expected_refs(struct address_space *mapping,
 		struct folio *folio)
 {
 	int refs = 1;
@@ -711,7 +711,7 @@ EXPORT_SYMBOL(migrate_folio);
 
 #ifdef CONFIG_BUFFER_HEAD
 /* Returns true if all buffers are successfully locked */
-static bool buffer_migrate_lock_buffers(struct buffer_head *head,
+bool buffer_migrate_lock_buffers(struct buffer_head *head,
 							enum migrate_mode mode)
 {
 	struct buffer_head *bh = head;
@@ -887,7 +887,7 @@ EXPORT_SYMBOL_GPL(filemap_migrate_folio);
 /*
  * Writeback a folio to clean the dirty state
  */
-static int writeout(struct address_space *mapping, struct folio *folio)
+int writeout(struct address_space *mapping, struct folio *folio)
 {
 	struct writeback_control wbc = {
 		.sync_mode = WB_SYNC_NONE,
@@ -2424,7 +2424,7 @@ static int do_pages_stat(struct mm_struct *mm, unsigned long nr_pages,
 	return nr_pages ? -EFAULT : 0;
 }
 
-static struct mm_struct *find_mm_struct(pid_t pid, nodemask_t *mem_nodes)
+struct mm_struct *find_mm_struct(pid_t pid, nodemask_t *mem_nodes)
 {
 	struct task_struct *task;
 	struct mm_struct *mm;
diff --git a/mm/mm_init.c b/mm/mm_init.c
index 3ec04933f7fd..35df34b1b9ab 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -157,6 +157,7 @@ early_param("mminit_loglevel", set_mminit_loglevel);
 #endif /* CONFIG_DEBUG_MEMORY_INIT */
 
 struct kobject *mm_kobj;
+EXPORT_SYMBOL_GPL(mm_kobj);
 
 #ifdef CONFIG_SMP
 s32 vm_committed_as_batch = 32;
diff --git a/mm/shmem.c b/mm/shmem.c
index c1befe046c7e..01b83e32838c 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -273,6 +273,7 @@ bool vma_is_anon_shmem(struct vm_area_struct *vma)
 {
 	return vma->vm_ops == &shmem_anon_vm_ops;
 }
+EXPORT_SYMBOL_GPL(vma_is_anon_shmem);
 
 bool vma_is_shmem(struct vm_area_struct *vma)
 {
diff --git a/mm/show_mem.c b/mm/show_mem.c
index bdb439551eef..cd783a59ceb2 100644
--- a/mm/show_mem.c
+++ b/mm/show_mem.c
@@ -116,6 +116,7 @@ void si_meminfo_node(struct sysinfo *val, int nid)
 #endif
 	val->mem_unit = PAGE_SIZE;
 }
+EXPORT_SYMBOL(si_meminfo_node);
 #endif
 
 /*
diff --git a/mm/swap.c b/mm/swap.c
index 67786cb77130..732f73f22fb8 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -909,6 +909,7 @@ void lru_add_drain_all(void)
 {
 	__lru_add_drain_all(false);
 }
+EXPORT_SYMBOL(lru_add_drain_all);
 #else
 void lru_add_drain_all(void)
 {
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 2e34de9cd0d4..87629e10eb70 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -818,6 +818,7 @@ void folio_putback_lru(struct folio *folio)
 	folio_add_lru(folio);
 	folio_put(folio);		/* drop ref from isolate */
 }
+EXPORT_SYMBOL(folio_putback_lru);
 
 enum folio_references {
 	FOLIOREF_RECLAIM,
@@ -1761,6 +1762,7 @@ bool folio_isolate_lru(struct folio *folio)
 
 	return ret;
 }
+EXPORT_SYMBOL(folio_isolate_lru);
 
 /*
  * A direct reclaimer may isolate SWAP_CLUSTER_MAX pages from the LRU list and
diff --git a/mm/vmstat.c b/mm/vmstat.c
index 8507c497218b..44abee695119 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -135,6 +135,32 @@ void all_vm_events(unsigned long *ret)
 }
 EXPORT_SYMBOL_GPL(all_vm_events);
 
+void clear_all_vm_events(void)
+{
+	int cpu;
+
+	for_each_online_cpu(cpu) {
+		struct vm_event_state *this = &per_cpu(vm_event_states, cpu);
+		int sz = NR_VM_EVENT_ITEMS * sizeof(unsigned long);
+
+		memset(this->event, 0, sz);
+	}
+}
+EXPORT_SYMBOL_GPL(clear_all_vm_events);
+
+/*
+ * This is the node to reset all vm events in /proc/vmstat
+ * via /proc/sys/vm/clear_all_vm_events
+ */
+int sysctl_clearvmevents_handler(struct ctl_table *table, int write,
+	void __user *buffer, size_t *length, loff_t *ppos)
+{
+	if (write)
+		clear_all_vm_events();
+
+	return 0;
+}
+
 /*
  * Fold the foreign cpu events into our own.
  *
@@ -1324,6 +1350,21 @@ const char * const vmstat_text[] = {
 	"thp_migration_fail",
 	"thp_migration_split",
 #endif
+	"pebs_nr_sampled",
+	"pebs_nr_sampled_fmem",
+	"pebs_nr_sampled_smem",
+	"pebs_nr_discarded",
+	"pebs_nr_discarded_null",
+	"pebs_nr_discarded_pid",
+	"pebs_nr_discarded_error",
+	"pebs_nr_discarded_ignore",
+	"folio_exchange",
+	"folio_exchange_success",
+	"folio_exchange_failed",
+	"folio_exchange_failed_isolate",
+	"folio_exchange_failed_lock",
+	"folio_exchange_failed_support",
+	"folio_exchange_failed_move",
 #ifdef CONFIG_COMPACTION
 	"compact_migrate_scanned",
 	"compact_free_scanned",
diff --git a/scripts/Makefile.lib b/scripts/Makefile.lib
index 9f06f6aaf7fc..a69964962ce0 100644
--- a/scripts/Makefile.lib
+++ b/scripts/Makefile.lib
@@ -573,6 +573,9 @@ quiet_cmd_zstd = ZSTD    $@
 quiet_cmd_zstd22 = ZSTD22  $@
       cmd_zstd22 = cat $(real-prereqs) | $(ZSTD) -22 --ultra > $@
 
+quiet_cmd_zstd_with_size = ZSTD  $@
+      cmd_zstd_with_size = { cat $(real-prereqs) | $(ZSTD) -T0; $(size_append); } > $@
+
 quiet_cmd_zstd22_with_size = ZSTD22  $@
       cmd_zstd22_with_size = { cat $(real-prereqs) | $(ZSTD) -22 --ultra; $(size_append); } > $@
 
